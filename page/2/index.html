<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    

    <!--Author-->
    
        <meta name="author" content="Robert Lexis">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="This is Robert Lexis."/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="This is Robert Lexis."/>

    <!--Type page-->
    
        <meta property="og:type" content="website" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>page - This is Robert Lexis.</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
            <h1 id="main-title" class="title">This is Robert Lexis.</h1>
        
    </div>
</header>

        <section class="main">
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/08/01/Nvidia-Jetson-TX2-开箱及刷机/">
                Nvidia Jetson TX2 开箱及刷机
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-08-01</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p>钱学森实验室购买了 Nvidia <a href="https://developer.nvidia.com/embedded-computing" target="_blank" rel="external">Jetson</a> TX2 支持嵌入式 AI 开发。</p>
<blockquote>
<p>Jetson, the Platform for AI at the Edge<br>NVIDIA Jetson with GPU-accelerated parallel processing is the world’s leading embedded AI computing platform. The Jetson portfolio of devices, featuring the new NVIDIA Jetson TX2, delivers more performance and features for Artificial Intelligence at the edge. Devices at the edge, from drones to intelligent cameras, need on-board AI to process complex data without relying on network connectivity. AI at the Edge is the future of industry, transforming processes in manufacturing, industrial inspection, agriculture, general robotics, security, and AI cities.</p>
</blockquote>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/jetson/Jetson.jpeg" alt="Jetson"><br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/jetson/Jetson_1.jpeg" alt="Jetson"></p>
<h3 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h3><p>NVIDIA Tegra Linux Driver Package – L4T 28.2.1 <a href="https://developer.nvidia.com/embedded/linux-tegra" target="_blank" rel="external">Linux For Tegra</a> 是为支持 Jetson 平台上的开发而设计的。<br>通过 <code>uname -a</code> 查看到的信息是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Linux tegra-ubuntu 4.4.38-tegra #1 SMP PREEMPT Thu May 17 00:15:19 PDT 2018 aarch64 aarch64 aarch64 GNU/Linux</div></pre></td></tr></table></figure></p>
<p>可以看出系统适配的是 aarch 处理器架构，而不是通常的 x86_64 架构。<br>L4T 28.2.1 支持的 Jetson 平台是 NVIDIA &#174; Tegra &#174; X2 series (Jetson TX2, Jetson TX2i)</p>
<p>L4T 28.2.1 System Requirements </p>
<ul>
<li>Host PC running Ubuntu Linux version 16.04 is recommended.</li>
<li>Tegra Linux Driver Package providing a kernel image, bootloader, NVIDIA drivers, and flashing utilities. </li>
</ul>
<h3 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h3><p><a href="https://www.nvidia.com/object/tegra.html" target="_blank" rel="external">Tegra</a> The World’s Fastest Mobile Processors.<br>Tegra ARM based processors.</p>
<h3 id="JetPack"><a href="#JetPack" class="headerlink" title="JetPack"></a>JetPack</h3><p>NVIDIA <a href="https://developer.nvidia.com/embedded/jetpack" target="_blank" rel="external">JetPack</a> SDK is the most comprehensive solution for building AI applications. Use the JetPack installer to flash your Jetson Developer Kit with <strong>the latest OS image</strong>, to install developer tools for both <strong>host PC</strong> and <strong>Developer Kit</strong>, and to install the libraries and APIs, samples, and documentation needed to jumpstart your development environment.</p>
<p>JetPack bundles all the Jetson platform software, including <a href="https://developer.nvidia.com/tensorrt" target="_blank" rel="external">TensorRT</a>, cuDNN, CUDA Toolkit, VisionWorks, GStreamer, and OpenCV, all built <strong>on top of L4T with LTS Linux kernel</strong>.</p>
<p>在<strong>主机 host PC</strong> 上下载安装 JetPack 3.3, JetPack 3.3 contains CUDA toolkit for the host (Ubuntu) and target platform, the latest NVIDIA Developer Tools (Tegra Graphics Debugger 2.5 and NVIDIA System Profiler 4.0), VisionWorks 1.6, cuDNN v7.1.5, Multimedia API v28.2, OpenCV 3.3.1, and TensorRT 4.0 GA.<a href="https://developer.nvidia.com/embedded/jetpack-notes" target="_blank" rel="external">JetPack Release Notes</a></p>
<p>JetPack 3.3 对主机的要求是 Ubuntu Linux x64 v16.04，对目标平台（Target Platform）的要求是 Jetson Developer Kit with Jetson TX2, Jetson TX2i, or Jetson TX1 module。</p>
<p>另外也需要一根以太网网线，An Ethernet cable plugged into the on-board Ethernet port, which is connected to either a secondary network card on your Linux host or the same network router providing Internet access for the Linux host.</p>
<p>JetPack 3.3 启动之后会下载所需的各种软件包，目前[2018/08/02]下载这些软件包需要主机 host PC 能够翻墙，可在 Ubuntu 下安装 ShadowSocks 客户端并配合 privoxy 连接 VPN 服务器实现翻墙，需要 VPN 服务器的可以联系我进行借用。</p>
<ol>
<li><p><code>sudo pip install shadowsocks</code></p>
</li>
<li><p>shawdowsocks.json</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&#123; </div><div class="line">&quot;server&quot;: &quot;_._._._&quot;, </div><div class="line">&quot;server_port&quot;: _, </div><div class="line">&quot;local_port&quot;: 1080, </div><div class="line">&quot;password&quot;: &quot;_&quot;, </div><div class="line">&quot;timeout&quot;: 600, </div><div class="line">&quot;method&quot;: &quot;aes-256-cfb&quot; </div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p><code>sudo sslocal -c shawdowsocks.json -d start</code></p>
</li>
<li><p>安装配置 privoxy。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install privoxy</div><div class="line"></div><div class="line">sudo vim /etc/privoxy/config</div><div class="line"></div><div class="line">1336行处 ：forward-socks5t / 127.0.0.1:1080 .</div><div class="line">sudo service privoxy start</div></pre></td></tr></table></figure>
</li>
<li><p>设置 http 和 https 全局代理 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">export http_proxy=&apos;http://localhost:8118&apos;</div><div class="line">export https_proxy=&apos;https://localhost:8118&apos;</div></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget www.google.com</div></pre></td></tr></table></figure>
</li>
</ol>
<p>JetPack 安装图示参考<a href="https://docs.nvidia.com/jetpack-l4t/index.html#jetpack/3.3/install.htm" target="_blank" rel="external">官方文档</a>。</p>
<h3 id="NVIDIA-TensorRT"><a href="#NVIDIA-TensorRT" class="headerlink" title="NVIDIA TensorRT"></a>NVIDIA <a href="https://developer.nvidia.com/tensorrt" target="_blank" rel="external">TensorRT</a></h3><p>Programmable Inference Accelerator (可编程推理加速器)<br>NVIDIA TensorRT is a high-performance deep learning <strong>inference</strong>(只有推理，NOT for Training) optimizer and runtime that delivers low latency and high-throughput for deep learning inference applications. With TensorRT, you can optimize neural network models, calibrate for lower precision with high accuracy, and finally deploy the models to hyperscale data centers, embedded, or automotive product platforms. TensorRT-based applications on GPUs perform up to 100x faster than CPU during inference for models trained in all major frameworks.</p>
<p>TensorRT provides INT8 and FP16 optimizations for production deployments of deep learning inference applications such as video streaming, speech recognition, recommendation and natural language processing. Reduced precision inference significantly lowers application latency, which is a requirement for many real-time services as well as auto and embedded applications.</p>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/jetson/TRT.png" alt="TRT"><br>You can import trained models from every deep learning framework into TensorRT. After applying optimizations, TensorRT selects platform specific kernels to maximize performance on Tesla GPUs in the datacenter, Jetson embedded platforms, and NVIDIA DRIVE autonomous driving platforms. </p>
<p>With TensorRT developers can focus on creating novel AI-powered applications rather than performance tuning for inference deployment.<br>关于使用详情，<a href="https://docs.nvidia.com/deeplearning/sdk/" target="_blank" rel="external">NVIDIA Deep Learning SDK</a>，<a href="https://devblogs.nvidia.com/deploying-deep-learning-nvidia-tensorrt/" target="_blank" rel="external">Deploying Deep Neural Networks with NVIDIA TensorRT</a>。</p>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/07/26/Stacked-Hourglass-Networks-卫星部件检测/">
                Stacked Hourglass Networks 卫星部件检测
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-07-26</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h3 id="STACKED-HOURGLASS-NETWORKS"><a href="#STACKED-HOURGLASS-NETWORKS" class="headerlink" title="STACKED HOURGLASS NETWORKS"></a>STACKED HOURGLASS NETWORKS</h3><p><a href="http://www-personal.umich.edu/~alnewell/pose/" target="_blank" rel="external">STACKED HOURGLASS NETWORKS</a> 是由密歇根大学的研究团队设计的一个专门用来进行人体姿态估计的网络结构。在卫星的部件检测项目中，我们对其进行了修改和调整，利用同样的网络结构完成了太阳帆板的检测定位（类似图像分割任务）、发动机和对接环的定位（关键点检测任务）、发动机和对接环的检测定位（类似图像分割任务）。<br>下面对 STACKED HOURGLASS NETWORKS 进行介绍及对自主实现的 Keras 关键代码进行分析。</p>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>在该论文中，作者指出：</p>
<blockquote>
<p>This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a “stacked hourglass” network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.</p>
</blockquote>
<p>特征在各个不同尺度上的进行处理并合并，可以让神经网络更好地把握部件的空间关系。将 bottom-up, top-down 的处理模块（hourglass 模块）进行重复堆叠（stack），在辅以中间监督 （intermediate supervision），可以极大地提高网络的性能。<br>实际上，是通过卷积神经网络对部件模型进行了编码表示，是一种隐式空间模型（implicit spatial model）。<br>STACKED HOURGLASS NETWORKS 与 CPM (Convolutional Pose Machines) 方法相比，思路更明晰，网络更简洁。<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/satellite/stacked-hg.png" alt="stacked hourglass"></p>
<h5 id="hourglass-模块"><a href="#hourglass-模块" class="headerlink" title="hourglass 模块"></a>hourglass 模块</h5><p>一个 hourglass 模块如下图所示，体现了跳级的思想。除红框内的 box 之外，每一个 box 都是一个 residual 模块的输出。<br>下图是 4 阶 hourglass 模块，进行了 4 次下采样（及对应的 4 次上采样）。n 阶 hourglass 模块可以提取从原始尺度到\(\frac{1}{2^n}\)<br>尺度的特征。<strong>在一个 hourglass 模块中特征图的通道数目保持不变。</strong><br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/satellite/hourglass.png" alt="hourglass"></p>
<h5 id="residual-模块"><a href="#residual-模块" class="headerlink" title="residual 模块"></a>residual 模块</h5><p>一个 residual 模块如下图所示，residual 模块的 upper branch 是一个卷积模块 conv block，卷积模块首先进行一次\( 1 \times 1 \) 的卷积操作，缩减特征图的通道数目，然后进行一次 \( 3 \times 3 \)的卷积操作，最后再进行一次\( 1 \times 1 \)的卷积操作。residual 模块的 lower branch 是一个 skip layer，是一个跳级结构，在 skip layer 的输入与 conv block 的输出的通道数目不一致时，skip layer 会进行必要的 \( 1 \times 1 \) 卷积操作，使得其输出与 conv block 的输出通道数目一致，从而能够进行 element-wise 的相加。<br>一个 residual 模块可以看成是一个尺寸保持、通道数目可变可不变的“卷积层”。<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/satellite/residual.png" alt="residual"></p>
<h5 id="intermediate-supervision"><a href="#intermediate-supervision" class="headerlink" title="intermediate supervision"></a>intermediate supervision</h5><p>中间监督（intermediate supervision）的结构如下图所示，网络进行分支，并产生图中蓝色所示的热图（heatmap），热图可以被用来计算损失。在热图上应用一次 \( 1 \times 1\) 卷积操作，增加通道数目使得其可以与网络的中间特征图进行相加。图中虚线是来自上一个 hourglass 模块输出的特征图，这里也体现了跳级（skip layer）的思想。</p>
<p>跳级结构一方面可以看成是一个梯度反向传播的高速通道，另一方面也是网络正则化的一个替代，允许训练过程中神经网络自主选择其深度（结构复杂度）。<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/satellite/intermediate_supervision.png" alt="intermediate_supervision"></p>
<h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><p>对于 \( H \times W  \times N \)<br>的输入，每一个 hourglass 模块都会生成一个 \( \frac{H}{2}  \times \frac{W}{2} \times K\)<br>的热图。对于每个热图，都比较其与真值（groundtruth）的误差作为损失，体现了中间监督(intermediate supervision)的思想。</p>
<h5 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h5><p>在做预测时，仅选择最后一个 hourglass 模块输出的热图作为结果。</p>
<h3 id="关键代码"><a href="#关键代码" class="headerlink" title="关键代码"></a>关键代码</h3><h5 id="residual-模块-1"><a href="#residual-模块-1" class="headerlink" title="residual 模块"></a>residual 模块</h5><h6 id="conv-block"><a href="#conv-block" class="headerlink" title="conv block"></a>conv block</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span><span class="params">(inputs, num_output)</span>:</span></div><div class="line">    <span class="string">""" Convolutional Block</span></div><div class="line"><span class="string">    Args:</span></div><div class="line"><span class="string">        inputs  : Input Tensor (Not after being activated!!!!!)</span></div><div class="line"><span class="string">        num_output  : Desired output number of channel</span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">        output  : Output Tensor</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    x = BatchNormalization()(inputs)</div><div class="line">    x = Activation(<span class="string">'relu'</span>)(x)</div><div class="line">    x = Conv2D(num_output // <span class="number">2</span>, (<span class="number">1</span>, <span class="number">1</span>), strides=<span class="number">1</span>, padding=<span class="string">'valid'</span>)(x)</div><div class="line"></div><div class="line">    x = BatchNormalization()(x)</div><div class="line">    x = Activation(<span class="string">'relu'</span>)(x)</div><div class="line">    x = Conv2D(num_output // <span class="number">2</span>, (<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">1</span>, padding=<span class="string">'same'</span>)(x)</div><div class="line"></div><div class="line">    x = BatchNormalization()(x)</div><div class="line">    x = Activation(<span class="string">'relu'</span>)(x)</div><div class="line">    output = Conv2D(num_output, (<span class="number">1</span>, <span class="number">1</span>), strides=<span class="number">1</span>, padding=<span class="string">'valid'</span>)(x)</div><div class="line">    <span class="keyword">return</span> output</div></pre></td></tr></table></figure>
<h6 id="skip-layer"><a href="#skip-layer" class="headerlink" title="skip layer"></a>skip layer</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">skip_layer</span><span class="params">(inputs, num_output)</span>:</span></div><div class="line">    <span class="string">""" Skip Layer</span></div><div class="line"><span class="string">    Args:</span></div><div class="line"><span class="string">        inputs  : Input Tensor</span></div><div class="line"><span class="string">        num_output  : Desired output number of channel</span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">        Tensor of shape (None, inputs.height, inputs.width, num_output)</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="keyword">if</span> inputs.get_shape().as_list()[<span class="number">3</span>] == num_output:</div><div class="line">        <span class="keyword">return</span> inputs</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        x = BatchNormalization()(inputs)</div><div class="line">        x = Activation(<span class="string">'relu'</span>)(x)</div><div class="line">        output = Conv2D(num_output, (<span class="number">1</span>, <span class="number">1</span>), strides=<span class="number">1</span>, padding=<span class="string">'valid'</span>)(x)</div><div class="line">        <span class="keyword">return</span> output</div></pre></td></tr></table></figure>
<h6 id="residual"><a href="#residual" class="headerlink" title="residual"></a>residual</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">residual</span><span class="params">(inputs, num_output)</span>:</span></div><div class="line">    <span class="string">""" Residual Unit</span></div><div class="line"><span class="string">    Args:</span></div><div class="line"><span class="string">        inputs  : Input Tensor</span></div><div class="line"><span class="string">        num_output  : Number of Output Features (channels)</span></div><div class="line"><span class="string">    """</span></div><div class="line">    convb = conv_block(inputs, num_output)</div><div class="line">    skipl = skip_layer(inputs, num_output)</div><div class="line">    <span class="keyword">return</span> Add()([convb, skipl])</div></pre></td></tr></table></figure>
<h5 id="hourglass-模块-1"><a href="#hourglass-模块-1" class="headerlink" title="hourglass 模块"></a>hourglass 模块</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">hourglass</span><span class="params">(inputs, num_low, num_output)</span>:</span></div><div class="line">    <span class="string">""" Hourglass Module</span></div><div class="line"><span class="string">    Args:</span></div><div class="line"><span class="string">        inputs  : Input Tensor</span></div><div class="line"><span class="string">        num_low       : Number of downsampling step</span></div><div class="line"><span class="string">        num_output  : Number of Output Features (channels)</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="comment"># Upper Branch</span></div><div class="line">    up_1 = residual(inputs, num_output)</div><div class="line"></div><div class="line">    <span class="comment"># Lower Branch</span></div><div class="line">    low_ = MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))(inputs)</div><div class="line">    low_1= residual(low_, num_output)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> num_low &gt; <span class="number">0</span>:</div><div class="line">        low_2 = hourglass(low_1, num_low<span class="number">-1</span>, num_output)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        low_2 = residual(low_1, num_output)</div><div class="line"></div><div class="line">    low_3 = residual(low_2, num_output)</div><div class="line"></div><div class="line">    up_2 = UpSampling2D(size=(<span class="number">2</span>, <span class="number">2</span>))(low_3)</div><div class="line">    <span class="keyword">return</span> Add()([up_2, up_1])</div></pre></td></tr></table></figure>
<h5 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h5><p>在使用 Keras 对损失函数进行实现的时候，需要注意的几点是：</p>
<ul>
<li>定义的 Python 损失函数需要传入模型的 <code>fit</code> 或者 <code>fit_generator</code> 方法，这两个方法会在内部调用该函数并为其传入训练数据中的标签和神经网络的输出，这两个参数是损失函数计算损失的唯二参数，此处由于使用中间监督的模型结构，即网络中间产生的热图也需要参与到损失值的计算，所以需要将所有这些热图都 stack 起来当作网络的输出，从而可以被传入损失函数完成损失值的计算。</li>
<li>Keras 中定义 loss，返回的是 batch_size 长度的 tensor， 而不是像 tensorflow 中那样是一个 scalar。</li>
<li><code>K.binary_crossentropy</code> 的默认设置是 <code>from_logits=False</code>,即该函数的默认设置假设输入是经过 <code>sigmoid</code> 激活之后的值，而不是仅经过卷积操作之后的<code>logits</code>，这是由其需要注意的一点，<code>K.binary_crossentropy</code> 的源码如下，如果<code>from_logits==True</code>，则直接调用 TensorFlow 的 <code>tf.nn.sigmoid_cross_entropy_with_logits</code>；如果<code>from_logits==False</code>，则首先将输入转换回 logits，再调用<code>tf.nn.sigmoid_cross_entropy_with_logits</code>。如果使用错误，就会 logits 就会被 <code>clip_by_value</code> 裁剪到（0，1）之间。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_crossentropy</span><span class="params">(target, output, from_logits=False)</span>:</span></div><div class="line">    <span class="string">"""Binary crossentropy between an output tensor and a target tensor.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    # Arguments</span></div><div class="line"><span class="string">        target: A tensor with the same shape as `output`.</span></div><div class="line"><span class="string">        output: A tensor.</span></div><div class="line"><span class="string">        from_logits: Whether `output` is expected to be a logits tensor.</span></div><div class="line"><span class="string">            By default, we consider that `output`</span></div><div class="line"><span class="string">            encodes a probability distribution.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    # Returns</span></div><div class="line"><span class="string">        A tensor.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="comment"># Note: tf.nn.sigmoid_cross_entropy_with_logits</span></div><div class="line">    <span class="comment"># expects logits, Keras expects probabilities.</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> from_logits:</div><div class="line">        <span class="comment"># transform back to logits</span></div><div class="line">        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)</div><div class="line">        output = tf.clip_by_value(output, _epsilon, <span class="number">1</span> - _epsilon)</div><div class="line">        output = tf.log(output / (<span class="number">1</span> - output))</div><div class="line"></div><div class="line">    <span class="keyword">return</span> tf.nn.sigmoid_cross_entropy_with_logits(labels=target,</div><div class="line">                                                   logits=output)</div></pre></td></tr></table></figure>
<p>定义的损失函数如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(y_true, y_pred)</span>:</span></div><div class="line">	<span class="string">"""</span></div><div class="line"><span class="string">	# Arguments</span></div><div class="line"><span class="string">		y_true: A tensor of shape (batch_size, W, H, K), K is the num of heatmaps.</span></div><div class="line"><span class="string">		y_pred: A tensor of shape (batch_size, N, W, H, K), N is the num of hourglass modules. </span></div><div class="line"><span class="string">	"""</span></div><div class="line">    losses = []</div><div class="line">    shapes = y_pred.get_shape().as_list()</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(shapes[<span class="number">1</span>]):</div><div class="line">        losses.append(K.reshape(</div><div class="line">            K.mean(K.binary_crossentropy(y_true, y_pred[:, i], from_logits=<span class="keyword">True</span>), axis=<span class="number">-1</span>), </div><div class="line">            (<span class="number">-1</span>, shapes[<span class="number">2</span>]*shapes[<span class="number">3</span>])))</div><div class="line">    </div><div class="line">    loss = K.mean(K.mean(K.stack(losses, axis=<span class="number">1</span>), axis=<span class="number">1</span>), axis=<span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> loss</div></pre></td></tr></table></figure></p>
<h3 id="数据集及结果"><a href="#数据集及结果" class="headerlink" title="数据集及结果"></a>数据集及结果</h3><h5 id="发动机和对接环的定位（关键点检测任务）"><a href="#发动机和对接环的定位（关键点检测任务）" class="headerlink" title="发动机和对接环的定位（关键点检测任务）"></a>发动机和对接环的定位（关键点检测任务）</h5><h6 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h6><p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/satellite/satellite_key_pts.png" alt="satellite_key_pts"></p>
<h6 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h6><p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/satellite/satellite_key_pts_ret.png" alt="satellite_key_pts_ret"></p>
<h5 id="发动机和对接环的检测定位（类似图像分割任务）"><a href="#发动机和对接环的检测定位（类似图像分割任务）" class="headerlink" title="发动机和对接环的检测定位（类似图像分割任务）"></a>发动机和对接环的检测定位（类似图像分割任务）</h5><h6 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h6><p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/satellite/satellite.png" alt="satellite"></p>
<h6 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h6><p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/satellite/satellite_ret.png" alt="satellite_ret"></p>
<h5 id="太阳帆板的检测定位（类似图像分割任务）"><a href="#太阳帆板的检测定位（类似图像分割任务）" class="headerlink" title="太阳帆板的检测定位（类似图像分割任务）"></a>太阳帆板的检测定位（类似图像分割任务）</h5><h6 id="数据集-2"><a href="#数据集-2" class="headerlink" title="数据集"></a>数据集</h6><p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/satellite/satellite_sol_pan.png" alt="satellite_sol_pan"></p>
<h6 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h6><p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/satellite/satellite_sol_pan_ret.png" alt="satellite_sol_pan_ret"></p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/07/03/Run-Keras-models-in-C-Tensorflow/">
                Run Keras models in C++ Tensorflow
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-07-03</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/07/03/策略梯度：倒立摆/">
                策略梯度：倒立摆
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-07-03</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/07/03/Advanced-machine-learning-with-scikit-learn/">
                Advanced machine learning with scikit-learn
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-07-03</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/07/03/Machine-learning-with-scikit-learn/">
                Machine learning with scikit-learn
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-07-03</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/07/02/Softmax-cross-entropy-推导及求导/">
                Softmax cross entropy 推导及求导
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-07-02</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p>主要分析了 binary_crossentropy 和 categorical_crossentropy 的定义， softmax 和 categorical_crossentropy 的求导。</p>
<h3 id="binary-crossentropy"><a href="#binary-crossentropy" class="headerlink" title="binary_crossentropy"></a>binary_crossentropy</h3><p>适用于每个类别相互独立但互不排斥的情况，常见于单类别任务\( (batch size, 1) \)和多类别中的多标签任务\( (batch size, num classes) \)。</p>
<p>\[<br>\begin{split}<br>    p_{i, j} &amp; = sigmoid(logits_{i, j}) \\\\<br>    &amp; = \frac{1}{1 + e^{-logits_{i, j} } } \\\\<br>    loss_{i, j} &amp; = -[y_{i, j} \times ln p_{i, j} + (1 - y_{i, j}) \times (1 - ln p_{i, j})]<br>\end{split}<br>\]<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_crossentropy</span><span class="params">(target, output, from_logits=False)</span>:</span></div><div class="line">    <span class="string">"""Binary crossentropy between an output tensor and a target tensor.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    # Arguments</span></div><div class="line"><span class="string">        target: A tensor with the same shape as `output`.</span></div><div class="line"><span class="string">        output: A tensor.</span></div><div class="line"><span class="string">        from_logits: Whether `output` is expected to be a logits tensor.</span></div><div class="line"><span class="string">            By default, we consider that `output`</span></div><div class="line"><span class="string">            encodes a probability distribution.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    # Returns</span></div><div class="line"><span class="string">        A tensor.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="comment"># Note: tf.nn.sigmoid_cross_entropy_with_logits</span></div><div class="line">    <span class="comment"># expects logits, Keras expects probabilities.</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> from_logits:</div><div class="line">        <span class="comment"># transform back to logits</span></div><div class="line">        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)</div><div class="line">        output = tf.clip_by_value(output, _epsilon, <span class="number">1</span> - _epsilon)</div><div class="line">        output = tf.log(output / (<span class="number">1</span> - output))</div><div class="line"></div><div class="line">    <span class="keyword">return</span> tf.nn.sigmoid_cross_entropy_with_logits(labels=target,</div><div class="line">                                                   logits=output)</div></pre></td></tr></table></figure></p>
<h3 id="categorical-crossentropy"><a href="#categorical-crossentropy" class="headerlink" title="categorical_crossentropy"></a>categorical_crossentropy</h3><p>适用于每个类别相互独立且排斥的情况（onehot），即多类别中的单标签任务\( (batch size, num classes) \)。<br>\[<br>\begin{split}<br>    p_{i, j} &amp; = \frac{e^{logits_{i, j} } }{\sum_{j=0}^{num classes - 1} e^{logits_{i, j} } } \\\\<br>    loss_i &amp; = - \sum_{j=0}^{num  classes - 1} y_{i, j} ln p_{i, j}<br>\end{split}<br>\]<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">categorical_crossentropy</span><span class="params">(target, output, from_logits=False, axis=<span class="number">-1</span>)</span>:</span></div><div class="line">    <span class="string">"""Categorical crossentropy between an output tensor and a target tensor.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    # Arguments</span></div><div class="line"><span class="string">        target: A tensor of the same shape as `output`.</span></div><div class="line"><span class="string">        output: A tensor resulting from a softmax</span></div><div class="line"><span class="string">            (unless `from_logits` is True, in which</span></div><div class="line"><span class="string">            case `output` is expected to be the logits).</span></div><div class="line"><span class="string">        from_logits: Boolean, whether `output` is the</span></div><div class="line"><span class="string">            result of a softmax, or is a tensor of logits.</span></div><div class="line"><span class="string">        axis: Int specifying the channels axis. `axis=-1`</span></div><div class="line"><span class="string">            corresponds to data format `channels_last`,</span></div><div class="line"><span class="string">            and `axis=1` corresponds to data format</span></div><div class="line"><span class="string">            `channels_first`.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    # Returns</span></div><div class="line"><span class="string">        Output tensor.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    # Raises</span></div><div class="line"><span class="string">        ValueError: if `axis` is neither -1 nor one of</span></div><div class="line"><span class="string">            the axes of `output`.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    output_dimensions = list(range(len(output.get_shape())))</div><div class="line">    <span class="keyword">if</span> axis != <span class="number">-1</span> <span class="keyword">and</span> axis <span class="keyword">not</span> <span class="keyword">in</span> output_dimensions:</div><div class="line">        <span class="keyword">raise</span> ValueError(</div><div class="line">            <span class="string">'&#123;&#125;&#123;&#125;&#123;&#125;'</span>.format(</div><div class="line">                <span class="string">'Unexpected channels axis &#123;&#125;. '</span>.format(axis),</div><div class="line">                <span class="string">'Expected to be -1 or one of the axes of `output`, '</span>,</div><div class="line">                <span class="string">'which has &#123;&#125; dimensions.'</span>.format(len(output.get_shape()))))</div><div class="line">    <span class="comment"># Note: tf.nn.softmax_cross_entropy_with_logits</span></div><div class="line">    <span class="comment"># expects logits, Keras expects probabilities.</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> from_logits:</div><div class="line">        <span class="comment"># scale preds so that the class probas of each sample sum to 1</span></div><div class="line">        output /= tf.reduce_sum(output, axis, <span class="keyword">True</span>)</div><div class="line">        <span class="comment"># manual computation of crossentropy</span></div><div class="line">        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)</div><div class="line">        output = tf.clip_by_value(output, _epsilon, <span class="number">1.</span> - _epsilon)</div><div class="line">        <span class="keyword">return</span> - tf.reduce_sum(target * tf.log(output), axis)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> tf.nn.softmax_cross_entropy_with_logits(labels=target,</div><div class="line">                                                       logits=output)</div></pre></td></tr></table></figure></p>
<h3 id="weighted-cross-entropy"><a href="#weighted-cross-entropy" class="headerlink" title="weighted_cross_entropy"></a>weighted_cross_entropy</h3><p>\[<br>\begin{split}<br>    p_{i, j} &amp; = sigmoid(logits_{i, j}) \\\\<br>    &amp; = \frac{1}{1 + e^{-logits_{i, j} } } \\\\<br>\end{split}<br>\]</p>
<p>\[<br>loss_{i, j} = -[ pos\_weight \times y_{i, j} \times ln p_{i, j} + (1 - y_{i, j}) \times (1 - ln p_{i, j})]<br>\]<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.nn.weighted_cross_entropy_with_logits(labels,logits, pos_weight, name=<span class="keyword">None</span>)</div></pre></td></tr></table></figure></p>
<h3 id="softmax-求导"><a href="#softmax-求导" class="headerlink" title="softmax 求导"></a>softmax 求导</h3><p>\[<br>\begin{split}<br>    y_i &amp; = \frac{e^{x_i} }{\sum_{j=0}^{m-1} e^{x_j} } \\\\<br>    \frac{\partial y_i}{\partial x_k} &amp; = \frac{\partial \frac{e^{x_i} }{\sum_{j=0}^{m-1} e^{x_j} } }{\partial x_k} \\\\<br>    &amp; = \frac{\frac{\partial e^{x_i} }{\partial x_k} \times \sum - e^{x_i} \times \frac{\partial \sum}{\partial x_k} }{ {\sum}^2 } \\\\<br>    &amp; = \frac{\frac{\partial e^{x_i} }{\partial x_k} \times \sum - e^{x_i} \times e^{x_k} }{ {\sum}^2 } \\\\<br>    &amp; = \begin{cases}<br>        \frac{e^{x_k} \times \sum - e^{x_i} \times e^{x_k} }{ {\sum}^2 }, \text{ if } i = k \\\\<br>        \frac{ - e^{x_i} \times e^{x_k} }{ {\sum}^2 }, \text{ if } i \neq k<br>    \end{cases} \\\\<br>    &amp; = \begin{cases}<br>    y_k(1 - y_i), \text{ if } i = k\\\\<br>    -y_k y_i, \text{ if } i \neq k<br>    \end{cases}<br>\end{split}<br>\]</p>
<p>\[<br>    \mathbf{y} = softmax(\mathbf{x})<br>\]</p>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/softmax/softmax.png" alt="softmax"></p>
<p>雅可比矩阵<br>\[<br>Jacobian_{\mathbf{y} }(\mathbf{x}) =<br>\left[<br> \begin{matrix}<br>   \frac{\partial y_0}{\partial x_0} &amp; \frac{\partial y_0}{\partial x_1} &amp; \dots &amp; \frac{\partial y_0}{\partial x_{m-1} } \\\\<br>   \vdots &amp;  \vdots &amp; \ddots &amp; \vdots \\\\<br>   \frac{\partial y_{m-1} }{\partial x_0} &amp; \frac{\partial y_{m-1} }{\partial x_1} &amp; \dots &amp; \frac{\partial y_{m-1}}{\partial x_{m-1} }<br>  \end{matrix}<br>  \right]<br>\]</p>
<h3 id="categorical-crossentropy-求导"><a href="#categorical-crossentropy-求导" class="headerlink" title="categorical_crossentropy 求导"></a>categorical_crossentropy 求导</h3><p>\[<br>\begin{split}<br>    \frac{\partial loss}{\partial logits_k} &amp; = \frac{\partial {- \sum_{j=0}^{num  classes - 1} y_{j} ln p_{j} } }{\partial logits_k} \\\\<br>    &amp; = - \sum_{j=0}^{num classes - 1} y_{j}  \frac{\partial ln p_{j} }{\partial logits_k} \\\\<br>    &amp; = - \sum_{j=0}^{num classes - 1} y_{j}  \frac{1}{p_{j} } \frac{\partial p_{j} }{\partial logits_k} \\\\<br>    &amp; = - \sum_{j=0}^{num classes - 1} y_{j}  \frac{1}{p_{j} } \begin{cases}<br>                                                            p_k (1 - p_j), \text{ if } j = k \\\\<br>                                                            -p_j p_k, \text{ if } j \neq k<br>                                                            \end{cases} \\\\<br>    &amp; = - y_{k}(1 - p_k) - \sum_{j=0, j \neq k}^{num classes - 1} y_{j} (-p_k) \\\\<br>    &amp; = - y_{k} + y_{k} p_k + \sum_{j=0, j \neq k}^{num classes - 1} y_{j} p_k \\\\<br>    &amp; = - y_{k} + p_k \sum_{j=0}^{num classes - 1} y_{j} \\\\<br>    &amp; = p_k - y_{k}<br>\end{split}<br>\]<br>可以看出 categorical_crossentropy 的导数很简洁，即预测概率与真实概率的差。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">tf.nn.softmax_cross_entropy_with_logits(</div><div class="line">    _sentinel=<span class="keyword">None</span>,</div><div class="line">    labels=<span class="keyword">None</span>,</div><div class="line">    logits=<span class="keyword">None</span>,</div><div class="line">    dim=<span class="number">-1</span>,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure></p>
<blockquote>
<p>WARNING: This op expects unscaled logits, since it performs a softmax on logits internally for efficiency. Do not call this op with the output of softmax, as it will produce incorrect results.</p>
<p>Backpropagation will happen only into logits. To calculate a cross entropy loss that allows backpropagation into both logits and labels, see tf.nn.softmax_cross_entropy_with_logits_v2.</p>
</blockquote>
<p>相较于相继调用 <code>softmax</code> 和 <code>cross_entropy</code> （正向传播，反向传播），<code>softmax_cross_entropy_with_logits</code> 的反向传播速度更快，原因就是可以直接使用上面的的推导结果\( \frac{\partial loss}{\partial logits_k} =  p_k - y_{k} \) 简单快速地求得 <code>logits</code> 上的梯度，并在此基础上继续反向传播，而不必对 <code>cross_entropy</code> 和 <code>softmax</code> 依次反向传播之后才得到 <code>logits</code> 上的梯度\( \sum_{j=0}^{m-1} \frac{\partial loss}{\partial p_j} \frac{\partial p_j}{\partial logits_k} \)[upstream gradients local gradients]。将这两个操作合为一个操作，正向传播速度一样，反向传播实现了加速。</p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/07/01/模型评估-量化预测质量/">
                模型评估 量化预测质量
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-07-01</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p>本博文主要是在阅读 scikit-learn 官方文档<a href="http://scikit-learn.org/stable/modules/model_evaluation.html" target="_blank" rel="external">3.3. Model evaluation: quantifying the quality of predictions</a> 之后的总结。<br>首先看一下根据 label 特点的不同做出的分类任务类型的划分：<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/metrics/1.PNG" alt="不同的分类任务"><br>scikit-learn 提供了三种不同的 API 来评估模型的预测质量。</p>
<ul>
<li>Estimator score method（学习器的score 方法）</li>
<li>Scoring parameter （评分策略参数） 基于交叉验证的模型评估工具（比如 <code>model_selection.cross_val_score</code> and <code>model_selection.GridSearchCV</code>) 依赖于一个内部的评分策略 ）</li>
<li>Metric functions（评价函数），metrics 模块提供了很多这样的函数。</li>
</ul>
<p>另外一点值得指出的是，可以使用 <code>Dummy estimators</code> 获得评价指标的基准值。</p>
<h2 id="使用-scoring-参数来定义模型评估的规则"><a href="#使用-scoring-参数来定义模型评估的规则" class="headerlink" title="使用 scoring 参数来定义模型评估的规则"></a>使用 scoring 参数来定义模型评估的规则</h2><p>Model selection and evaluation using tools, such as <code>model_selection.GridSearchCV</code> and <code>model_selection.cross_val_score</code>, take a scoring parameter that controls what metric they apply to the estimators evaluated.<br>值得注意的是，所有的评分指标都遵循“大值优于小值”的约定。因此对于一些基于“距离”的度量，像 <code>metrics.mean_squared_error</code>，在此使用的是其相反数 <code>neg_mean_squared_error</code>。<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/metrics/scoring_parameters.PNG" alt="scoring_parameters"></p>
<h2 id="分类问题的评估指标"><a href="#分类问题的评估指标" class="headerlink" title="分类问题的评估指标"></a>分类问题的评估指标</h2><p><code>sklearn.metrics</code> 模块提供了很多的损失（loss），评分（score）和辅助函数（utility）来度量分类器的性能。<br>一些指标函数需要分类器能够给出概率估计、confidence values, 和 binary decisions values。并且大部分的实现也允许通过提供 <code>sample_weights</code> 参数来计算指标的一个加权值。<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/metrics/classification_metrics.PNG" alt="classification_metrics"></p>
<p>下面关注一下一些常见的指标。</p>
<h4 id="Accuracy-score"><a href="#Accuracy-score" class="headerlink" title="Accuracy score"></a>Accuracy score</h4><p>\(\hat{y_i}\) 是第 i 个样本的预测值，\(y_i\) 是第 i 个样本的真实值。<br>$$\texttt{accuracy}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} 1(\hat{y}_i = y_i)$$<br>其中 \(1(x)\) 是示性函数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">from sklearn.metrics import accuracy_score</div><div class="line">y_pred = [0, 2, 1, 3]</div><div class="line">y_true = [0, 1, 2, 3]</div><div class="line">accuracy_score(y_true, y_pred)</div><div class="line">0.5</div><div class="line">accuracy_score(y_true, y_pred, normalize=False)</div><div class="line">2</div></pre></td></tr></table></figure></p>
<p>在多标签（multilabel）任务下，使用二进制标签矩阵（binary label indicators），示性函数需要\(\hat{y}_i\) 和 \(y_i\) 完全匹配才取 1 值。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))</div><div class="line">0.5</div></pre></td></tr></table></figure></p>
<h4 id="Classification-report"><a href="#Classification-report" class="headerlink" title="Classification report"></a>Classification report</h4><p>这个函数会构建一个展示常见分类度量指标的文本报告（text report）。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">from sklearn.metrics import classification_report</div><div class="line">y_true = [0, 1, 2, 2, 0]</div><div class="line">y_pred = [0, 0, 2, 1, 0]</div><div class="line">target_names = [&apos;class 0&apos;, &apos;class 1&apos;, &apos;class 2&apos;]</div><div class="line">print(classification_report(y_true, y_pred, target_names=target_names))</div><div class="line">             precision    recall  f1-score   support</div><div class="line"></div><div class="line">    class 0       0.67      1.00      0.80         2</div><div class="line">    class 1       0.00      0.00      0.00         1</div><div class="line">    class 2       1.00      0.50      0.67         2</div><div class="line"></div><div class="line">avg / total       0.67      0.60      0.59         5</div></pre></td></tr></table></figure></p>
<h4 id="Precision-recall-and-F-measures"><a href="#Precision-recall-and-F-measures" class="headerlink" title="Precision, recall and F-measures"></a>Precision, recall and F-measures</h4><p>对二分类问题而言，<br>$$<br>\text{precision} = \frac{tp}{tp + fp},<br>$$<br>$$<br>\text{recall} = \frac{tp}{tp + fn},<br>$$<br>$$<br>F_\beta = (1 + \beta^2) \frac{\text{precision} \times \text{recall}}{\beta^2 \text{precision} + \text{recall}}.<br>$$<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/metrics/recall_precision_f_beta.PNG" alt="recall_precision_f_beta"><br>对多类别和多标签分类而言，precision, recall, and F-measures 可以独立地应用于各个标签，然后可以通过 <code>average</code> 参数指定的方式将各个标签上的结果进行综合。 <code>average</code> 参数可以应用于 <code>average_precision_score</code> (multilabel only), <code>f1_score</code>, <code>fbeta_score</code>, <code>precision_recall_fscore_support</code>, <code>precision_score</code> and <code>recall_score</code> 这些函数。<br><em>Note that for “micro”-averaging in a multiclass setting with all labels included will produce equal precision, recall and F, while “weighted” averaging may produce an F-score that is not between precision and recall.</em></p>
<ul>
<li>\(y\) the set of predicted (sample, label) pairs</li>
<li>\(\hat{y}\) the set of true (sample, label) pairs</li>
<li>\(L\) the set of labels</li>
<li>\(S\) the set of samples</li>
<li>\(y_s\) the subset of \(y\) with sample \(s\), i.e. \( y_s := \lbrace (s’, l) \in y | s = s’ \rbrace \)</li>
<li>\(y_l\) the subset of \(y\) with label \(l\)</li>
<li>similarly, \(\hat{y_s}\) and \(\hat{y_l}\) are subsets of \(\hat{y}\)</li>
<li>\(P(A, B) := \frac{\left| A \cap B \right|}{\left|A\right|}\)</li>
<li>\(R(A, B) := \frac{\left| A \cap B \right|}{\left|B\right|}\) (Conventions vary on handling \(B = \emptyset\); this implementation uses \(R(A, B):=0\), and similar for \(P\).)</li>
<li>\(F_\beta(A, B) := \left(1 + \beta^2\right) \frac{P(A, B) \times R(A, B)}{\beta^2 P(A, B) + R(A, B)}\)<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/metrics/2.PNG" alt="多标签任务的真实标签"><br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/metrics/3.PNG" alt="多标签任务的预测标签"><br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/metrics/4.PNG" alt="多标签任务指标的计算图示"><br>Then the metrics are defined as:<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/metrics/recall_precision_f_beta_formulations.PNG" alt="recall_precision_f_beta_formulations"></li>
</ul>
<h4 id="precision-recall-curve"><a href="#precision-recall-curve" class="headerlink" title="precision_recall_curve"></a>precision_recall_curve</h4><p>The <code>precision_recall_curve</code> computes a precision-recall curve from the ground truth label and a score given by the classifier by varying a decision threshold.<br><strong>仅仅能够应用在二分类问题中。</strong></p>
<h4 id="average-precision-score"><a href="#average-precision-score" class="headerlink" title="average_precision_score"></a>average_precision_score</h4><p>The <code>average_precision_score</code> function computes the average precision (AP) from prediction scores. The value is between 0 and 1 and higher is better. AP is defined as<br>$$AP = \sum_n (R_n - R_{n-1}) P_n$$<br>where \(P_n\) and \(R_n\) are the precision and recall at the nth threshold. With random predictions, the AP is the fraction of positive samples.<br><strong>仅能够应用在二分类问题和多标签问题中。</strong></p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/06/08/笔记-for-PyData-2015-Introduction-to-scikit-image/">
                笔记 for PyData 2015 - Introduction to scikit-image
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-06-08</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p>这篇博文是 Emmanuelle Gouillart (scikit-image 开发者) 在 PyData 2015 上演讲 Introduction to scikit-image 的笔记。</p>
<h3 id="scikit-image"><a href="#scikit-image" class="headerlink" title="scikit-image"></a>scikit-image</h3><p>scikit-image is a general-purpose image processing module for the Python programming language. It is designed to interact efficiently with other popular scientific Python libraries, such as NumPy and SciPy. In particular, scikit-image leverages the powerful data array container of NumPy, that can store images of various dimensions (2-D, 2D RGB, 3D, 4D…).</p>
<h3 id="Foundation"><a href="#Foundation" class="headerlink" title="Foundation"></a>Foundation</h3><p>NumPy-native: images as NumPy arrays</p>
<h3 id="Filtering：-transforming-image-data"><a href="#Filtering：-transforming-image-data" class="headerlink" title="Filtering： transforming image data"></a>Filtering： transforming image data</h3><p>modules:</p>
<ul>
<li>skimage.filter</li>
<li>skimage.exposure</li>
<li>skimage.restoration</li>
</ul>
<p>word cloud:</p>
<ul>
<li>median</li>
<li>gaussian</li>
<li>canny</li>
<li>sobel</li>
<li>wiener</li>
<li>equalize</li>
<li>denoising</li>
<li>enhance_contrast</li>
<li>total_variation</li>
</ul>
<h3 id="Extracting-features"><a href="#Extracting-features" class="headerlink" title="Extracting features"></a>Extracting features</h3><p>Feature detection: extracting features from images to feed into classifiers or estimators.<br>modules:</p>
<ul>
<li>skimage.feature</li>
<li>skimage.filter</li>
</ul>
<p>word cloud:</p>
<ul>
<li>corners</li>
<li>daisy</li>
<li>local_maxima</li>
<li>gabor</li>
<li>harris</li>
<li>hog</li>
<li>hough</li>
<li>cooccurance</li>
<li>canny</li>
</ul>
<h3 id="Geometrical-transformations"><a href="#Geometrical-transformations" class="headerlink" title="Geometrical transformations"></a>Geometrical transformations</h3><p>modules:</p>
<ul>
<li>skimage.transform</li>
</ul>
<p>word cloud</p>
<ul>
<li>scale</li>
<li>zoom</li>
<li>rotate</li>
<li>swirl</li>
<li>wrap</li>
</ul>
<h3 id="Segmentation-labelling-regions"><a href="#Segmentation-labelling-regions" class="headerlink" title="Segmentation: labelling regions"></a>Segmentation: labelling regions</h3><p>label different pixels to separate objects.<br>modules:</p>
<ul>
<li>skimage.segmentation</li>
</ul>
<p>word cloud:</p>
<ul>
<li>thresholding: histogram based</li>
<li>otsu</li>
<li>randomwalker</li>
<li>superpixel: separating connected objects</li>
<li>watershed</li>
</ul>
<h3 id="Measures-on-images"><a href="#Measures-on-images" class="headerlink" title="Measures on images"></a>Measures on images</h3><p>After image segmentation, you have some regions, you may want to compute statistics and label each regions.<br>modules:<br>skimage.measure</p>
<p>word cloud:</p>
<ul>
<li>measure</li>
<li>label: very useful, give different labels to different connected components.</li>
<li>size</li>
<li>histogram</li>
<li>regionprops</li>
</ul>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/skimage/skimage_label.png" alt="skimage.measure.label"></p>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/06/07/手工-Backpropagation-推导与-TensorFlow-automatic-differentiation/">
                手工 Backpropagation 推导与 TensorFlow automatic differentiation
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-06-07</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h3 id="主要对比梯度反向传播手工推导与TensorFLow-自动差分"><a href="#主要对比梯度反向传播手工推导与TensorFLow-自动差分" class="headerlink" title="主要对比梯度反向传播手工推导与TensorFLow 自动差分"></a>主要对比梯度反向传播手工推导与TensorFLow 自动差分</h3><p>关于梯度反向传播手工推导的学习可以参考斯坦福大学的 <a href="http://cs231n.stanford.edu/" target="_blank" rel="external">CS231n: Convolutional Neural Networks for Visual Recognition</a> 的 Lecture 4。<br>下面通过代码对这两者进行对比，代码的Jupyter NoteBook 可以在我的 <a href="https://github.com/RobertLexis/TensorFlow-automatic-differentiation" target="_blank" rel="external">GitHub 仓库</a>中找到。</p>
<h4 id="imports"><a href="#imports" class="headerlink" title="imports"></a>imports</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">import os</div><div class="line">os.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;]=&apos;3&apos;</div><div class="line">import tensorflow as tf</div></pre></td></tr></table></figure>
<h4 id="定义变量"><a href="#定义变量" class="headerlink" title="定义变量"></a>定义变量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">with tf.variable_scope(&quot;params&quot;, reuse=tf.AUTO_REUSE):</div><div class="line">    w = tf.get_variable(&quot;w&quot;, initializer=tf.constant([2.0]))</div><div class="line">    b = tf.get_variable(&quot;b&quot;, initializer=tf.constant([0.0]))</div></pre></td></tr></table></figure>
<h4 id="定义占位符"><a href="#定义占位符" class="headerlink" title="定义占位符"></a>定义占位符</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = tf.placeholder(tf.float32, shape=[None], name=&quot;x&quot;)</div><div class="line">y = tf.placeholder(tf.float32, shape=[None], name=&quot;y&quot;)</div></pre></td></tr></table></figure>
<h4 id="定义表达式"><a href="#定义表达式" class="headerlink" title="定义表达式"></a>定义表达式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">y_pred = w*x + b</div></pre></td></tr></table></figure>
<h4 id="定义代价函数"><a href="#定义代价函数" class="headerlink" title="定义代价函数"></a>定义代价函数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loss = tf.reduce_mean(tf.square(y_pred - y))</div></pre></td></tr></table></figure>
<h4 id="定义优化器"><a href="#定义优化器" class="headerlink" title="定义优化器"></a>定义优化器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)</div><div class="line">grads_and_vars = optimizer.compute_gradients(loss)</div><div class="line">train_op = optimizer.apply_gradients(grads_and_vars)</div></pre></td></tr></table></figure>
<h4 id="创建会话并初始化变量"><a href="#创建会话并初始化变量" class="headerlink" title="创建会话并初始化变量"></a>创建会话并初始化变量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">init_op = tf.global_variables_initializer()</div><div class="line"></div><div class="line">sess = tf.InteractiveSession()</div><div class="line">sess.run(init_op)</div></pre></td></tr></table></figure>
<h4 id="根据函数-y-4x-3-给出训练数据"><a href="#根据函数-y-4x-3-给出训练数据" class="headerlink" title="根据函数 y = 4x + 3 给出训练数据"></a>根据函数 y = 4x + 3 给出训练数据</h4><h4 id="对于单个数据点输入，对比-TensorFlow-automatic-differentiation-与手工推导"><a href="#对于单个数据点输入，对比-TensorFlow-automatic-differentiation-与手工推导" class="headerlink" title="对于单个数据点输入，对比 TensorFlow automatic differentiation 与手工推导"></a>对于单个数据点输入，对比 TensorFlow automatic differentiation 与手工推导</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># y_ = 4 * 2 + 3</div><div class="line">x_ = [2]</div><div class="line">y_ = [11] # y_ = 4 * 2 + 3</div></pre></td></tr></table></figure>
<h6 id="手工推导-Backpropagation-过程"><a href="#手工推导-Backpropagation-过程" class="headerlink" title="手工推导 Backpropagation 过程"></a>手工推导 Backpropagation 过程</h6><p><img src="http://oytnj8g2y.bkt.clouddn.com/backwards/backward_1.png" alt="backward_1"></p>
<h6 id="用-TensorFlow-计算-loss-及梯度"><a href="#用-TensorFlow-计算-loss-及梯度" class="headerlink" title="用 TensorFlow 计算 loss 及梯度"></a>用 TensorFlow 计算 loss 及梯度</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sess.run([loss, grads_and_vars], feed_dict=&#123;x: x_, y: y_&#125;)</div></pre></td></tr></table></figure>
<p>Output:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[49.0,</div><div class="line"> [(array([-28.], dtype=float32), array([2.], dtype=float32)),</div><div class="line">  (array([-14.], dtype=float32), array([0.], dtype=float32))]]</div></pre></td></tr></table></figure></p>
<h4 id="对于-batch-输入，对比-TensorFlow-automatic-differentiation-与手工推导"><a href="#对于-batch-输入，对比-TensorFlow-automatic-differentiation-与手工推导" class="headerlink" title="对于 batch 输入，对比 TensorFlow automatic differentiation 与手工推导"></a>对于 batch 输入，对比 TensorFlow automatic differentiation 与手工推导</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x_ = [2, 3]</div><div class="line">y_ = [11, 15]</div></pre></td></tr></table></figure>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/backwards/backward_1.png" alt="backward_1"><br><img src="http://oytnj8g2y.bkt.clouddn.com/backwards/backward_2.png" alt="backward_2"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sess.run([loss, grads_and_vars], feed_dict=&#123;x: x_, y: y_&#125;)</div></pre></td></tr></table></figure>
<p>Output:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[65.0,</div><div class="line"> [(array([-41.], dtype=float32), array([2.], dtype=float32)),</div><div class="line">  (array([-16.], dtype=float32), array([0.], dtype=float32))]]</div></pre></td></tr></table></figure></p>
<h6 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">65 == (81+49)/2</div><div class="line">-41 == (-28-54)/2</div><div class="line">-16 == (-14-18)/2</div></pre></td></tr></table></figure>
<p>可以看出 TensorFlow 计算出的 loss 是两次 loss 的均值，两个变量上的梯度也是各自两次梯度值的均值。</p>
<h4 id="以-learning-rate-为步幅对-w，b进行一次更新"><a href="#以-learning-rate-为步幅对-w，b进行一次更新" class="headerlink" title="以 learning_rate 为步幅对 w，b进行一次更新"></a>以 learning_rate 为步幅对 w，b进行一次更新</h4><h6 id="手工计算"><a href="#手工计算" class="headerlink" title="手工计算"></a>手工计算</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">2 - 0.001 * (-41) = 2.041</div><div class="line">0 - 0.001 * (-16) = 0.016</div></pre></td></tr></table></figure>
<h6 id="TensorFlow-计算结果"><a href="#TensorFlow-计算结果" class="headerlink" title="TensorFlow 计算结果"></a>TensorFlow 计算结果</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div></pre></td></tr></table></figure>
<p>Output:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[array([2.041], dtype=float32), array([0.016], dtype=float32)]</div></pre></td></tr></table></figure></p>
<h4 id="下面对模型进行训练并观察-w，b-的变化过程"><a href="#下面对模型进行训练并观察-w，b-的变化过程" class="headerlink" title="下面对模型进行训练并观察 w，b 的变化过程"></a>下面对模型进行训练并观察 w，b 的变化过程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">for _ in range(1000):</div><div class="line">    sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div><div class="line">[array([4.700393], dtype=float32), array([1.1883683], dtype=float32)]</div><div class="line"></div><div class="line"></div><div class="line">for _ in range(1000):</div><div class="line">    sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div><div class="line">[array([4.6550403], dtype=float32), array([1.3056908], dtype=float32)]</div><div class="line"></div><div class="line"></div><div class="line">for _ in range(1000):</div><div class="line">    sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div><div class="line">[array([4.6126213], dtype=float32), array([1.4154125], dtype=float32)]</div><div class="line"></div><div class="line"></div><div class="line">for _ in range(10000):</div><div class="line">    sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div><div class="line">[array([4.3136806], dtype=float32), array([2.1886632], dtype=float32)]</div><div class="line"></div><div class="line"></div><div class="line">for _ in range(10000):</div><div class="line">    sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div><div class="line">[array([4.1606426], dtype=float32), array([2.5844746], dtype=float32)]</div></pre></td></tr></table></figure>
<p>可以看出随着训练步骤的增多，w，b 逐渐逼近目标值 4， 3</p>
<h4 id="关闭会话，释放资源"><a href="#关闭会话，释放资源" class="headerlink" title="关闭会话，释放资源"></a>关闭会话，释放资源</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sess.close()</div></pre></td></tr></table></figure>
<h3 id="Caveats"><a href="#Caveats" class="headerlink" title="Caveats"></a>Caveats</h3><p>在上面我们定义的loss是如下的均值（0-d Tensor/scalar/shape ()）形式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loss = tf.reduce_mean(tf.square(y_pred - y))</div></pre></td></tr></table></figure></p>
<p>假设我们没有进行 <code>reduce_mean</code>，即 loss 是如下的形式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loss = tf.square(y_pred - y)</div></pre></td></tr></table></figure></p>
<p>当 <code>x_ = [2, 3]</code>、<code>y_ = [11, 15]</code>时，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sess.run([loss, grads_and_vars], feed_dict=&#123;x: x_, y: y_&#125;)</div></pre></td></tr></table></figure></p>
<p>输出为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[array([49., 81.], dtype=float32),</div><div class="line"> [(array([-82.], dtype=float32), array([2.], dtype=float32)),</div><div class="line">  (array([-32.], dtype=float32), array([0.], dtype=float32))]]</div></pre></td></tr></table></figure></p>
<p>有两个 loss 值分别为49 和 81，和我们手工计算出的值是一致的，而两个变量的梯度为各自的两次梯度值的加和。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">-82 == (-28) + (-54)</div><div class="line">-32 == (-14) + (-18)</div></pre></td></tr></table></figure></p>
<p>即有两个 loss，相当于更新了两次。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>Loss 仅仅是一个度量指标，其均值是对模型性能的一个更有意义的评价，是为此而求的均值，这个均值也仅仅用于模型性能的评价，其具体数值并不实际参与到反向传播更新各个参数的过程。loss 的具体数值意义不大，因为反向传播最起始的upstream gradient总是1。<br>这也就解释了我之前遇到过的一个问题，为什么单纯给优化器传入一个loss 的数值，TF 会报错，无法计算梯度，因为真正重要的不是这个末端的loss 数值，而是在计算这个末端loss 数值的过程中的每一个中间值及涉及到的操作类型（add mul max sub square ）。利用这些中间值和操作类型结合，反向进行梯度的传播。</li>
<li>对于batch 样本输入，正确的 loss 定义计算的是各个样本上loss 的均值，各个参数上的梯度也是各自在各个样本上梯度值的均值，可以这样看，各个样本单独输入求loss ，求梯度，然后再在各个样本的结果上进行平均。</li>
</ul>
<h3 id="Reminder-from-cs231n"><a href="#Reminder-from-cs231n" class="headerlink" title="Reminder from cs231n"></a>Reminder from cs231n</h3><table>
<thead>
<tr>
<th>Operation</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>add</td>
<td>distributor</td>
</tr>
<tr>
<td>max</td>
<td>router</td>
</tr>
<tr>
<td>mul</td>
<td>switcher</td>
</tr>
</tbody>
</table>

        </div>
    

</div>
            
        </section>
    </div>
</div>



    <div class="row">
        <div class="col-sm-12">
            <div class="wrap-pagination">
                <a class="" href="/">
                    <i class="fa fa-chevron-left" aria-hidden="true"></i>
                </a>
                <a class="" href="/page/3/">
                    <i class="fa fa-chevron-right" aria-hidden="true"></i>
                </a>
            </div>
        </div>
    </div>




</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This is Robert Lexis (FengCun Li). To see the world, things dangerous to come to, to see behind walls, to draw closer, to find each other and to feel. That is the purpose of LIFE.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2018/08/26/YOLO/">YOLO</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/26/非最大值抑制/">非最大值抑制</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/26/目标定位-vs-目标检测/">目标定位 vs 目标检测</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/13/消失的梯度/">消失的梯度</a>
            </li>
            
        </ul>
    </div>



            
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/RobertLexis">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:robert_lexis@163.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Robert Lexis Loves Wenny
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



</body>

</html>