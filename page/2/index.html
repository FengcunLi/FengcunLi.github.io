<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    

    <!--Author-->
    
        <meta name="author" content="Robert Lexis">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="This is Robert Lexis."/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="This is Robert Lexis."/>

    <!--Type page-->
    
        <meta property="og:type" content="website" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>page - This is Robert Lexis.</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about.html">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact.html">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
            <h1 id="main-title" class="title">This is Robert Lexis.</h1>
        
    </div>
</header>

        <section class="main">
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/07/03/策略梯度：倒立摆/">
                策略梯度：倒立摆
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-07-03</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/07/03/Advanced-machine-learning-with-scikit-learn/">
                Advanced machine learning with scikit-learn
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-07-03</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/07/03/Machine-learning-with-scikit-learn/">
                Machine learning with scikit-learn
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-07-03</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/07/02/Softmax-cross-entropy-推导及求导/">
                Softmax cross entropy 推导及求导
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-07-02</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p>主要分析了 binary_crossentropy 和 categorical_crossentropy 的定义， softmax 和 categorical_crossentropy 的求导。</p>
<h3 id="binary-crossentropy"><a href="#binary-crossentropy" class="headerlink" title="binary_crossentropy"></a>binary_crossentropy</h3><p>适用于每个类别相互独立但互不排斥的情况，常见于单类别任务\( (batch size, 1) \)和多类别中的多标签任务\( (batch size, num classes) \)。</p>
<p>\[<br>\begin{split}<br>    p_{i, j} &amp; = sigmoid(logits_{i, j}) \\\\<br>    &amp; = \frac{1}{1 + e^{-logits_{i, j} } } \\\\<br>    loss_{i, j} &amp; = -[y_{i, j} \times ln p_{i, j} + (1 - y_{i, j}) \times (1 - ln p_{i, j})]<br>\end{split}<br>\]<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_crossentropy</span><span class="params">(target, output, from_logits=False)</span>:</span></div><div class="line">    <span class="string">"""Binary crossentropy between an output tensor and a target tensor.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    # Arguments</span></div><div class="line"><span class="string">        target: A tensor with the same shape as `output`.</span></div><div class="line"><span class="string">        output: A tensor.</span></div><div class="line"><span class="string">        from_logits: Whether `output` is expected to be a logits tensor.</span></div><div class="line"><span class="string">            By default, we consider that `output`</span></div><div class="line"><span class="string">            encodes a probability distribution.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    # Returns</span></div><div class="line"><span class="string">        A tensor.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="comment"># Note: tf.nn.sigmoid_cross_entropy_with_logits</span></div><div class="line">    <span class="comment"># expects logits, Keras expects probabilities.</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> from_logits:</div><div class="line">        <span class="comment"># transform back to logits</span></div><div class="line">        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)</div><div class="line">        output = tf.clip_by_value(output, _epsilon, <span class="number">1</span> - _epsilon)</div><div class="line">        output = tf.log(output / (<span class="number">1</span> - output))</div><div class="line"></div><div class="line">    <span class="keyword">return</span> tf.nn.sigmoid_cross_entropy_with_logits(labels=target,</div><div class="line">                                                   logits=output)</div></pre></td></tr></table></figure></p>
<h3 id="categorical-crossentropy"><a href="#categorical-crossentropy" class="headerlink" title="categorical_crossentropy"></a>categorical_crossentropy</h3><p>适用于每个类别相互独立且排斥的情况（onehot），即多类别中的单标签任务\( (batch size, num classes) \)。<br>\[<br>\begin{split}<br>    p_{i, j} &amp; = \frac{e^{logits_{i, j} } }{\sum_{j=0}^{num classes - 1} e^{logits_{i, j} } } \\\\<br>    loss_i &amp; = - \sum_{j=0}^{num  classes - 1} y_{i, j} ln p_{i, j}<br>\end{split}<br>\]<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">categorical_crossentropy</span><span class="params">(target, output, from_logits=False, axis=<span class="number">-1</span>)</span>:</span></div><div class="line">    <span class="string">"""Categorical crossentropy between an output tensor and a target tensor.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    # Arguments</span></div><div class="line"><span class="string">        target: A tensor of the same shape as `output`.</span></div><div class="line"><span class="string">        output: A tensor resulting from a softmax</span></div><div class="line"><span class="string">            (unless `from_logits` is True, in which</span></div><div class="line"><span class="string">            case `output` is expected to be the logits).</span></div><div class="line"><span class="string">        from_logits: Boolean, whether `output` is the</span></div><div class="line"><span class="string">            result of a softmax, or is a tensor of logits.</span></div><div class="line"><span class="string">        axis: Int specifying the channels axis. `axis=-1`</span></div><div class="line"><span class="string">            corresponds to data format `channels_last`,</span></div><div class="line"><span class="string">            and `axis=1` corresponds to data format</span></div><div class="line"><span class="string">            `channels_first`.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    # Returns</span></div><div class="line"><span class="string">        Output tensor.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    # Raises</span></div><div class="line"><span class="string">        ValueError: if `axis` is neither -1 nor one of</span></div><div class="line"><span class="string">            the axes of `output`.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    output_dimensions = list(range(len(output.get_shape())))</div><div class="line">    <span class="keyword">if</span> axis != <span class="number">-1</span> <span class="keyword">and</span> axis <span class="keyword">not</span> <span class="keyword">in</span> output_dimensions:</div><div class="line">        <span class="keyword">raise</span> ValueError(</div><div class="line">            <span class="string">'&#123;&#125;&#123;&#125;&#123;&#125;'</span>.format(</div><div class="line">                <span class="string">'Unexpected channels axis &#123;&#125;. '</span>.format(axis),</div><div class="line">                <span class="string">'Expected to be -1 or one of the axes of `output`, '</span>,</div><div class="line">                <span class="string">'which has &#123;&#125; dimensions.'</span>.format(len(output.get_shape()))))</div><div class="line">    <span class="comment"># Note: tf.nn.softmax_cross_entropy_with_logits</span></div><div class="line">    <span class="comment"># expects logits, Keras expects probabilities.</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> from_logits:</div><div class="line">        <span class="comment"># scale preds so that the class probas of each sample sum to 1</span></div><div class="line">        output /= tf.reduce_sum(output, axis, <span class="keyword">True</span>)</div><div class="line">        <span class="comment"># manual computation of crossentropy</span></div><div class="line">        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)</div><div class="line">        output = tf.clip_by_value(output, _epsilon, <span class="number">1.</span> - _epsilon)</div><div class="line">        <span class="keyword">return</span> - tf.reduce_sum(target * tf.log(output), axis)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> tf.nn.softmax_cross_entropy_with_logits(labels=target,</div><div class="line">                                                       logits=output)</div></pre></td></tr></table></figure></p>
<h3 id="weighted-cross-entropy"><a href="#weighted-cross-entropy" class="headerlink" title="weighted_cross_entropy"></a>weighted_cross_entropy</h3><p>\[<br>\begin{split}<br>    p_{i, j} &amp; = sigmoid(logits_{i, j}) \\\\<br>    &amp; = \frac{1}{1 + e^{-logits_{i, j} } } \\\\<br>\end{split}<br>\]</p>
<p>\[<br>loss_{i, j} = -[ pos\_weight \times y_{i, j} \times ln p_{i, j} + (1 - y_{i, j}) \times (1 - ln p_{i, j})]<br>\]<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.nn.weighted_cross_entropy_with_logits(labels,logits, pos_weight, name=<span class="keyword">None</span>)</div></pre></td></tr></table></figure></p>
<h3 id="softmax-求导"><a href="#softmax-求导" class="headerlink" title="softmax 求导"></a>softmax 求导</h3><p>\[<br>\begin{split}<br>    y_i &amp; = \frac{e^{x_i} }{\sum_{j=0}^{m-1} e^{x_j} } \\\\<br>    \frac{\partial y_i}{\partial x_k} &amp; = \frac{\partial \frac{e^{x_i} }{\sum_{j=0}^{m-1} e^{x_j} } }{\partial x_k} \\\\<br>    &amp; = \frac{\frac{\partial e^{x_i} }{\partial x_k} \times \sum - e^{x_i} \times \frac{\partial \sum}{\partial x_k} }{ {\sum}^2 } \\\\<br>    &amp; = \frac{\frac{\partial e^{x_i} }{\partial x_k} \times \sum - e^{x_i} \times e^{x_k} }{ {\sum}^2 } \\\\<br>    &amp; = \begin{cases}<br>        \frac{e^{x_k} \times \sum - e^{x_i} \times e^{x_k} }{ {\sum}^2 }, \text{ if } i = k \\\\<br>        \frac{ - e^{x_i} \times e^{x_k} }{ {\sum}^2 }, \text{ if } i \neq k<br>    \end{cases} \\\\<br>    &amp; = \begin{cases}<br>    y_k(1 - y_i), \text{ if } i = k\\\\<br>    -y_k y_i, \text{ if } i \neq k<br>    \end{cases}<br>\end{split}<br>\]</p>
<p>\[<br>    \mathbf{y} = softmax(\mathbf{x})<br>\]</p>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/softmax/softmax.png" alt="softmax"></p>
<p>雅可比矩阵<br>\[<br>Jacobian_{\mathbf{y} }(\mathbf{x}) =<br>\left[<br> \begin{matrix}<br>   \frac{\partial y_0}{\partial x_0} &amp; \frac{\partial y_0}{\partial x_1} &amp; \dots &amp; \frac{\partial y_0}{\partial x_{m-1} } \\\\<br>   \vdots &amp;  \vdots &amp; \ddots &amp; \vdots \\\\<br>   \frac{\partial y_{m-1} }{\partial x_0} &amp; \frac{\partial y_{m-1} }{\partial x_1} &amp; \dots &amp; \frac{\partial y_{m-1}}{\partial x_{m-1} }<br>  \end{matrix}<br>  \right]<br>\]</p>
<h3 id="categorical-crossentropy-求导"><a href="#categorical-crossentropy-求导" class="headerlink" title="categorical_crossentropy 求导"></a>categorical_crossentropy 求导</h3><p>\[<br>\begin{split}<br>    \frac{\partial loss}{\partial logits_k} &amp; = \frac{\partial {- \sum_{j=0}^{num  classes - 1} y_{j} ln p_{j} } }{\partial logits_k} \\\\<br>    &amp; = - \sum_{j=0}^{num classes - 1} y_{j}  \frac{\partial ln p_{j} }{\partial logits_k} \\\\<br>    &amp; = - \sum_{j=0}^{num classes - 1} y_{j}  \frac{1}{p_{j} } \frac{\partial p_{j} }{\partial logits_k} \\\\<br>    &amp; = - \sum_{j=0}^{num classes - 1} y_{j}  \frac{1}{p_{j} } \begin{cases}<br>                                                            p_k (1 - p_j), \text{ if } j = k \\\\<br>                                                            -p_j p_k, \text{ if } j \neq k<br>                                                            \end{cases} \\\\<br>    &amp; = - y_{k}(1 - p_k) - \sum_{j=0, j \neq k}^{num classes - 1} y_{j} (-p_k) \\\\<br>    &amp; = - y_{k} + y_{k} p_k + \sum_{j=0, j \neq k}^{num classes - 1} y_{j} p_k \\\\<br>    &amp; = - y_{k} + p_k \sum_{j=0}^{num classes - 1} y_{j} \\\\<br>    &amp; = p_k - y_{k}<br>\end{split}<br>\]<br>可以看出 categorical_crossentropy 的导数很简洁，即预测概率与真实概率的差。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">tf.nn.softmax_cross_entropy_with_logits(</div><div class="line">    _sentinel=<span class="keyword">None</span>,</div><div class="line">    labels=<span class="keyword">None</span>,</div><div class="line">    logits=<span class="keyword">None</span>,</div><div class="line">    dim=<span class="number">-1</span>,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure></p>
<blockquote>
<p>WARNING: This op expects unscaled logits, since it performs a softmax on logits internally for efficiency. Do not call this op with the output of softmax, as it will produce incorrect results.</p>
<p>Backpropagation will happen only into logits. To calculate a cross entropy loss that allows backpropagation into both logits and labels, see tf.nn.softmax_cross_entropy_with_logits_v2.</p>
</blockquote>
<p>相较于相继调用 <code>softmax</code> 和 <code>cross_entropy</code> （正向传播，反向传播），<code>softmax_cross_entropy_with_logits</code> 的反向传播速度更快，原因就是可以直接使用上面的的推导结果\( \frac{\partial loss}{\partial logits_k} =  p_k - y_{k} \) 简单快速地求得 <code>logits</code> 上的梯度，并在此基础上继续反向传播，而不必对 <code>cross_entropy</code> 和 <code>softmax</code> 依次反向传播之后才得到 <code>logits</code> 上的梯度\( \sum_{j=0}^{m-1} \frac{\partial loss}{\partial p_j} \frac{\partial p_j}{\partial logits_k} \)[upstream gradients local gradients]。将这两个操作合为一个操作，正向传播速度一样，反向传播实现了加速。</p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/07/01/模型评估-量化预测质量/">
                模型评估 量化预测质量
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-07-01</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p>本博文主要是在阅读 scikit-learn 官方文档<a href="http://scikit-learn.org/stable/modules/model_evaluation.html" target="_blank" rel="external">3.3. Model evaluation: quantifying the quality of predictions</a> 之后的总结。<br>首先看一下根据 label 特点的不同做出的分类任务类型的划分：<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/metrics/1.PNG" alt="不同的分类任务"><br>scikit-learn 提供了三种不同的 API 来评估模型的预测质量。</p>
<ul>
<li>Estimator score method（学习器的score 方法）</li>
<li>Scoring parameter （评分策略参数） 基于交叉验证的模型评估工具（比如 <code>model_selection.cross_val_score</code> and <code>model_selection.GridSearchCV</code>) 依赖于一个内部的评分策略 ）</li>
<li>Metric functions（评价函数），metrics 模块提供了很多这样的函数。</li>
</ul>
<p>另外一点值得指出的是，可以使用 <code>Dummy estimators</code> 获得评价指标的基准值。</p>
<h2 id="使用-scoring-参数来定义模型评估的规则"><a href="#使用-scoring-参数来定义模型评估的规则" class="headerlink" title="使用 scoring 参数来定义模型评估的规则"></a>使用 scoring 参数来定义模型评估的规则</h2><p>Model selection and evaluation using tools, such as <code>model_selection.GridSearchCV</code> and <code>model_selection.cross_val_score</code>, take a scoring parameter that controls what metric they apply to the estimators evaluated.<br>值得注意的是，所有的评分指标都遵循“大值优于小值”的约定。因此对于一些基于“距离”的度量，像 <code>metrics.mean_squared_error</code>，在此使用的是其相反数 <code>neg_mean_squared_error</code>。<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/metrics/scoring_parameters.PNG" alt="scoring_parameters"></p>
<h2 id="分类问题的评估指标"><a href="#分类问题的评估指标" class="headerlink" title="分类问题的评估指标"></a>分类问题的评估指标</h2><p><code>sklearn.metrics</code> 模块提供了很多的损失（loss），评分（score）和辅助函数（utility）来度量分类器的性能。<br>一些指标函数需要分类器能够给出概率估计、confidence values, 和 binary decisions values。并且大部分的实现也允许通过提供 <code>sample_weights</code> 参数来计算指标的一个加权值。<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/metrics/classification_metrics.PNG" alt="classification_metrics"></p>
<p>下面关注一下一些常见的指标。</p>
<h4 id="Accuracy-score"><a href="#Accuracy-score" class="headerlink" title="Accuracy score"></a>Accuracy score</h4><p>\(\hat{y_i}\) 是第 i 个样本的预测值，\(y_i\) 是第 i 个样本的真实值。<br>$$\texttt{accuracy}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} 1(\hat{y}_i = y_i)$$<br>其中 \(1(x)\) 是示性函数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">from sklearn.metrics import accuracy_score</div><div class="line">y_pred = [0, 2, 1, 3]</div><div class="line">y_true = [0, 1, 2, 3]</div><div class="line">accuracy_score(y_true, y_pred)</div><div class="line">0.5</div><div class="line">accuracy_score(y_true, y_pred, normalize=False)</div><div class="line">2</div></pre></td></tr></table></figure></p>
<p>在多标签（multilabel）任务下，使用二进制标签矩阵（binary label indicators），示性函数需要\(\hat{y}_i\) 和 \(y_i\) 完全匹配才取 1 值。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))</div><div class="line">0.5</div></pre></td></tr></table></figure></p>
<h4 id="Classification-report"><a href="#Classification-report" class="headerlink" title="Classification report"></a>Classification report</h4><p>这个函数会构建一个展示常见分类度量指标的文本报告（text report）。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">from sklearn.metrics import classification_report</div><div class="line">y_true = [0, 1, 2, 2, 0]</div><div class="line">y_pred = [0, 0, 2, 1, 0]</div><div class="line">target_names = [&apos;class 0&apos;, &apos;class 1&apos;, &apos;class 2&apos;]</div><div class="line">print(classification_report(y_true, y_pred, target_names=target_names))</div><div class="line">             precision    recall  f1-score   support</div><div class="line"></div><div class="line">    class 0       0.67      1.00      0.80         2</div><div class="line">    class 1       0.00      0.00      0.00         1</div><div class="line">    class 2       1.00      0.50      0.67         2</div><div class="line"></div><div class="line">avg / total       0.67      0.60      0.59         5</div></pre></td></tr></table></figure></p>
<h4 id="Precision-recall-and-F-measures"><a href="#Precision-recall-and-F-measures" class="headerlink" title="Precision, recall and F-measures"></a>Precision, recall and F-measures</h4><p>对二分类问题而言，<br>$$<br>\text{precision} = \frac{tp}{tp + fp},<br>$$<br>$$<br>\text{recall} = \frac{tp}{tp + fn},<br>$$<br>$$<br>F_\beta = (1 + \beta^2) \frac{\text{precision} \times \text{recall}}{\beta^2 \text{precision} + \text{recall}}.<br>$$<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/metrics/recall_precision_f_beta.PNG" alt="recall_precision_f_beta"><br>对多类别和多标签分类而言，precision, recall, and F-measures 可以独立地应用于各个标签，然后可以通过 <code>average</code> 参数指定的方式将各个标签上的结果进行综合。 <code>average</code> 参数可以应用于 <code>average_precision_score</code> (multilabel only), <code>f1_score</code>, <code>fbeta_score</code>, <code>precision_recall_fscore_support</code>, <code>precision_score</code> and <code>recall_score</code> 这些函数。<br><em>Note that for “micro”-averaging in a multiclass setting with all labels included will produce equal precision, recall and F, while “weighted” averaging may produce an F-score that is not between precision and recall.</em></p>
<ul>
<li>\(y\) the set of predicted (sample, label) pairs</li>
<li>\(\hat{y}\) the set of true (sample, label) pairs</li>
<li>\(L\) the set of labels</li>
<li>\(S\) the set of samples</li>
<li>\(y_s\) the subset of \(y\) with sample \(s\), i.e. \( y_s := \lbrace (s’, l) \in y | s = s’ \rbrace \)</li>
<li>\(y_l\) the subset of \(y\) with label \(l\)</li>
<li>similarly, \(\hat{y_s}\) and \(\hat{y_l}\) are subsets of \(\hat{y}\)</li>
<li>\(P(A, B) := \frac{\left| A \cap B \right|}{\left|A\right|}\)</li>
<li>\(R(A, B) := \frac{\left| A \cap B \right|}{\left|B\right|}\) (Conventions vary on handling \(B = \emptyset\); this implementation uses \(R(A, B):=0\), and similar for \(P\).)</li>
<li>\(F_\beta(A, B) := \left(1 + \beta^2\right) \frac{P(A, B) \times R(A, B)}{\beta^2 P(A, B) + R(A, B)}\)<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/metrics/2.PNG" alt="多标签任务的真实标签"><br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/metrics/3.PNG" alt="多标签任务的预测标签"><br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/metrics/4.PNG" alt="多标签任务指标的计算图示"><br>Then the metrics are defined as:<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/metrics/recall_precision_f_beta_formulations.PNG" alt="recall_precision_f_beta_formulations"></li>
</ul>
<h4 id="precision-recall-curve"><a href="#precision-recall-curve" class="headerlink" title="precision_recall_curve"></a>precision_recall_curve</h4><p>The <code>precision_recall_curve</code> computes a precision-recall curve from the ground truth label and a score given by the classifier by varying a decision threshold.<br><strong>仅仅能够应用在二分类问题中。</strong></p>
<h4 id="average-precision-score"><a href="#average-precision-score" class="headerlink" title="average_precision_score"></a>average_precision_score</h4><p>The <code>average_precision_score</code> function computes the average precision (AP) from prediction scores. The value is between 0 and 1 and higher is better. AP is defined as<br>$$AP = \sum_n (R_n - R_{n-1}) P_n$$<br>where \(P_n\) and \(R_n\) are the precision and recall at the nth threshold. With random predictions, the AP is the fraction of positive samples.<br><strong>仅能够应用在二分类问题和多标签问题中。</strong></p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/06/08/笔记-for-PyData-2015-Introduction-to-scikit-image/">
                笔记 for PyData 2015 - Introduction to scikit-image
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-06-08</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p>这篇博文是 Emmanuelle Gouillart (scikit-image 开发者) 在 PyData 2015 上演讲 Introduction to scikit-image 的笔记。</p>
<h3 id="scikit-image"><a href="#scikit-image" class="headerlink" title="scikit-image"></a>scikit-image</h3><p>scikit-image is a general-purpose image processing module for the Python programming language. It is designed to interact efficiently with other popular scientific Python libraries, such as NumPy and SciPy. In particular, scikit-image leverages the powerful data array container of NumPy, that can store images of various dimensions (2-D, 2D RGB, 3D, 4D…).</p>
<h3 id="Foundation"><a href="#Foundation" class="headerlink" title="Foundation"></a>Foundation</h3><p>NumPy-native: images as NumPy arrays</p>
<h3 id="Filtering：-transforming-image-data"><a href="#Filtering：-transforming-image-data" class="headerlink" title="Filtering： transforming image data"></a>Filtering： transforming image data</h3><p>modules:</p>
<ul>
<li>skimage.filter</li>
<li>skimage.exposure</li>
<li>skimage.restoration</li>
</ul>
<p>word cloud:</p>
<ul>
<li>median</li>
<li>gaussian</li>
<li>canny</li>
<li>sobel</li>
<li>wiener</li>
<li>equalize</li>
<li>denoising</li>
<li>enhance_contrast</li>
<li>total_variation</li>
</ul>
<h3 id="Extracting-features"><a href="#Extracting-features" class="headerlink" title="Extracting features"></a>Extracting features</h3><p>Feature detection: extracting features from images to feed into classifiers or estimators.<br>modules:</p>
<ul>
<li>skimage.feature</li>
<li>skimage.filter</li>
</ul>
<p>word cloud:</p>
<ul>
<li>corners</li>
<li>daisy</li>
<li>local_maxima</li>
<li>gabor</li>
<li>harris</li>
<li>hog</li>
<li>hough</li>
<li>cooccurance</li>
<li>canny</li>
</ul>
<h3 id="Geometrical-transformations"><a href="#Geometrical-transformations" class="headerlink" title="Geometrical transformations"></a>Geometrical transformations</h3><p>modules:</p>
<ul>
<li>skimage.transform</li>
</ul>
<p>word cloud</p>
<ul>
<li>scale</li>
<li>zoom</li>
<li>rotate</li>
<li>swirl</li>
<li>wrap</li>
</ul>
<h3 id="Segmentation-labelling-regions"><a href="#Segmentation-labelling-regions" class="headerlink" title="Segmentation: labelling regions"></a>Segmentation: labelling regions</h3><p>label different pixels to separate objects.<br>modules:</p>
<ul>
<li>skimage.segmentation</li>
</ul>
<p>word cloud:</p>
<ul>
<li>thresholding: histogram based</li>
<li>otsu</li>
<li>randomwalker</li>
<li>superpixel: separating connected objects</li>
<li>watershed</li>
</ul>
<h3 id="Measures-on-images"><a href="#Measures-on-images" class="headerlink" title="Measures on images"></a>Measures on images</h3><p>After image segmentation, you have some regions, you may want to compute statistics and label each regions.<br>modules:<br>skimage.measure</p>
<p>word cloud:</p>
<ul>
<li>measure</li>
<li>label: very useful, give different labels to different connected components.</li>
<li>size</li>
<li>histogram</li>
<li>regionprops</li>
</ul>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/skimage/skimage_label.png" alt="skimage.measure.label"></p>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/06/07/手工-Backpropagation-推导与-TensorFlow-automatic-differentiation/">
                手工 Backpropagation 推导与 TensorFlow automatic differentiation
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-06-07</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h3 id="主要对比梯度反向传播手工推导与TensorFLow-自动差分"><a href="#主要对比梯度反向传播手工推导与TensorFLow-自动差分" class="headerlink" title="主要对比梯度反向传播手工推导与TensorFLow 自动差分"></a>主要对比梯度反向传播手工推导与TensorFLow 自动差分</h3><p>关于梯度反向传播手工推导的学习可以参考斯坦福大学的 <a href="http://cs231n.stanford.edu/" target="_blank" rel="external">CS231n: Convolutional Neural Networks for Visual Recognition</a> 的 Lecture 4。<br>下面通过代码对这两者进行对比，代码的Jupyter NoteBook 可以在我的 <a href="https://github.com/RobertLexis/TensorFlow-automatic-differentiation" target="_blank" rel="external">GitHub 仓库</a>中找到。</p>
<h4 id="imports"><a href="#imports" class="headerlink" title="imports"></a>imports</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">import os</div><div class="line">os.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;]=&apos;3&apos;</div><div class="line">import tensorflow as tf</div></pre></td></tr></table></figure>
<h4 id="定义变量"><a href="#定义变量" class="headerlink" title="定义变量"></a>定义变量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">with tf.variable_scope(&quot;params&quot;, reuse=tf.AUTO_REUSE):</div><div class="line">    w = tf.get_variable(&quot;w&quot;, initializer=tf.constant([2.0]))</div><div class="line">    b = tf.get_variable(&quot;b&quot;, initializer=tf.constant([0.0]))</div></pre></td></tr></table></figure>
<h4 id="定义占位符"><a href="#定义占位符" class="headerlink" title="定义占位符"></a>定义占位符</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = tf.placeholder(tf.float32, shape=[None], name=&quot;x&quot;)</div><div class="line">y = tf.placeholder(tf.float32, shape=[None], name=&quot;y&quot;)</div></pre></td></tr></table></figure>
<h4 id="定义表达式"><a href="#定义表达式" class="headerlink" title="定义表达式"></a>定义表达式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">y_pred = w*x + b</div></pre></td></tr></table></figure>
<h4 id="定义代价函数"><a href="#定义代价函数" class="headerlink" title="定义代价函数"></a>定义代价函数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loss = tf.reduce_mean(tf.square(y_pred - y))</div></pre></td></tr></table></figure>
<h4 id="定义优化器"><a href="#定义优化器" class="headerlink" title="定义优化器"></a>定义优化器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)</div><div class="line">grads_and_vars = optimizer.compute_gradients(loss)</div><div class="line">train_op = optimizer.apply_gradients(grads_and_vars)</div></pre></td></tr></table></figure>
<h4 id="创建会话并初始化变量"><a href="#创建会话并初始化变量" class="headerlink" title="创建会话并初始化变量"></a>创建会话并初始化变量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">init_op = tf.global_variables_initializer()</div><div class="line"></div><div class="line">sess = tf.InteractiveSession()</div><div class="line">sess.run(init_op)</div></pre></td></tr></table></figure>
<h4 id="根据函数-y-4x-3-给出训练数据"><a href="#根据函数-y-4x-3-给出训练数据" class="headerlink" title="根据函数 y = 4x + 3 给出训练数据"></a>根据函数 y = 4x + 3 给出训练数据</h4><h4 id="对于单个数据点输入，对比-TensorFlow-automatic-differentiation-与手工推导"><a href="#对于单个数据点输入，对比-TensorFlow-automatic-differentiation-与手工推导" class="headerlink" title="对于单个数据点输入，对比 TensorFlow automatic differentiation 与手工推导"></a>对于单个数据点输入，对比 TensorFlow automatic differentiation 与手工推导</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># y_ = 4 * 2 + 3</div><div class="line">x_ = [2]</div><div class="line">y_ = [11] # y_ = 4 * 2 + 3</div></pre></td></tr></table></figure>
<h6 id="手工推导-Backpropagation-过程"><a href="#手工推导-Backpropagation-过程" class="headerlink" title="手工推导 Backpropagation 过程"></a>手工推导 Backpropagation 过程</h6><p><img src="http://oytnj8g2y.bkt.clouddn.com/backwards/backward_1.png" alt="backward_1"></p>
<h6 id="用-TensorFlow-计算-loss-及梯度"><a href="#用-TensorFlow-计算-loss-及梯度" class="headerlink" title="用 TensorFlow 计算 loss 及梯度"></a>用 TensorFlow 计算 loss 及梯度</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sess.run([loss, grads_and_vars], feed_dict=&#123;x: x_, y: y_&#125;)</div></pre></td></tr></table></figure>
<p>Output:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[49.0,</div><div class="line"> [(array([-28.], dtype=float32), array([2.], dtype=float32)),</div><div class="line">  (array([-14.], dtype=float32), array([0.], dtype=float32))]]</div></pre></td></tr></table></figure></p>
<h4 id="对于-batch-输入，对比-TensorFlow-automatic-differentiation-与手工推导"><a href="#对于-batch-输入，对比-TensorFlow-automatic-differentiation-与手工推导" class="headerlink" title="对于 batch 输入，对比 TensorFlow automatic differentiation 与手工推导"></a>对于 batch 输入，对比 TensorFlow automatic differentiation 与手工推导</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x_ = [2, 3]</div><div class="line">y_ = [11, 15]</div></pre></td></tr></table></figure>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/backwards/backward_1.png" alt="backward_1"><br><img src="http://oytnj8g2y.bkt.clouddn.com/backwards/backward_2.png" alt="backward_2"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sess.run([loss, grads_and_vars], feed_dict=&#123;x: x_, y: y_&#125;)</div></pre></td></tr></table></figure>
<p>Output:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[65.0,</div><div class="line"> [(array([-41.], dtype=float32), array([2.], dtype=float32)),</div><div class="line">  (array([-16.], dtype=float32), array([0.], dtype=float32))]]</div></pre></td></tr></table></figure></p>
<h6 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">65 == (81+49)/2</div><div class="line">-41 == (-28-54)/2</div><div class="line">-16 == (-14-18)/2</div></pre></td></tr></table></figure>
<p>可以看出 TensorFlow 计算出的 loss 是两次 loss 的均值，两个变量上的梯度也是各自两次梯度值的均值。</p>
<h4 id="以-learning-rate-为步幅对-w，b进行一次更新"><a href="#以-learning-rate-为步幅对-w，b进行一次更新" class="headerlink" title="以 learning_rate 为步幅对 w，b进行一次更新"></a>以 learning_rate 为步幅对 w，b进行一次更新</h4><h6 id="手工计算"><a href="#手工计算" class="headerlink" title="手工计算"></a>手工计算</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">2 - 0.001 * (-41) = 2.041</div><div class="line">0 - 0.001 * (-16) = 0.016</div></pre></td></tr></table></figure>
<h6 id="TensorFlow-计算结果"><a href="#TensorFlow-计算结果" class="headerlink" title="TensorFlow 计算结果"></a>TensorFlow 计算结果</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div></pre></td></tr></table></figure>
<p>Output:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[array([2.041], dtype=float32), array([0.016], dtype=float32)]</div></pre></td></tr></table></figure></p>
<h4 id="下面对模型进行训练并观察-w，b-的变化过程"><a href="#下面对模型进行训练并观察-w，b-的变化过程" class="headerlink" title="下面对模型进行训练并观察 w，b 的变化过程"></a>下面对模型进行训练并观察 w，b 的变化过程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">for _ in range(1000):</div><div class="line">    sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div><div class="line">[array([4.700393], dtype=float32), array([1.1883683], dtype=float32)]</div><div class="line"></div><div class="line"></div><div class="line">for _ in range(1000):</div><div class="line">    sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div><div class="line">[array([4.6550403], dtype=float32), array([1.3056908], dtype=float32)]</div><div class="line"></div><div class="line"></div><div class="line">for _ in range(1000):</div><div class="line">    sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div><div class="line">[array([4.6126213], dtype=float32), array([1.4154125], dtype=float32)]</div><div class="line"></div><div class="line"></div><div class="line">for _ in range(10000):</div><div class="line">    sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div><div class="line">[array([4.3136806], dtype=float32), array([2.1886632], dtype=float32)]</div><div class="line"></div><div class="line"></div><div class="line">for _ in range(10000):</div><div class="line">    sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div><div class="line">[array([4.1606426], dtype=float32), array([2.5844746], dtype=float32)]</div></pre></td></tr></table></figure>
<p>可以看出随着训练步骤的增多，w，b 逐渐逼近目标值 4， 3</p>
<h4 id="关闭会话，释放资源"><a href="#关闭会话，释放资源" class="headerlink" title="关闭会话，释放资源"></a>关闭会话，释放资源</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sess.close()</div></pre></td></tr></table></figure>
<h3 id="Caveats"><a href="#Caveats" class="headerlink" title="Caveats"></a>Caveats</h3><p>在上面我们定义的loss是如下的均值（0-d Tensor/scalar/shape ()）形式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loss = tf.reduce_mean(tf.square(y_pred - y))</div></pre></td></tr></table></figure></p>
<p>假设我们没有进行 <code>reduce_mean</code>，即 loss 是如下的形式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loss = tf.square(y_pred - y)</div></pre></td></tr></table></figure></p>
<p>当 <code>x_ = [2, 3]</code>、<code>y_ = [11, 15]</code>时，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sess.run([loss, grads_and_vars], feed_dict=&#123;x: x_, y: y_&#125;)</div></pre></td></tr></table></figure></p>
<p>输出为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[array([49., 81.], dtype=float32),</div><div class="line"> [(array([-82.], dtype=float32), array([2.], dtype=float32)),</div><div class="line">  (array([-32.], dtype=float32), array([0.], dtype=float32))]]</div></pre></td></tr></table></figure></p>
<p>有两个 loss 值分别为49 和 81，和我们手工计算出的值是一致的，而两个变量的梯度为各自的两次梯度值的加和。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">-82 == (-28) + (-54)</div><div class="line">-32 == (-14) + (-18)</div></pre></td></tr></table></figure></p>
<p>即有两个 loss，相当于更新了两次。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>Loss 仅仅是一个度量指标，其均值是对模型性能的一个更有意义的评价，是为此而求的均值，这个均值也仅仅用于模型性能的评价，其具体数值并不实际参与到反向传播更新各个参数的过程。loss 的具体数值意义不大，因为反向传播最起始的upstream gradient总是1。<br>这也就解释了我之前遇到过的一个问题，为什么单纯给优化器传入一个loss 的数值，TF 会报错，无法计算梯度，因为真正重要的不是这个末端的loss 数值，而是在计算这个末端loss 数值的过程中的每一个中间值及涉及到的操作类型（add mul max sub square ）。利用这些中间值和操作类型结合，反向进行梯度的传播。</li>
<li>对于batch 样本输入，正确的 loss 定义计算的是各个样本上loss 的均值，各个参数上的梯度也是各自在各个样本上梯度值的均值，可以这样看，各个样本单独输入求loss ，求梯度，然后再在各个样本的结果上进行平均。</li>
</ul>
<h3 id="Reminder-from-cs231n"><a href="#Reminder-from-cs231n" class="headerlink" title="Reminder from cs231n"></a>Reminder from cs231n</h3><table>
<thead>
<tr>
<th>Operation</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>add</td>
<td>distributor</td>
</tr>
<tr>
<td>max</td>
<td>router</td>
</tr>
<tr>
<td>mul</td>
<td>switcher</td>
</tr>
</tbody>
</table>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/05/28/Deep-learning-资料汇总/">
                Deep learning 资料汇总
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-05-28</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p><a href="https://web.stanford.edu/class/cs20si/" target="_blank" rel="external">CS 20: Tensorflow for Deep Learning Research</a><br><a href="https://github.com/chiphuyen/stanford-tensorflow-tutorials" target="_blank" rel="external">stanford-tensorflow-tutorials from github</a><br><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM Networks from colah’s blog</a><br><a href="https://blog.csdn.net/qunnie_yi/article/details/80129434" target="_blank" rel="external">详解TensorBoard如何调参</a><br><a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials" target="_blank" rel="external">TensorFlow Tutorial from Hvass Laboratories YouTube</a><br><a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/index.html" target="_blank" rel="external">CS 294: Deep Reinforcement Learning, Fall 2017</a><br>Amazing!!!看过的最好的关于Python高级（专家）特性的讲解 <a href="https://www.youtube.com/watch?v=7lmCu8wz8ro&amp;index=2&amp;list=WL" target="_blank" rel="external">What Does It Take To Be An Expert At Python?</a><br><a href="https://www.pyimagesearch.com/" target="_blank" rel="external">pyimagesearch 博客</a><br><a href="http://cv-tricks.com/" target="_blank" rel="external">Learn Machine Learning, AI &amp; Computer vision</a><br><a href="https://www.youtube.com/watch?v=oaxf3rk0KGM&amp;list=PL9Hr9sNUjfsmEu1ZniY0XpHSzl5uihcXZ&amp;index=26" target="_blank" rel="external">TensorFlow Tutorial #19 Hyper-Parameter Optimization from YouTube Hvass Laboratories</a></p>
<h3 id="深度学习产品化"><a href="#深度学习产品化" class="headerlink" title="深度学习产品化"></a>深度学习产品化</h3><p><a href="http://www.bitbionic.com/2017/08/18/run-your-keras-models-in-c-tensorflow/" target="_blank" rel="external">Run your Keras models in C++ Tensorflow</a><br><a href="https://blog.keras.io/building-a-simple-keras-deep-learning-rest-api.html" target="_blank" rel="external">Building a simple Keras + deep learning REST API</a><br><a href="https://scotch.io/bar-talk/processing-incoming-request-data-in-flask" target="_blank" rel="external">Processing Incoming Request Data in Flask</a><br><a href="https://www.pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/" target="_blank" rel="external">A scalable Keras + deep learning REST API</a><br><a href="https://www.pyimagesearch.com/2018/02/05/deep-learning-production-keras-redis-flask-apache/" target="_blank" rel="external">Deep learning in production with Keras, Redis, Flask, and Apache</a><br><a href="https://www.youtube.com/watch?v=f6Bf3gl4hWY" target="_blank" rel="external">How to Deploy Keras Models to Production from YouTube Siraj Raval</a><br><a href="https://www.youtube.com/watch?v=T_afaArR0E8" target="_blank" rel="external">How to Deploy a Tensorflow Model to Production from YouTube Siraj Raval</a></p>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/05/17/For-FreeWheel/">
                For FreeWheel
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-05-17</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h2 id="epoll"><a href="#epoll" class="headerlink" title="epoll"></a>epoll</h2><p>epoll is a Linux kernel <strong>system call</strong> for a scalable <strong>I/O event notification mechanism</strong>. Its function is to monitor multiple file descriptors to see if I/O is possible on any of them. It is meant to replace the older POSIX select(2) and poll(2) system calls, to achieve better performance in more demanding applications, where the number of watched file descriptors is large (unlike the older system calls, which operate in O(n) time, epoll operates in O(1) time).</p>
<h4 id="API"><a href="#API" class="headerlink" title="API"></a>API</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">int epoll_create1(int flags);</div></pre></td></tr></table></figure>
<p>Creates an epoll object and returns its file descriptor(标识内核事件表). The flags parameter allows epoll behavior to be modified. It has only one valid value, EPOLL_CLOEXEC. epoll_create() is an older variant of epoll_create1() and is deprecated as of Linux kernel version 2.6.27 and glibc version 2.9.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);</div></pre></td></tr></table></figure></p>
<p>Controls (configures) which file descriptors are watched by this object, and for which events. op can be ADD, MODIFY or DELETE.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);</div></pre></td></tr></table></figure></p>
<p>Waits for any of the events registered for with epoll_ctl, until at least one occurs or the timeout elapses. Returns the occurred events in events, up to maxevents at once.</p>
<h4 id="Triggering-modes"><a href="#Triggering-modes" class="headerlink" title="Triggering modes"></a>Triggering modes</h4><p>epoll provides both edge-triggered and level-triggered modes. In edge-triggered mode, a call to epoll_wait will return only when a new event is enqueued with the epoll object, while in level-triggered mode, epoll_wait will return as long as the condition holds（可读）.</p>
<p>For instance, if a pipe registered with epoll has received data, a call to epoll_wait will return, signaling the presence of data to be read. Suppose the reader only consumed part of data from the buffer. In level-triggered mode, further calls to epoll_wait will return immediately, as long as the pipe’s buffer contains data to be read. （每次读一部分就可以了，因为会不断地触发）In edge-triggered mode, however, epoll_wait will return only once new data is written to the pipe.（因此需要一次性读完）<br>ET 是高效工作模式，很大程度上降低同一个事件被重复触发的次数。</p>
<h2 id="HTTP-Hypertext-Transfer-Protocol"><a href="#HTTP-Hypertext-Transfer-Protocol" class="headerlink" title="HTTP Hypertext Transfer Protocol"></a>HTTP Hypertext Transfer Protocol</h2><p>Client request<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">GET /index.html HTTP/1.1</div><div class="line">Host: www.example.com</div></pre></td></tr></table></figure></p>
<p>Server response<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">HTTP/1.1 200 OK</div><div class="line">Date: Mon, 23 May 2005 22:38:34 GMT</div><div class="line">Content-Type: text/html; charset=UTF-8</div><div class="line">Content-Encoding: UTF-8</div><div class="line">Content-Length: 138</div><div class="line">Last-Modified: Wed, 08 Jan 2003 23:11:55 GMT</div><div class="line">Server: Apache/1.3.3.7 (Unix) (Red-Hat/Linux)</div><div class="line">ETag: &quot;3f80f-1b6-3e1cb03b&quot;</div><div class="line">Accept-Ranges: bytes</div><div class="line">Connection: close</div><div class="line"></div><div class="line">&lt;html&gt;</div><div class="line">&lt;head&gt;</div><div class="line">  &lt;title&gt;An Example Page&lt;/title&gt;</div><div class="line">&lt;/head&gt;</div><div class="line">&lt;body&gt;</div><div class="line">  Hello World, this is a very simple HTML document.</div><div class="line">&lt;/body&gt;</div><div class="line">&lt;/html&gt;</div></pre></td></tr></table></figure></p>
<p>Status codes<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Informational 1XX</div><div class="line">Successful 2XX</div><div class="line">Redirection 3XX</div><div class="line">Client Error 4XX</div><div class="line">Server Error 5XX</div></pre></td></tr></table></figure></p>
<p>The <strong>ETag (entity tag)</strong> header field is used to determine if a cached version of the requested resource is identical to the current version of the resource on the server.<br>Content-Type specifies the Internet media type of the data conveyed by the HTTP message, while Content-Length indicates its length in bytes. The HTTP/1.1 webserver publishes its ability to respond to requests for certain byte ranges of the document by setting the field Accept-Ranges: bytes. This is useful, if the client needs to have only certain portions of a resource sent by the server, which is called <strong>byte serving</strong>.<br>When Connection: close is sent, it means that the web server will close the TCP connection immediately after the transfer of this response.<br>A Content-Encoding like gzip can be used to compress the transmitted data.</p>
<h4 id="Message-format"><a href="#Message-format" class="headerlink" title="Message format"></a>Message format</h4><p>The client and server communicate by sending plain-text (ASCII) messages. The client sends requests to the server and the server sends responses.</p>
<h6 id="Request-message"><a href="#Request-message" class="headerlink" title="Request message"></a>Request message</h6><p>The request message consists of the following:<br><strong>A request line</strong> (e.g., GET /images/logo.png HTTP/1.1, which requests a resource called /images/logo.png from the server).<br><strong>Request header fields</strong> (e.g., Accept-Language: en).<br>An empty line.<br>An optional message body.<br>The request line and other header fields must each end with <cr><lf>. The empty line must consist of only <cr><lf> and no other whitespace. In the HTTP/1.1 protocol, all header fields except Host are optional.</lf></cr></lf></cr></p>
<h6 id="Response-message"><a href="#Response-message" class="headerlink" title="Response message"></a>Response message</h6><p>The response message consists of the following:<br><strong>A status line</strong> which includes the status code and reason message (e.g., HTTP/1.1 200 OK, which indicates that the client’s request succeeded).<br><strong>Response header fields</strong> (e.g., Content-Type: text/html).<br>An empty line.<br>An optional message body.<br>The status line and other header fields must all end with <cr><lf>. The empty line must consist of only <cr><lf> and no other whitespace.</lf></cr></lf></cr></p>
<h2 id="C-内存布局"><a href="#C-内存布局" class="headerlink" title="C++ 内存布局"></a>C++ 内存布局</h2><p>在 C++ 中，内存分成5个区，他们分别是堆、栈、自由存储区、全局/静态存储区和常量存储区。</p>
<ol>
<li>栈，在执行函数时，函数内局部变量的存储单元都可以在栈上创建，函数执行结束时这些存储单元自动被释放。栈内存分配运算内置于处理器的指令集中，效率很高，但是分配的内存容量有限。</li>
<li>堆，就是那些由 new 分配的内存块，他们的释放编译器不去管，由我们的应用程序去控制，一般一个new就要对应一个delete。如果程序员没有释放掉，那么在程序结束后，操作系统会自动回收。</li>
<li>自由存储区，就是那些由malloc等分配的内存块，他和堆是十分相似的，不过它是用free来结束自己的生命的。</li>
<li>全局/静态存储区，全局变量和静态变量被分配到同一块内存中，在以前的C语言中，全局变量又分为初始化的和未初始化的，在C++里面没有这个区分了，他们共同占用同一块内存区。</li>
<li>常量存储区，这是一块比较特殊的存储区，他们里面存放的是常量，不允许修改。</li>
</ol>
<h2 id="fork"><a href="#fork" class="headerlink" title="fork"></a>fork</h2><p>Fork (system call)<br>In computing, particularly in the context of the Unix operating system, fork is an operation whereby a process creates a copy of itself. It is usually a system call, implemented in the kernel. Fork is the primary (and historically, only) method of process creation on Unix-like operating systems.</p>
<h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>In multitasking operating systems, processes (running programs) need a way to create new processes, e.g. to run other programs. Fork and its variants are typically the only way of doing so in Unix-like systems. For a process to start the execution of a different program, it first forks to create a copy of itself. Then, the copy, called the “child process”, calls the <strong>exec system call</strong> to overlay itself with the other program: it ceases execution of its former program in favor of the other.</p>
<p>The fork operation creates a separate <strong>address space</strong> for the child. The child process has an exact copy of all the memory segments of the parent process. In modern UNIX variants that follow the <strong>virtual memory model</strong> from SunOS-4.0, <strong>copy-on-write</strong> semantics are implemented and the physical memory need not be actually copied. Instead, <strong>virtual memory pages in both processes may refer to the same pages of physical memory until one of them writes to such a page: then it is copied</strong>. This optimization is important in the common case where fork is used in conjunction with exec to execute a new program: typically, the child process performs only a small set of actions before it ceases execution of its program in favour of the program to be started, and it requires very few, if any, of its parent’s data structures.<br>堆、栈、全局/静态存储区均被复制成独立的两份，但打开的文件等是同一个，总不能因为 fork 复制进程映像就去拷贝文件吧。</p>
<h2 id="strcpy-memcpy-memset"><a href="#strcpy-memcpy-memset" class="headerlink" title="strcpy/memcpy/memset"></a>strcpy/memcpy/memset</h2><p>都是标准 C 库函数。</p>
<h4 id="strcpy"><a href="#strcpy" class="headerlink" title="strcpy"></a>strcpy</h4><p>strcpy 提供了字符串的复制。即 strcpy 只用于字符串复制，并且它不仅复制字符串内容，还会复制字符串的结束符。<br>strcpy 函数的原型是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">char* strcpy(char* dest, const char* src);</div></pre></td></tr></table></figure></p>
<p>实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">char * strcpy(char * dest, const char * src) // 实现src到dest的复制 </div><div class="line">&#123; </div><div class="line">　　if ((src == NULL) || (dest == NULL)) // 判断参数src和dest的有效性 </div><div class="line">　　&#123;</div><div class="line"></div><div class="line">　　　　　　return NULL; </div><div class="line">　　&#125; </div><div class="line">　　char *strdest = dest; // 保存目标字符串的首地址 </div><div class="line">　　while ((*dest++ = *src++)!=’\0’); // 把src字符串的内容复制到dest下 </div><div class="line">　　return strdest; </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h4 id="memcpy"><a href="#memcpy" class="headerlink" title="memcpy"></a>memcpy</h4><p>memcpy 提供了一般内存的复制。即 memcpy 对于需要复制的内容没有限制，因此用途更广。<br>函数的原型是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">void *memcpy( void *dest, const void *src, size_t count );</div></pre></td></tr></table></figure></p>
<p>实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">void *memcpy(void *memTo, const void *memFrom, size_t size) </div><div class="line">&#123; </div><div class="line">　　if((memTo == NULL) || (memFrom == NULL)) // memTo和memFrom必须有效 </div><div class="line">        return NULL; </div><div class="line">　　char* tempFrom = (char* )memFrom; // 保存memFrom首地址 </div><div class="line">　　char* tempTo = (char* )memTo; // 保存memTo首地址 </div><div class="line">　　while(size-–&gt;0) // 循环size次，复制memFrom的值到memTo中 </div><div class="line">        *tempTo++ = *tempFrom++ ; </div><div class="line">　　return memTo; </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>说明： memTo 和 memFrom 所指内存区域不能重叠，函数返回指向 memTo 的指针.可以拿它拷贝任何数据类型的对象。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">char a[10],b[5];              </div><div class="line">memcpy(b, a, sizeof(b)); /*注意如果用sizeof(a)，会造成b的内存地址溢出*/</div></pre></td></tr></table></figure></p>
<h4 id="memset"><a href="#memset" class="headerlink" title="memset"></a>memset</h4><p>把 buffer 所指内存区域的前 count 个字节设置成字符 c，主要用于初始化某个内存空间。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">extern void *memset(void *buffer, int c, int count);</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">char a[10];                        </div><div class="line">memset(a, &apos;\0&apos;, sizeof(a));</div></pre></td></tr></table></figure>
<h4 id="strcpy-和-memcpy-主要有以下3方面的区别。"><a href="#strcpy-和-memcpy-主要有以下3方面的区别。" class="headerlink" title="strcpy 和 memcpy 主要有以下3方面的区别。"></a>strcpy 和 memcpy 主要有以下3方面的区别。</h4><p>1、复制的内容不同。strcpy只能复制字符串，而memcpy可以复制任意内容，例如字符数组、整型、结构体、类等。<br>2、复制的方法不同。strcpy不需要指定长度，它遇到被复制字符的串结束符”\0”才结束，所以容易溢出。memcpy则是根据其第3个参数决定复制的长度。<br>3、用途不同。<br>emcpy和strcpy区别以及它们的效率分析<br>2015年08月13日 17:46:21<br>阅读数：3457<br>1、首先介绍这两个函数的原型：</p>
<p> #include <cstring><br>char <em>strcpy( char </em>to, const char *from );</cstring></p>
<p> #include <cstring><br>  void <em>memcpy( void </em>to, const void *from, size_t count );<br>从以上两个函数的参数类型和返回类型，可以看出两个函数的第一个不同点：<br>a、strcpy只能应用字符类型的复制，而memcpy应用范围更广，任何类型都可以；</cstring></p>
<p>其次，函数memcpy多了一个count参数，用于拷贝指定字节大小的数据。从这可以得出它们的第二个不同点，</p>
<p>b、memcpy相比使用strcpy会更加的安全，当然也可以使用strcpy的安全板本strncpy函数；</p>
<p>从表面我们还是不能看到这两个函数更加深入的异同，以及它们到底哪个效率更高。下面从它们的源码出发，期望可以解决问题。</p>
<p>2、strcpy函数和memcpy的源码的windows版本</p>
<p>strcpy函数源码：</p>
<p>[cpp] view plain copy<br>char <em> __cdecl strcpy(char </em> dst, const char <em> src)<br>{<br>        char </em> cp = dst;  </p>
<pre><code>while( *cp++ = *src++ )  
        ;               /* Copy src over dst */  

return( dst );  
</code></pre><p>}  </p>
<p>memcpy函数源码：<br>[cpp] view plain copy<br>void <em> __cdecl memcpy (<br>        void </em> dst,<br>        const void <em> src,<br>        size_t count<br>        )<br>{<br>        void </em> ret = dst;  </p>
<p>#if defined (_M_MRX000) || defined (_M_ALPHA) || defined (_M_PPC)<br>        {<br>        extern void RtlMoveMemory( void <em>, const void </em>, size_t count );  </p>
<pre><code>RtlMoveMemory( dst, src, count );  
}  
</code></pre><p>#else  /<em> defined (_M_MRX000) || defined (_M_ALPHA) || defined (_M_PPC) </em>/<br>        /* </p>
<pre><code> * copy from lower addresses to higher addresses 
 */  
while (count--) {  
        *(char *)dst = *(char *)src;  
        dst = (char *)dst + 1;  
        src = (char *)src + 1;  
}  
</code></pre><p>#endif  /<em> defined (_M_MRX000) || defined (_M_ALPHA) || defined (_M_PPC) </em>/  </p>
<pre><code>return(ret);  
</code></pre><p>}  </p>
<p>memcpy中的预定义不用考虑，现在都不支持了，先分析strcpy源码中关键代码：<br>[cpp] view plain copy<br>while( <em>cp++ = </em>src++ )<br>可以看出该循环退出的条件是*cp = ‘\0’，也就是说，源字符串中的结尾‘\0’也被拷贝到目的字符串中，这个很关键。<br>而memcpy中退出循环是count为0,也就是按照用户的要求，拷贝count个字节。</p>
<p>所以从这可以得出两个函数的第3个不同点：</p>
<p>C、strcpy一定会拷贝字符串结尾符’\0’，memcpy在拷贝字符串的时候，根据指定拷贝字节数拷贝字符串，是否拷贝‘\0’结束符，根据count的大小。</p>
<p>最后，从两个函数的源码中，可以分析出哪个函数的效率更快（当然是都拷贝相同的字符串），比较它们的关键代码，也就是循环，可以看出，memcpy函数中，有3个变量在变化，分别是count, dst, src，而strcpy只有两个变量变化，分别是cp和src，从这可以看出strcpy更胜一筹。再者，</p>
<p>[cpp] view plain copy<br><em>cp++ = </em>src++   和   <pre name="code" class="cpp"><em>(char </em>)dst = <em>(char </em>)src;<br>dst = (char <em>)dst + 1;<br>src = (char </em>)src + 1;<br>效果是一样，但下面耗费更大，它有类型转换的花费，在上面中没有的，从这也可以看出strcpy更胜一筹。</pre></p>
<p>综上两点，我认为在拷贝相同字符串，相同字节的情况下，strcpy的效率比memcpy效率更高。</p>
<p>3、下面再实践证明下，如下图。</p>
<p>测试代码：</p>
<p>[cpp] view plain copy</p>
<p>#include <stdio.h>  </stdio.h></p>
<p>#include <string.h><br>int main()<br>{<br>    char src[] = “hello,memcpy and strcpy!”;<br>    char dest[32];<br>    memcpy(dest, src, 25);<br>    //strcpy(dest, src);<br>    return 0;<br>}  </string.h></p>
<p>运行上面代码用时如下：</p>
<p>将memcpy(dest, src, 25);注释，取消下面strcpy(dest, src）运行代码用时如下：</p>
<p>从上可以看出，strcpy以微弱的优势，相比memcpy效率更快。</p>
<p>所以memcpy和strcpy的第四个不同点是：</p>
<p>d、在拷贝相同的字符串，且字节数相同(包括‘]0’)的情况下，strcpy效率比memcpy效率更快。</p>
<p>4、总结，memcpy和strcpy的区别与比较</p>
<p>a、strcpy只能应用字符类型的复制，而memcpy应用范围更广，任何类型都可以；</p>
<p>b、memcpy相比使用strcpy会更加的安全，当然也可以使用strcpy的安全板本strncpy函数；</p>
<p>c、strcpy一定会拷贝字符串结尾符’\0’，memcpy在拷贝字符串的时候，根据指定拷贝字节数拷贝字符串，是否拷贝‘\0’结束符，根据count的大小;</p>
<p>d、在拷贝相同的字符串，且字节数相同(包括‘]0’)的情况下，strcpy效率比memcpy效率更快。</p>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/05/17/ssd-源码分析/">
                ssd 源码分析
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-05-17</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p>本文主要讲解 Annotations 中的 label 和 bboxes 是如何在 SSD 中使用的。<br>网络的输入 <code>glabels</code> 和 <code>gbboxes</code> 是无法直接和网络的输出进行比较并计算 losses 的。因此需要首先通过 <code>bboxes_encode</code> 进行编码，得到对应于不同特征层（block）的 <code>feat_labels</code>，<code>feat_localizations</code>，<code>feat_scores</code>(杰卡德系数)，再通过<br><code>feat_labels</code>，<code>feat_localizations</code> 与网络不同特征层（block）的输出进行比较并计算 losses。</p>
<h3 id="anchors-的生成"><a href="#anchors-的生成" class="headerlink" title="anchors 的生成"></a>anchors 的生成</h3><h5 id="维度："><a href="#维度：" class="headerlink" title="维度："></a>维度：</h5><p>anchors type: list A of list B<br>len(A): num_of_blocks, for 300 it’s 6, for 512 it’s 7.<br>B: [y, x, h, w]<br>shape of y, x: for block 4 of 300 is 38 <em> 38 </em> 1(1 is an expanded dim)<br>shape of h, w: for block 4 of 300 is (len(sizes) + len(ratios) = 4, )</p>
<h5 id="生成过程-of-300"><a href="#生成过程-of-300" class="headerlink" title="生成过程 of 300"></a>生成过程 of 300</h5><ol>
<li>feat_shapes: [(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)]<br>分别对应 blocks: [‘block4’, ‘block7’, ‘block8’, ‘block9’, ‘block10’, ‘block11’]</li>
<li>anchor_sizes: [(21., 45.), (45., 99.), (99., 153.), (153., 207.), (207., 261.), (261., 315.)]</li>
<li>anchor_ratios: [[2, .5], [2, .5, 3, 1./3], [2, .5, 3, 1./3], [2, .5, 3, 1./3], [2, .5], [2, .5]]</li>
<li>anchor_steps: [8, 16, 32, 64, 100, 300]</li>
<li>anchor_offset: 0.5<br>For the first feature map (block 4), the y, x:<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">y, x = np.mgrid[0:feat_shape[0], 0:feat_shape[1]]</div><div class="line">y = (y.astype(dtype) + offset) * step / img_shape[0]</div><div class="line">x = (x.astype(dtype) + offset) * step / img_shape[1]</div><div class="line"></div><div class="line"># Expand dims to support easy broadcasting.</div><div class="line">y = np.expand_dims(y, axis=-1)</div><div class="line">x = np.expand_dims(x, axis=-1)</div></pre></td></tr></table></figure>
</li>
</ol>
<p>(38 - 1 + 0.5) * 8 = 300<br>So the range of y, x is [4/300, 1.0].<br>y, x 在后续的处理中是当作中心使用的。<br>对于各个 block 的 y，x ，它们都是映射到（大概）原图上的 (0, 1) 之间的值，标记着一些散布在原图上的锚点。</p>
<p>For the first feature map (block 4), the h, w:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">num_anchors = len(sizes) + len(ratios)</div><div class="line">h = np.zeros((num_anchors, ), dtype=dtype)</div><div class="line">w = np.zeros((num_anchors, ), dtype=dtype)</div><div class="line"># Add first anchor boxes with ratio=1.</div><div class="line">h[0] = sizes[0] / img_shape[0]</div><div class="line">w[0] = sizes[0] / img_shape[1]</div><div class="line">di = 1</div><div class="line">if len(sizes) &gt; 1:</div><div class="line">    h[1] = math.sqrt(sizes[0] * sizes[1]) / img_shape[0]</div><div class="line">    w[1] = math.sqrt(sizes[0] * sizes[1]) / img_shape[1]</div><div class="line">    di += 1</div><div class="line">for i, r in enumerate(ratios):</div><div class="line">    h[i+di] = sizes[0] / img_shape[0] / math.sqrt(r)</div><div class="line">    w[i+di] = sizes[0] / img_shape[1] * math.sqrt(r)</div></pre></td></tr></table></figure></p>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/hw.jpg" alt=""></p>
<p>feat_shapes 并非任意指定的值，而真的就是来自卷积神经网络的某一层特征图的尺寸值，即该层特征图的输出尺寸决定了 feat_shapes，feat_shapes 用来指导 anchors 的生成，anchors 的生成决定了神经网络输入的编码，最终<strong>特征图的输出</strong>和<strong>输入编码</strong>之间计算 Loss，指导神经网络的学习过程。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">def ssd_multibox_layer(inputs,</div><div class="line">                       num_classes,</div><div class="line">                       sizes,</div><div class="line">                       ratios=[1],</div><div class="line">                       normalization=-1,</div><div class="line">                       bn_normalization=False):</div><div class="line">    &quot;&quot;&quot;Construct a multibox layer, return a class and localization predictions.</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    net = inputs</div><div class="line">    if normalization &gt; 0:</div><div class="line">        net = custom_layers.l2_normalization(net, scaling=True)</div><div class="line">    # Number of anchors.</div><div class="line">    num_anchors = len(sizes) + len(ratios)</div><div class="line"></div><div class="line">    # Location.</div><div class="line">    num_loc_pred = num_anchors * 4</div><div class="line">    loc_pred = slim.conv2d(net, num_loc_pred, [3, 3], activation_fn=None,</div><div class="line">                           scope=&apos;conv_loc&apos;)</div><div class="line">    loc_pred = custom_layers.channel_to_last(loc_pred)</div><div class="line">    loc_pred = tf.reshape(loc_pred,</div><div class="line">                          tensor_shape(loc_pred, 4)[:-1]+[num_anchors, 4])</div><div class="line">    # Class prediction.</div><div class="line">    num_cls_pred = num_anchors * num_classes</div><div class="line">    cls_pred = slim.conv2d(net, num_cls_pred, [3, 3], activation_fn=None,</div><div class="line">                           scope=&apos;conv_cls&apos;)</div><div class="line">    cls_pred = custom_layers.channel_to_last(cls_pred)</div><div class="line">    cls_pred = tf.reshape(cls_pred,</div><div class="line">                          tensor_shape(cls_pred, 4)[:-1]+[num_anchors, num_classes])</div><div class="line">    return cls_pred, loc_pred</div></pre></td></tr></table></figure></p>
<h3 id="输入编码"><a href="#输入编码" class="headerlink" title="输入编码"></a>输入编码</h3><p><code>bboxes_encode -&gt; tf_ssd_bboxes_encode -&gt; tf_ssd_bboxes_encode_layer</code><br>input: labels, bboxes of a single image, not a batch of images, because the call of bboxes_encode is invoked before the construction of batch.<br>labels: such as [1, 1, 3]<br>bboxes: such as [[…, …, …, …], […, …, …, …], […, …, …, …]]</p>
<h6 id="编码过程"><a href="#编码过程" class="headerlink" title="编码过程"></a>编码过程</h6><p>一张图，一份 labels ，一份 bboxes，一个 anchor_layer ====&gt;&gt;&gt;&gt; for 循环遍历 labels 和 bboxes，不断计算杰卡德系数，更新属于该 anchor_layer 的对应于该图的 feat_labels 和 feat_scores(也就是杰卡德系数)。<br><img src="http://oytnj8g2y.bkt.clouddn.com/jaccard.jpg" alt=""><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div></pre></td><td class="code"><pre><div class="line">def tf_ssd_bboxes_encode_layer(labels,</div><div class="line">                               bboxes,</div><div class="line">                               anchors_layer,</div><div class="line">                               num_classes,</div><div class="line">                               no_annotation_label,</div><div class="line">                               ignore_threshold=0.5,</div><div class="line">                               prior_scaling=[0.1, 0.1, 0.2, 0.2],</div><div class="line">                               dtype=tf.float32):</div><div class="line">    # 这里的 anchors_layer 就是上面讲到的 anchors 生成中的一个</div><div class="line">    # 下面的代码利用到了 numpy 的 auto broadcasting，</div><div class="line">    # 使得 ymin，xmin，ymax，xmax，vol_anchors 的维度都成为类似 block4 的 (38, 38, 4)。</div><div class="line">    yref, xref, href, wref = anchors_layer</div><div class="line">    ymin = yref - href / 2.</div><div class="line">    xmin = xref - wref / 2.</div><div class="line">    ymax = yref + href / 2.</div><div class="line">    xmax = xref + wref / 2.</div><div class="line">    vol_anchors = (xmax - xmin) * (ymax - ymin)</div><div class="line"></div><div class="line">    # feat_labels，feat_scores，feat_ymin，feat_xmin，feat_ymax，feat_xmax 的维度</div><div class="line">    # 都成为类似 block4 的 (38, 38, 4)。</div><div class="line">    shape = (yref.shape[0], yref.shape[1], href.size)</div><div class="line">    feat_labels = tf.zeros(shape, dtype=tf.int64)</div><div class="line">    feat_scores = tf.zeros(shape, dtype=dtype)</div><div class="line">    feat_ymin = tf.zeros(shape, dtype=dtype)</div><div class="line">    feat_xmin = tf.zeros(shape, dtype=dtype)</div><div class="line">    feat_ymax = tf.ones(shape, dtype=dtype)</div><div class="line">    feat_xmax = tf.ones(shape, dtype=dtype)</div><div class="line"></div><div class="line">    # 计算一个 bbox 与该 anchor_layer 上所有 anchor 的杰卡德系数</div><div class="line">    # 返回的 jaccard 维度也类似于 (38, 38, 4)</div><div class="line">    def jaccard_with_anchors(bbox):</div><div class="line">        &quot;&quot;&quot;Compute jaccard score between a box and the anchors.</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        int_ymin = tf.maximum(ymin, bbox[0])</div><div class="line">        int_xmin = tf.maximum(xmin, bbox[1])</div><div class="line">        int_ymax = tf.minimum(ymax, bbox[2])</div><div class="line">        int_xmax = tf.minimum(xmax, bbox[3])</div><div class="line">        h = tf.maximum(int_ymax - int_ymin, 0.)</div><div class="line">        w = tf.maximum(int_xmax - int_xmin, 0.)</div><div class="line">        # Volumes.</div><div class="line">        inter_vol = h * w</div><div class="line">        union_vol = vol_anchors - inter_vol \</div><div class="line">            + (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])</div><div class="line">        jaccard = tf.div(inter_vol, union_vol)</div><div class="line">        return jaccard</div><div class="line"></div><div class="line">    def condition(i, feat_labels, feat_scores,</div><div class="line">                    feat_ymin, feat_xmin, feat_ymax, feat_xmax):</div><div class="line">        &quot;&quot;&quot;Condition: check label index.</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        r = tf.less(i, tf.shape(labels))</div><div class="line">        return r[0]</div><div class="line"></div><div class="line">    def body(i, feat_labels, feat_scores,</div><div class="line">                feat_ymin, feat_xmin, feat_ymax, feat_xmax):</div><div class="line">        &quot;&quot;&quot;Body: update feature labels, scores and bboxes.</div><div class="line">        Follow the original SSD paper for that purpose:</div><div class="line">            - assign values when jaccard &gt; 0.5;</div><div class="line">            - only update if beat the score of other bboxes.</div><div class="line">        &quot;&quot;&quot;</div><div class="line"></div><div class="line">        label = labels[i]</div><div class="line">        bbox = bboxes[i]</div><div class="line">        jaccard = jaccard_with_anchors(bbox)</div><div class="line"></div><div class="line">        # mask 代表仅更新 jaccard 系数大于当前的 feat_scores 的，且 label 有效的部分，其余部分不更新。</div><div class="line">        # label 与 num_classes 的大小关系一定要对，这在 mask = tf.logical_and(mask, label &lt; num_classes) </div><div class="line">        # 中提出了要求，</div><div class="line">        # 一旦 label &lt; num_classes 为假，那么 mask 就会被全部清零，失去更新 feat_labels 和 feat_scores 的机会，</div><div class="line">        # 从而无法作为输入参与到训练中来。</div><div class="line">        # 这会影响到 mAP么？ TO DO!!!!</div><div class="line"></div><div class="line">        mask = tf.greater(jaccard, feat_scores)</div><div class="line">        mask = tf.logical_and(mask, feat_scores &gt; -0.5)</div><div class="line">        mask = tf.logical_and(mask, label &lt; num_classes)</div><div class="line">        imask = tf.cast(mask, tf.int64)</div><div class="line">        fmask = tf.cast(mask, dtype)</div><div class="line"></div><div class="line">        # Update values using mask.</div><div class="line">        feat_labels = imask * label + (1 - imask) * feat_labels</div><div class="line">        feat_scores = tf.where(mask, jaccard, feat_scores)</div><div class="line">        feat_ymin = fmask * bbox[0] + (1 - fmask) * feat_ymin</div><div class="line">        feat_xmin = fmask * bbox[1] + (1 - fmask) * feat_xmin</div><div class="line">        feat_ymax = fmask * bbox[2] + (1 - fmask) * feat_ymax</div><div class="line">        feat_xmax = fmask * bbox[3] + (1 - fmask) * feat_xmax</div><div class="line">        return [i+1, feat_labels, feat_scores,</div><div class="line">                feat_ymin, feat_xmin, feat_ymax, feat_xmax]</div><div class="line">    # Main loop definition.</div><div class="line">    i = 0</div><div class="line">    [i, feat_labels, feat_scores,</div><div class="line">        feat_ymin, feat_xmin,</div><div class="line">        feat_ymax, feat_xmax] = tf.while_loop(condition, body,</div><div class="line">                                            [i, feat_labels, feat_scores,</div><div class="line">                                            feat_ymin, feat_xmin,</div><div class="line">                                            feat_ymax, feat_xmax])</div><div class="line">    </div><div class="line">    # Transform to center / size.</div><div class="line">    feat_cy = (feat_ymax + feat_ymin) / 2.</div><div class="line">    feat_cx = (feat_xmax + feat_xmin) / 2.</div><div class="line">    feat_h = feat_ymax - feat_ymin</div><div class="line">    feat_w = feat_xmax - feat_xmin</div><div class="line"></div><div class="line">    # Encode features.</div><div class="line">    feat_cy = (feat_cy - yref) / href / prior_scaling[0]</div><div class="line">    feat_cx = (feat_cx - xref) / wref / prior_scaling[1]</div><div class="line">    feat_h = tf.log(feat_h / href) / prior_scaling[2]</div><div class="line">    feat_w = tf.log(feat_w / wref) / prior_scaling[3]</div><div class="line"></div><div class="line">    # Use SSD ordering: x / y / w / h instead of ours.</div><div class="line">    feat_localizations = tf.stack([feat_cx, feat_cy, feat_w, feat_h], axis=-1)</div><div class="line">    return feat_labels, feat_localizations, feat_scores</div></pre></td></tr></table></figure></p>
<p>shape of feat_labels, feat_scores is: such as 38 38 4<br>feat_localizations: such as 38 38 4 4<br>feat_localizations 中的每一个值都不是绝对数值，而是相对数值并经过了缩放编码，缩放可控制坐标、长宽两者之间对于 loss 的相对影响力，以及它们和 feat_labels 之间对于 loss的相对影响力。<br>这样就将 glabels 和 gbboxes 通过杰卡德系数这一标准映射到了 anchors 上，glabels 没什么可说的，就是给了 anchor 一个标签，关于 gbboxes 则是给了 anchor 一个绝对的上下左右的界限，并在后续将这个界限转成了相对的缩放后的中心位置和长宽。<br>即将 glabels 和 gbboxes 编到了 anchors 上，形成了图 feat_labels, feat_localizations, feat_scores。</p>
<h3 id="losses-的计算"><a href="#losses-的计算" class="headerlink" title="losses 的计算"></a>losses 的计算</h3><p>logits shape：类似于 (N, 38, 38, 4, num_classes)<br>计算的思想就是首先根据 groundtruth 构建正负样本，gscores 中杰卡德系数大于门限值的对应的就是正样本，小于门限值的对应的就是负样本。由于负样本会远远多于正样本，因此只选择预测最差的一部分进行 hard mining。分别计算在正负样本上的loss并综合起来。一张图片会产生很多个（可能多于目标个数）的正样本以及很多的负样本。<br>上一步的输入编码之后得到的一系列 feat_labels, feat_localizations, feat_scores，分别组装成 list 输入到下面的函数用于与网络的各个特征 block 的输出计算 loss。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div></pre></td><td class="code"><pre><div class="line"># 参数 logits, localisations, gclasses, glocalisations, gscores 都是对应的是一个批次 batch。</div><div class="line">def ssd_losses(logits, localisations,</div><div class="line">               gclasses, glocalisations, gscores,</div><div class="line">               match_threshold=0.5,</div><div class="line">               negative_ratio=3.,</div><div class="line">               alpha=1.,</div><div class="line">               label_smoothing=0.,</div><div class="line">               scope=None):</div><div class="line">    with tf.name_scope(scope, &apos;ssd_losses&apos;):</div><div class="line">        l_cross_pos = []</div><div class="line">        l_cross_neg = []</div><div class="line">        l_loc = []</div><div class="line">        # 一个 block 一个 block 的来处理</div><div class="line">        for i in range(len(logits)):</div><div class="line">            dtype = logits[i].dtype</div><div class="line">            with tf.name_scope(&apos;block_%i&apos; % i):</div><div class="line">                # positive mask 正样本的 mask</div><div class="line">                pmask = gscores[i] &gt; match_threshold</div><div class="line">                # final positive mask</div><div class="line">                fpmask = tf.cast(pmask, dtype)</div><div class="line">                n_positives = tf.reduce_sum(fpmask)</div><div class="line"></div><div class="line">                # Negative mask. 负样本的 mask，并利用 nvalues 进行筛选。</div><div class="line">                # no_classes 与所有目标重合度不足的为0，表示是背景，1 表示是目标。one vs others。</div><div class="line">                no_classes = tf.cast(pmask, tf.int32)</div><div class="line">                predictions = slim.softmax(logits[i])</div><div class="line">                nmask = tf.logical_and(tf.logical_not(pmask),</div><div class="line">                                       gscores[i] &gt; -0.5)</div><div class="line">                fnmask = tf.cast(nmask, dtype)</div><div class="line">                # nvalues 若anchor是目标则为1， 若anchor是背景则为预测为背景的概率，</div><div class="line">                # nvalues 的值用来筛选 hard negative，如果anchor 即 groundtruth 为背景，</div><div class="line">                # 而predictions[:, :, :, :, 0]很小，则其为 hard negative。</div><div class="line">                # 由 predictions[:, :, :, :, 0] 可知将背景作为一个0类“目标”的必要性。</div><div class="line">                nvalues = tf.where(nmask,</div><div class="line">                                   predictions[:, :, :, :, 0],</div><div class="line">                                   1. - fnmask)</div><div class="line">                nvalues_flat = tf.reshape(nvalues, [-1])</div><div class="line">                # Number of negative entries to select.</div><div class="line">                n_neg = tf.cast(negative_ratio * n_positives, tf.int32)</div><div class="line">                n_neg = tf.maximum(n_neg, tf.size(nvalues_flat) // 8)</div><div class="line">                n_neg = tf.maximum(n_neg, tf.shape(nvalues)[0] * 4)</div><div class="line">                max_neg_entries = 1 + tf.cast(tf.reduce_sum(fnmask), tf.int32)</div><div class="line">                n_neg = tf.minimum(n_neg, max_neg_entries)</div><div class="line"></div><div class="line">                val, idxes = tf.nn.top_k(-nvalues_flat, k=n_neg)</div><div class="line">                minval = val[-1]</div><div class="line">                # Final negative mask.</div><div class="line">                nmask = tf.logical_and(nmask, -nvalues &gt; minval)</div><div class="line">                fnmask = tf.cast(nmask, dtype)</div><div class="line"></div><div class="line">                # Add cross-entropy loss.</div><div class="line">                with tf.name_scope(&apos;cross_entropy_pos&apos;):</div><div class="line">                    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[i],</div><div class="line">                                                                          labels=gclasses[i])</div><div class="line">                    # 只计算 positive 的loss</div><div class="line">                    loss = tf.losses.compute_weighted_loss(loss, fpmask)</div><div class="line">                    l_cross_pos.append(loss)</div><div class="line"></div><div class="line">                with tf.name_scope(&apos;cross_entropy_neg&apos;):</div><div class="line">                    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[i],</div><div class="line">                                                                          labels=no_classes)</div><div class="line">                    # 只计算 hard negative 的 loss</div><div class="line">                    loss = tf.losses.compute_weighted_loss(loss, fnmask)</div><div class="line">                    l_cross_neg.append(loss)</div><div class="line"></div><div class="line">                # Add localization loss: smooth L1, L2, ...</div><div class="line">                with tf.name_scope(&apos;localization&apos;):</div><div class="line">                    # Weights Tensor: positive mask + random negative.</div><div class="line">                    weights = tf.expand_dims(alpha * fpmask, axis=-1)</div><div class="line">                    loss = custom_layers.abs_smooth(localisations[i] - glocalisations[i])</div><div class="line">                    loss = tf.losses.compute_weighted_loss(loss, weights)</div><div class="line">                    l_loc.append(loss)</div><div class="line"></div><div class="line">        # Additional total losses...</div><div class="line">        with tf.name_scope(&apos;total&apos;):</div><div class="line">            total_cross_pos = tf.add_n(l_cross_pos, &apos;cross_entropy_pos&apos;)</div><div class="line">            total_cross_neg = tf.add_n(l_cross_neg, &apos;cross_entropy_neg&apos;)</div><div class="line">            total_cross = tf.add(total_cross_pos, total_cross_neg, &apos;cross_entropy&apos;)</div><div class="line">            total_loc = tf.add_n(l_loc, &apos;localization&apos;)</div><div class="line"></div><div class="line">            # Add to EXTRA LOSSES TF.collection</div><div class="line">            tf.add_to_collection(&apos;EXTRA_LOSSES&apos;, total_cross_pos)</div><div class="line">            tf.add_to_collection(&apos;EXTRA_LOSSES&apos;, total_cross_neg)</div><div class="line">            tf.add_to_collection(&apos;EXTRA_LOSSES&apos;, total_cross)</div><div class="line">            tf.add_to_collection(&apos;EXTRA_LOSSES&apos;, total_loc)</div></pre></td></tr></table></figure></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>整个过程就是生成 anchors，将 labels 和 bboxes 编码到 anchors 上，并和网络的输出比较分别计算出正负样本以及位置的loss。<br>重要的点是：</p>
<ol>
<li>num_classes 与每一个 label 的值关系要对，对于 num_classes =4 ,其 labels 就应该为[0, 1, 2, 3]，而不能是大于等于 num_classes 的其他值。</li>
<li>背景的 label 必须设为 0。</li>
<li><p>sparse_softmax_cross_entropy_with_logits 的实现就是 label = 0 对应着 logits 的第一“列”，label = 1 就对应着 logits 的第二“列”，以此类推，因此如果在设置类名和label之间的映射时出现错误则会使得 loss 的计算不能正确反映真实的loss。比如 num_classes = 3，此时logits为三列，而如果 labels = [0, 1, 3]，如此这般时 loss 的计算就会受到影响。</p>
<ul>
<li>labels: Tensor of shape [d_0, d_1, …, d_{r-1}] (where r is rank of labels and result) and dtype int32 or int64. Each entry in labels must be an index in [0, num_classes). Other values will raise an exception when this op is run on CPU, and return NaN for corresponding loss and gradient rows on GPU.</li>
<li>logits: Unscaled log probabilities of shape [d_0, d_1, …, d_{r-1}, num_classes] and dtype float32 or float64.</li>
</ul>
</li>
</ol>

        </div>
    

</div>
            
        </section>
    </div>
</div>



    <div class="row">
        <div class="col-sm-12">
            <div class="wrap-pagination">
                <a class="" href="/">
                    <i class="fa fa-chevron-left" aria-hidden="true"></i>
                </a>
                <a class="" href="/page/3/">
                    <i class="fa fa-chevron-right" aria-hidden="true"></i>
                </a>
            </div>
        </div>
    </div>




</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This is Robert Lexis (FengCun Li). To see the world, things dangerous to come to, to see behind walls, to draw closer, to find each other and to feel. That is the purpose of LIFE.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2018/08/13/消失的梯度/">消失的梯度</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/11/MobileNet-V1-and-V2-带来的卷积结构革命/">MobileNet V1 and V2 带来的卷积</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/09/卷积转置卷积关系在-TensorFLow-中的验证/">卷积转置卷积关系在 TensorFLow 中的验证</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/08/转置卷积-Transposed-Convolution/">转置卷积 Transposed Convoluti</a>
            </li>
            
        </ul>
    </div>



            
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/RobertLexis">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:robert_lexis@163.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Robert Lexis Loves Wenny
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



</body>

</html>