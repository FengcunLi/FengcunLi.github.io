<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    

    <!--Author-->
    
        <meta name="author" content="Robert Lexis">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="This is Robert Lexis."/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="This is Robert Lexis."/>

    <!--Type page-->
    
        <meta property="og:type" content="website" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>page - This is Robert Lexis.</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about.html">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact.html">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
            <h1 id="main-title" class="title">This is Robert Lexis.</h1>
        
    </div>
</header>

        <section class="main">
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/02/11/C-类型转换操作符的重载/">
                C++类型转换操作符的重载
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-02-11</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h2 id="C-的类型转换运算符的重载"><a href="#C-的类型转换运算符的重载" class="headerlink" title="C++ 的类型转换运算符的重载"></a>C++ 的类型转换运算符的重载</h2><h3 id="类型转换可以通过类型转换运算符和构造函数两种方式完成"><a href="#类型转换可以通过类型转换运算符和构造函数两种方式完成" class="headerlink" title="类型转换可以通过类型转换运算符和构造函数两种方式完成"></a>类型转换可以通过类型转换运算符和构造函数两种方式完成</h3><p>在需要做数据类型转换时，一般显式的写法是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">type2 d;  </div><div class="line">i = (type1)d; // 显式的类型转换，把 d 从 type2 类型转为 type1 类型</div></pre></td></tr></table></figure></p>
<p>这种写法不能做到隐式转换，也就是直接写 i = d。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">class Integer  </div><div class="line">&#123;  </div><div class="line">public:  </div><div class="line">    Integer(int v);  </div><div class="line">    Integer&amp; opetaror=(const int &amp;v); //重载赋值运算符，可以做到 Integer i = 10（错）; 这里原博文博主犯了一个错误，Integer it = 10 调用的是构造函数而不是赋值运算符，因为有新对象的生成必然有构造函数的调用。</div><div class="line">    Integer it; it=10; 也是错误的，这是因为 10 不是一个变量，不存在对其的引用。</div><div class="line">    正确的说法是： Integer it; int i=10; it=i;</div><div class="line">    operator int(); // 重载类型转换运算符，可以做到 int i; Integer it;  i = it;</div><div class="line">private:  </div><div class="line">    int data;</div><div class="line">&#125;;</div><div class="line">Integer::Integer(int v)</div><div class="line">&#123;  </div><div class="line">    data = v;</div><div class="line">&#125;  </div><div class="line"></div><div class="line">Integer&amp; Integer::opetaror =(const int &amp;v)  </div><div class="line">&#123;  </div><div class="line">    data = v;  </div><div class="line">    return *this;  </div><div class="line">&#125;  </div><div class="line">Integer::operator int()</div><div class="line">&#123;  </div><div class="line">    return data;  </div><div class="line">&#125;  </div><div class="line"> </div><div class="line">#include &lt;iostream&gt;  </div><div class="line">using namespace std;  </div><div class="line">  </div><div class="line">int main()  </div><div class="line">&#123;  </div><div class="line">    Integer integer1(10);</div><div class="line">    Integer integer2;  </div><div class="line">    int i1 = 10;  </div><div class="line">    integer2 = i1;</div><div class="line">  </div><div class="line">    int i2;  </div><div class="line">    i2 = integer1; //integer1是Integer类型，这里隐式把Integer类型转换为int，然后赋值给i2  </div><div class="line">                   //如果没有重载了int()，需要类型转换的话，这里必须写成 i2 = (int)integer1;</div><div class="line">    return 0;  </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="重载类型转换需要注意的问题"><a href="#重载类型转换需要注意的问题" class="headerlink" title="重载类型转换需要注意的问题"></a>重载类型转换需要注意的问题</h3><p>转换函数采用如下通用形式: <code>operator type();</code><br>这里，type表示内置类型、类类型或有类型别名。一般而言，不允许转换为数组或函数类型，转换为指针类型(数组和函数指针)以及引用类型是可以的。</p>
<p>注解：</p>
<ol>
<li>转换函数必须是成员函数</li>
<li>不能指定返回类型</li>
<li>形参列表必须为空。<br>下述所有的声明都是错误的：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">operator int(Small Int&amp;); // error:nonmember</div><div class="line">class SmallInt</div><div class="line">&#123;</div><div class="line">public:</div><div class="line">    int operator int(); // error: return list</div><div class="line">    operator int(int = 0);  // error:parameter list</div><div class="line">// ...</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
</li>
</ol>
<p>虽然转换函数不能指定返回类型，但是每个转换函数必须显式返回一个指定的值。例如，operator int 返回一个int值；如果定义operator Sales_item,它将返回一个Sales_item对象，诸如此类。</p>
<h3 id="最佳实践-转换函数一般不应该改变被转换的对象。因此，类型转换运算符通常应定义为const成员。"><a href="#最佳实践-转换函数一般不应该改变被转换的对象。因此，类型转换运算符通常应定义为const成员。" class="headerlink" title="最佳实践: 转换函数一般不应该改变被转换的对象。因此，类型转换运算符通常应定义为const成员。"></a>最佳实践: 转换函数一般不应该改变被转换的对象。因此，类型转换运算符通常应定义为const成员。</h3><ol>
<li>使用类类型转换<br>只要存在转换，编译器将在可以使用隐式转换的地方自动调用它<br>在表达式中:<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">SmallInt si;</div><div class="line">double dval;</div><div class="line">si &gt;= dval // si converted to int and then convert to bool</div></pre></td></tr></table></figure>
</li>
</ol>
<p>在条件中:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">if(si) // si convert to int and then convert to bool</div></pre></td></tr></table></figure></p>
<p>将实参传给函数或从函数返回：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">int calc(int);</div><div class="line">SmallInt si;</div><div class="line">int i = calc(si); // convert si to int and call calc</div></pre></td></tr></table></figure></p>
<p>作为重载运算符的运算数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">// convert si to int then call &lt;&lt; on the int value</div><div class="line"></div><div class="line">cout &lt;&lt; si &lt;&lt; endl;</div></pre></td></tr></table></figure></p>
<p>在显示类型转换中:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">int ival;</div><div class="line"></div><div class="line">SmallInt si = 3.54;</div><div class="line"></div><div class="line">// instruct compiler to cast si to int</div><div class="line"></div><div class="line">ival = static_cast&lt;int&gt;(si) + 3;</div></pre></td></tr></table></figure></p>
<ol>
<li>使用类类型转换和标准转换<br>使用转换函数时，被转换的类型不必与所在需要的类型完全匹配。必要时可在类类型转换之后跟上标准转换以获得想要的类型。例如，在一个SmallInt对象与一个double值的比较中：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">SmallInt si;</div><div class="line"></div><div class="line">double dval;</div><div class="line"></div><div class="line">si &gt;= dval // si converted to int and then convert to double.</div></pre></td></tr></table></figure>
</li>
</ol>
<p>首先将si从SmallInt对象转换为int值，然后将该int值转换为double值。</p>
<ol>
<li>只能应用一个类类型转换<br>类类型转换之后再跟另一个类类型转换。如果需要多个类类型转换，则代码将出错。<br>例如，假定有一个类Integral,它可以转换为SmallInt但不能转换为int:<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">// class to hold unsigned integral values</div><div class="line">class Integral</div><div class="line">&#123;</div><div class="line">public:</div><div class="line">    Integral(int i = 0):val(i)&#123;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    operator SmallInt() const</div><div class="line">    &#123;</div><div class="line">        return val % 256; // 这里就存在一个构造函数的调用，完成了隐式类型转换。</div><div class="line">    &#125;</div><div class="line">private:</div><div class="line">    std::size_t val;</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
</li>
</ol>
<p>可以在需要 SmallInt 的地方使用 Integral，但不能再需要 int 的地方使用 Integral:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">int calc(int);</div><div class="line">Integral intVal;</div><div class="line">SmallInt si(intVal); // ok:convert intVal to Small Int and copy to si</div><div class="line">int i = calc(si);  // ok:convert si to int and call calc</div><div class="line">int j = calc(intVal);// error:on convertion to int from Integral</div></pre></td></tr></table></figure></p>
<ol>
<li>标准转换可放在类类型转换之前<br>使用构造函数执行隐式转换的时候，构造函数的形参类型不必与所提供的类型完全匹配。例如，下面的代码调用SmallInt类中定义的构造函数SmallInt(int)将sobj转换为SmallInt类型:<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">void calc(SmallInt);</div><div class="line"></div><div class="line">short sobj;</div><div class="line"></div><div class="line">// sobj prometed from short to int </div><div class="line"></div><div class="line">// that int converted to SmallInt through the SmallInt(int) constructor</div><div class="line"></div><div class="line">calc(sobj);</div></pre></td></tr></table></figure>
</li>
</ol>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/02/10/C-const-extern-static-volatile/">
                C++ const/extern/static/volatile
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-02-10</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h2 id="C-C-中的-const-关键字"><a href="#C-C-中的-const-关键字" class="headerlink" title="C/C++ 中的 const 关键字"></a>C/C++ 中的 const 关键字</h2><p>const 可以用来修饰变量，指针，函数参数，函数返回值，成员函数本身。而后三者是 const 最具威力的用法。<br>const 成员函数有两个语义：</p>
<ol>
<li>使得 class 的接口更加容易被理解，方便的知道哪个函数可以修改调用对象的内容，哪个函数不行。</li>
<li>使得区别对待常量调用对象与非常量调用对象成为可能。（两个成员函数仅仅是常量性不同是可以被重载的）<br>改善 C++ 程序效率的一个根本办法是 pass-by-reference-to-const</li>
</ol>
<h2 id="C-C-中extern关键字详解"><a href="#C-C-中extern关键字详解" class="headerlink" title="C/C++中extern关键字详解"></a>C/C++中extern关键字详解</h2><h3 id="基本解释"><a href="#基本解释" class="headerlink" title="基本解释"></a>基本解释</h3><pre><code>+ extern 暗示这个变量/函数的定义存在于别的源文件中
+ 用来进行链接指定，当它与&quot;C&quot;一起连用时，如: extern &quot;C&quot; void fun(int a, int b);则告诉编译器在编译fun这个函数名时按着C的规则去翻译，而不是C++的，因为C++支持函数重载。
</code></pre><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol>
<li>extern 变量<br>在一个源文件里定义了一个数组：char a[6];在另外一个文件里用下列语句进行了声明：extern char *a；<br>请问，这样可以吗？<br>答案与分析：<ol>
<li>不可以，程序运行时会告诉你非法访问。原因在于，类型T的指针并不等价于类型T的数组。extern char *a声明的是一个字符指针而不是字符数组，因此与实际的定义不同，从而造成运行时非法访问。应该将声明改为extern char a[]。</li>
<li>在使用extern时候要严格对应声明时的格式，在实际编程中，这样的错误屡见不鲜。你在* .c文件中声明了一个全局的变量，这个全局的变量如果要被其他*.c引用，就放在*.h中并用 extern 来声明。</li>
</ol>
</li>
<li><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">//在.h文件的头上</div><div class="line">#ifdef __cplusplus</div><div class="line">#if __cplusplus</div><div class="line">extern &quot;C&quot;&#123;</div><div class="line">　#endif</div><div class="line">　#endif /* __cplusplus */ </div><div class="line">    ...............</div><div class="line">    ...............</div><div class="line">　#ifdef __cplusplus</div><div class="line">　#if __cplusplus</div><div class="line">&#125;</div><div class="line">#endif</div><div class="line">#endif /* __cplusplus */</div></pre></td></tr></table></figure>
</li>
<li><p>常常见extern放在函数的前面成为函数声明的一部分，那么，C语言的关键字extern在函数的声明中起什么作用？<br> 答案与分析：<br> 仅仅是暗示这个函数的定义存在于别的源文件中。</p>
</li>
<li>extern 和 static<ul>
<li>extern 暗示这个变量的定义存在于别的源文件中。</li>
<li>static 表示静态变量，分配内存的时候, 存储在静态区,不存储在栈上面。<br>static 修饰全局变量的作用域只是本编译单元，也就是说它的“全局”只对本编译单元有效，其他编译单元则看不到它。<br>h.h<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">#ifndef H_H</div><div class="line">#define H_H</div><div class="line"></div><div class="line">static char str[] = &quot;SJY&quot;;</div><div class="line">void func1();</div><div class="line">void func2();</div><div class="line"></div><div class="line">#endif</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
<p>a.cpp<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">#include &quot;h.h&quot;</div><div class="line">#include &lt;iostream&gt;</div><div class="line">void func1() &#123;</div><div class="line">    str[0] = &apos;L&apos;;</div><div class="line">    std::cout &lt;&lt; str &lt;&lt; std::endl;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>b.cpp<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">#include &quot;h.h&quot;</div><div class="line">#include &lt;iostream&gt;</div><div class="line">void func2() &#123;</div><div class="line">    str[1] = &apos;F&apos;;</div><div class="line">    std::cout &lt;&lt; str &lt;&lt; std::endl;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>main.cpp<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">#include &quot;h.h&quot;</div><div class="line">#include &lt;iostream&gt;</div><div class="line"></div><div class="line">int main(int argc, char const *argv[])</div><div class="line">&#123;</div><div class="line">    str[2] = &apos;C&apos;;</div><div class="line">    func1();</div><div class="line">    func2();</div><div class="line">    std::cout &lt;&lt; str &lt;&lt; std::endl;</div><div class="line">    return 0;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>./main 输出<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">LJY</div><div class="line">SFY</div><div class="line">SJC</div></pre></td></tr></table></figure></p>
<h2 id="static是C和C-的关键词，static在C-中比在C中有着更丰富的用法。"><a href="#static是C和C-的关键词，static在C-中比在C中有着更丰富的用法。" class="headerlink" title="static是C和C++的关键词，static在C++中比在C中有着更丰富的用法。"></a>static是C和C++的关键词，static在C++中比在C中有着更丰富的用法。</h2><h3 id="static在C中的作用"><a href="#static在C中的作用" class="headerlink" title="static在C中的作用"></a>static在C中的作用</h3><ol>
<li>static 修饰局部变量<br>static 修饰局部变量时，使得被修饰的变量成为静态变量，存储在静态区。存储在静态区的数据生命周期与程序相同，在 main 函数之前初始化，在程序退出时销毁。（无论是局部静态还是全局静态）。局部静态变量使得该变量在退出函数后，不会被销毁，因此再次调用该函数时，该变量的值与上次退出函数时值相同。<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">void function()</div><div class="line">&#123;</div><div class="line">    /*实际上nCount的初始化不是在函数体第一次执行时完成，而是在编译期其值就已经被</div><div class="line">       确定，在main函数之前就完成了初始化，所以局部静态变量只会初始化一次*/</div><div class="line">    static int nCount(0);    </div><div class="line">    std::cout &lt;&lt; &quot;Call function &quot; &lt;&lt; ++nCount &lt;&lt; &quot; times&quot; &lt;&lt; endl;</div><div class="line">&#125;</div><div class="line">int main()</div><div class="line">&#123;</div><div class="line">    for (int i = 0; i &lt; 5; ++i) </div><div class="line">    &#123;</div><div class="line">        function();</div><div class="line">    &#125;</div><div class="line">    return 0;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ol>
<p>输出结果是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Call function 1 times</div><div class="line">Call function 2 times</div><div class="line">Call function 3 times</div><div class="line">Call function 4 times</div><div class="line">Call function 5 times</div></pre></td></tr></table></figure></p>
<ol>
<li>static修饰全局变量<br>全局变量本来就存储在静态区，因此static并不能改变其存储位置。但是，static限制了其链接属性。static 修饰全局变量的作用域只是本编译单元，也就是说它的“全局”只对本编译单元有效，其他编译单元则看不到它。</li>
</ol>
<p>在头文件a.h中定义一个全局变量<br>a.h<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">#ifndef A_H</div><div class="line">    #define A_H</div><div class="line">    int a = 1;</div><div class="line">#endif</div></pre></td></tr></table></figure></p>
<p>b.h<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">#ifndef B_H</div><div class="line">    #define B_H</div><div class="line">    void func();</div><div class="line">#endif</div></pre></td></tr></table></figure></p>
<p>实现文件a.cpp,包含头文件a.h<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">#include &quot;a.h&quot;</div><div class="line">#include &quot;b.h&quot;</div><div class="line">#include &lt;iostream&gt;</div><div class="line"></div><div class="line">using namespace std;</div><div class="line"></div><div class="line">int main()</div><div class="line">&#123;</div><div class="line">    func();</div><div class="line">    return 0;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>实现文件b.cpp，包含头文件a.h<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">#include &quot;a.h&quot;</div><div class="line">#include &quot;b.h&quot;</div><div class="line">#include &lt;iostream&gt;</div><div class="line"></div><div class="line">using namespace std;</div><div class="line"></div><div class="line">void func()</div><div class="line">&#123;   a = 2;</div><div class="line">    cout &lt;&lt; a &lt;&lt; endl;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>编译出现重定义错误，因为头文件a被包含两次，在a.cpp和b.cpp分别被定义了一次。 </p>
<p>要解决这样的冲突怎么办？两种解决办法。 </p>
<pre><code>1. 将a定义为静态的全局变量，每个编译单元各一份。
a.h
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">#ifndef A_H</div><div class="line">    #define A_H</div><div class="line">    static int a = 1;</div><div class="line">#endif</div></pre></td></tr></table></figure>

b.h
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">#ifndef B_H</div><div class="line">    #define B_H</div><div class="line">    void func();</div><div class="line">#endif</div></pre></td></tr></table></figure>

实现文件a.cpp,包含头文件a.h
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">#include &quot;a.h&quot;</div><div class="line">#include &quot;b.h&quot;</div><div class="line">#include &lt;iostream&gt;</div><div class="line"></div><div class="line">using namespace std;</div><div class="line"></div><div class="line">int main()</div><div class="line">&#123;</div><div class="line">    func();</div><div class="line">    return 0;</div><div class="line">&#125;</div></pre></td></tr></table></figure>

实现文件b.cpp，包含头文件a.h
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">#include &quot;a.h&quot;</div><div class="line">#include &quot;b.h&quot;</div><div class="line">#include &lt;iostream&gt;</div><div class="line"></div><div class="line">using namespace std;</div><div class="line"></div><div class="line">void func()</div><div class="line">&#123;</div><div class="line">    cout &lt;&lt; a &lt;&lt; endl;</div><div class="line">&#125;</div></pre></td></tr></table></figure>

2. 使用extern，将a声明为extern类型的变量，所有的编译单元共用一个。
</code></pre><ol>
<li>static修饰函数<br>static 修饰函数的作用域只是本编译单元，也就是说它只对本编译单元有效，其他编译单元则看不到它。</li>
</ol>
<p><strong>不管是修饰函数或者变量，static 的语义就是仅限于此编译单元。</strong></p>
<h3 id="static在C-中的作用"><a href="#static在C-中的作用" class="headerlink" title="static在C++中的作用"></a>static在C++中的作用</h3><p>类属性 类方法<br>对于静态成员变量和静态成员函数，所有的对象都共享一份。 </p>
<h2 id="C-C-中的volatile-关键字"><a href="#C-C-中的volatile-关键字" class="headerlink" title="C/C++ 中的volatile 关键字"></a>C/C++ 中的volatile 关键字</h2><ol>
<li>volatile 是一个类型修饰符（type specifier），就像大家更熟悉的const一样，它是被设计用来修饰被不同线程访问和修改的变量。</li>
<li>volatile 的作用是作为指令关键字，确保本条指令不会因编译器的优化而省略，且要求每次直接读值。</li>
<li>volatile的变量是说这变量可能会被意想不到地改变，这样，编译器就不会去假设这个变量的值了。</li>
<li>精确地说就是，优化器在用到这个变量时必须每次都小心地重新读取这个变量的值，而不是使用保存在寄存器里的备份。</li>
<li><p>下面是volatile变量的几个例子：</p>
<ul>
<li>并行设备的硬件寄存器（如：状态寄存器）</li>
<li>一个中断服务子程序中会访问到的非自动变量（Non-automatic variables)</li>
<li><p>多线程应用中被几个任务共享的变量<br>简单地说就是防止编译器对代码进行优化。比如如下程序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">XBYTE[2]=0x55;</div><div class="line">XBYTE[2]=0x56;</div><div class="line">XBYTE[2]=0x57;</div><div class="line">XBYTE[2]=0x58;</div></pre></td></tr></table></figure>
<p>对外部硬件而言，上述四条语句分别表示不同的操作，会产生四种不同的动作，但是编译器却会对上述四条语句进行优化，认为只有XBYTE[2]=0x58（即忽略前三条语句，只产生一条机器代码）。如果键入volatile，则编译器会逐一地进行编译并产生相应的机器代码（产生四条代码）。</p>
</li>
</ul>
</li>
</ol>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/01/23/数据结构与算法分析C/">
                数据结构与算法分析C++
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-01-23</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p>递归的前两个基本法则：</p>
<ol>
<li>基准情形（base cases），必须总有某些基准情形，它们不用递归就能求解。</li>
<li>不断推进（making progress），对于那些要被递归求解的情形，递归调用必须总能够朝着一个基准情形推进。<br>递归程序不仅简化了算法设计而且有助于给出更加简洁的代码，但是递归绝不应该作为简单for循环的代替物。<br>另外的两个法则是：</li>
<li>设计法则。假设所有的递归调用都能运行。</li>
<li>合成效益法则，在求解一个问题的同一实例时，切勿在不同的递归调用中做重复性的工作。</li>
</ol>
<h4 id="接口与实现的分离"><a href="#接口与实现的分离" class="headerlink" title="接口与实现的分离"></a>接口与实现的分离</h4><p>接口列出了类及其成员（属性和方法），而实现则提供了函数的具体实现。<br>接口通常都放在以 .h 结尾的文件中，需要接口信息的源代码必须 #include 接口文件。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">#ifndef IntCell_H</div><div class="line">#define IntCell_H</div><div class="line"></div><div class="line">class IntCell &#123;</div><div class="line">    public:</div><div class="line">        explicit IntCell(int initialValue = 0);</div><div class="line">        int read() const;</div><div class="line">        void write(int x);</div><div class="line">    private:</div><div class="line">        int storedValue;</div><div class="line">&#125;;</div><div class="line">#endif</div></pre></td></tr></table></figure></p>
<p>注意的一点是：默认参数仅在接口中被定义，在实现中则被忽略。<br>在 C++ 中有两种使用零参数构造函数创建对象的方式，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">m = new IntCell();</div><div class="line">m = new IntCell;</div></pre></td></tr></table></figure></p>
<p>C++ 没有自动的垃圾收集。<br>C 和 Java 都是值传递，而 Python 是对象传递。<br>C++ 提供了三种传参方式，值传递，引用传递，常量引用传递。<br>C++ 也提供了三种返回传递方式，值传递，引用传递，常量引用传递。尽量不要使用引用传递。<br>值传递一般来说是最安全的。使用常量引用传递或者引用传递只有在确保返回语句中的表达式在函数返回之后依然有效时才是安全的。<br>如果编译器没有注意到这个问题，对于局部变量的引用传递返回既可能是正确的，也可能是错误的，这会取决于编译器释放局部变量所使用的内存的速度。<br>复制构造函数 用于构造新的对象，并被初始化为相同类型对象的一个副本。<br>在以下三个情形下会被调用：</p>
<ol>
<li><p>声明的同时初始化</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">IntCell B = C;</div><div class="line">IntCell B(C);</div><div class="line">而不是</div><div class="line">B = C; // 赋值运算符</div></pre></td></tr></table></figure>
</li>
<li><p>值传递传参</p>
</li>
<li>值传递返回<br>对于简单数据类型的数据成员，进行简单的赋值就可以了。对于对象数据成员，对象数据成员的复制构造函数依次被调用。<br>当 = 应用于两个已经构造的对象时，就会调用复制赋值运算符 operator=，默认情况下通过将其应用于每个数据成员的方式来实现。</li>
</ol>
<p>默认的析构函数、复制构造函数、operator= 都不会对指针指向的对象做任何的操作。<br>对于 IntCell，这些运算的签名是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">~IntCell();</div><div class="line">IntCell(const IntCell &amp; rhs);</div><div class="line">const IntCell &amp; operator=(const IntCell &amp; rhs);</div></pre></td></tr></table></figure></p>
<p>其中 operator= 的返回值类型和参数类型是一样的，是为了 a=b=c 这样的连续复制赋值运算符的调用。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">IntCell::~IntCell()&#123;</div><div class="line">    // Does nothing;</div><div class="line">    // If IntCell contained any class objects, their destructors would be called.</div><div class="line">&#125;</div><div class="line">IntCell::IntCell(const IntCell &amp; rhs):storedValue(rhs.storedValue)&#123;</div><div class="line">&#125;</div><div class="line">const IntCell &amp; operator=(const IntCell &amp; rhs)&#123;</div><div class="line">    // Standard alias test</div><div class="line">    if (this != &amp;rhs)&#123;</div><div class="line">        storedValue = rhs.storedValue;</div><div class="line">    &#125;</div><div class="line">    return *this;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h4 id="C-风格的数组和字符串"><a href="#C-风格的数组和字符串" class="headerlink" title="C 风格的数组和字符串"></a>C 风格的数组和字符串</h4><p><code>int arr[10];</code> 中 <code>arr</code> 实际上是一个指向足够存储10 个int的内存空间的指针，而且是常量指针。当 <code>arr</code> 传递给一个函数时，只有指针的值被传递，而数组的大小信息则丢失了，因此数组的大小必须作为一个附加参数进行传递，由于数组的大小未知，也就没有关于索引范围的检查。<br>在上面的定义中，在编译的时候数组的大小就必须被指定，如果数组的大小位置，就必须显式地声明一个指针，并用 new int[] 来分配内存。<br><code>int *arr = new int[n];</code>，<code>arr</code> 不再是常量指针，由于内存是动态分配地，需要使用 delete [] 进行释放，如果不释放，当数组很大时，泄露就会很严重。<br>内置的C风格字符串，就是字符数组来实现的，特殊的终止符’\0’用以标识字符串逻辑上的结束，以避免传递字符串的长度值。字符串可以使用’strcpy’，’strcmp’，’strlen’，这些字符串具有数组具有的所有问题，困难的内存管理问题。譬如当字符串进行复制的时候，总是假设目标数组数据空间是足够大的，如果空间不够大，常常会因为没有存储字符串终止符的空间而导致调试的极大困难。</p>
<h4 id="模板"><a href="#模板" class="headerlink" title="模板"></a>模板</h4><p>这本书描述的算法和数据结构都是类型无关的，当编写 C++ 代码的类型无关的算法或者数据结构时，我们更愿意只写一次，而不是为不同的类型都写一遍。在 C++ 中使用模板来写类型无关的算法，即泛型编程。<br>函数模板：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">/**</div><div class="line"> * Return the maximum item in array a.</div><div class="line"> * Assumes a.size() &gt; 0.</div><div class="line"> * Comparable objects must provide operator &gt; and operator =</div><div class="line">*/</div><div class="line">template &lt;typename Comparable&gt;</div><div class="line">const Comparable &amp; findMax(vector&lt;Comparable&gt; &amp; a)&#123;</div><div class="line">    int maxIndex = 0;</div><div class="line"></div><div class="line">    for(int i=1;i&lt;a.size();i++)&#123;</div><div class="line">        if(a[maxIndex] &lt; a[i])&#123;</div><div class="line">            maxIndex = i;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    return a[maxIndex];</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">int main()&#123;</div><div class="line">    vector&lt;int&gt; v1(37);</div><div class="line">    vector&lt;double&gt; v2(37);</div><div class="line">    vector&lt;string&gt; v3(37);</div><div class="line">    vector&lt;IntCell&gt; v4(37);</div><div class="line"></div><div class="line">    // Additional code to fill in the  vectors not shown;</div><div class="line"></div><div class="line">    cout &lt;&lt; findMax(v1) &lt;&lt; endl;</div><div class="line">    cout &lt;&lt; findMax(v2) &lt;&lt; endl;</div><div class="line">    cout &lt;&lt; findMax(v3) &lt;&lt; endl;</div><div class="line">    cout &lt;&lt; findMax(v4) &lt;&lt; endl; // Illegal; operator &lt; undefined;</div><div class="line"></div><div class="line">    return 0;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>类模板</p>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/01/18/机器学习工程师笔试面试/">
                机器学习工程师笔试面试
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-01-18</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h3 id="整体要求"><a href="#整体要求" class="headerlink" title="整体要求"></a>整体要求</h3><p>机器学习方面的面试主要分成三个部分：</p>
<ol>
<li>算法和理论基础</li>
<li>工程实现能力与编码水平</li>
<li>业务理解和思考深度</li>
</ol>
<h3 id="推荐书籍"><a href="#推荐书籍" class="headerlink" title="推荐书籍"></a>推荐书籍</h3><ol>
<li>理论方面，我推荐最经典的一本书《统计学习方法》，这书可能不是最全的，但是讲得最精髓，薄薄一本，适合面试前突击准备。</li>
</ol>
<h3 id="杂项"><a href="#杂项" class="headerlink" title="杂项"></a>杂项</h3><ol>
<li>统计学习的核心步骤：模型、策略、算法，你应当对logistic、SVM、决策树、KNN及各种聚类方法有深刻的理解。能够随手写出这些算法的核心递归步的伪代码以及他们优化的函数表达式和对偶问题形式。</li>
<li>数学知识方面，你应当深刻理解矩阵的各种变换，尤其是特征值相关的知识。</li>
<li>算法方面，你应当深刻理解常用的优化方法：梯度下降、牛顿法、各种随机搜索算法（基因、蚁群等等），深刻理解的意思是你要知道梯度下降是用平面来逼近局部，牛顿法是用曲面逼近局部等等。 </li>
<li>工程实现能力与编码水平<br> 机器学习从工程实现一般来讲都是某种数据结构上的搜索问题。<br> 你应当深刻理解各种算法对应应该采用的数据结构和对应的搜索方法。比如KNN对应的KD树、如何给图结构设计数据结构？如何将算法map-reduce化等等。<br> 一般来说要么你会写C，而且会用MPI，要么你懂Hadoop，工程上基本都是在这两个平台实现。实在不济你也学个python吧。 </li>
</ol>
<h3 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h3><ol>
<li>SVM的原理，SVM里面的核</li>
<li>K-means，如何用hadoop实现k-means</li>
<li>naive bayes和logistic regression的区别</li>
<li>LDA的原理和推导</li>
</ol>
<h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><ol>
<li>做广告点击率预测，用哪些数据、什么算法</li>
<li>推荐系统的算法中最近邻和矩阵分解各自适用场景</li>
<li>用户流失率预测怎么做（游戏公司的数据挖掘都喜欢问这个）</li>
<li>一个游戏的设计过程中该收集什么数据</li>
<li>如何从登陆日志中挖掘尽可能多的信息</li>
</ol>
<h3 id="其他人的一些经验总结"><a href="#其他人的一些经验总结" class="headerlink" title="其他人的一些经验总结"></a>其他人的一些经验总结</h3><ol>
<li>实际工作中，算法的先进性对真正业务结果的影响，大概不到30%。当然算法必须要足够快，离线算法最好能在4小时内完成，实时算法我没搞过，要求大概更高。机器学习大多数场景是搜索、广告、垃圾过滤、安全、推荐系统等等。对业务有深刻的理解对你做出来的系统的结果影响超过70%。这里你没做过实际的项目，是完全不可能有任何体会的，我做过一个推荐系统，没有什么算法上的高大上的改进，主要是业务逻辑的创新，直接就提高了很明显的一个CTR（具体数目不太方便透露，总之很明显就是了）。如果你做过实际的项目，一定要主动说出来，主动让面试官知道，这才是最大最大的加分项目。</li>
<li>这些问题的特点是很基础很简单，因为实际中很少用复杂的算法，复杂的算法不好控制，而且理论要求高。另一个特点是注重考查实际工程能力，我经常被问到自己实现了哪些算法。还有的问题很契合实际。</li>
</ol>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2017/12/12/RLAI-Monte-Carlo-Methods-学习笔记/">
                RLAI Monte Carlo Methods 学习笔记
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2017-12-12</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h2 id="Chapter-5"><a href="#Chapter-5" class="headerlink" title="Chapter 5"></a>Chapter 5</h2><h2 id="Monte-Carlo-Methods-一种估计值函数和发现最优策略的方法"><a href="#Monte-Carlo-Methods-一种估计值函数和发现最优策略的方法" class="headerlink" title="Monte Carlo Methods 一种估计值函数和发现最优策略的方法"></a>Monte Carlo Methods 一种估计值函数和发现最优策略的方法</h2><p>不需知道环境的完整信息，MC方法只需要 experience，也就是来自真实或者仿真环境下的状态、动作、奖励的序列。尽管需要环境模型，但是仅仅需要这个模型产生样本，而不是像动态规划那样需要完整的转移概率分布。在很多情况下，获得样本是容易的，而给出具体形式的分布确实困难的，这就使得MC方法十分的有用。</p>
<p>MC 方法基于采样与平均。experience 分为不同的 episode，只有在一个 episode 完成之后才会进行值估计和策略提升。MC 方法是 episode-by-episode （offline）的形式，而不是 step-by-step （online）的方式。</p>
<h3 id="Monte-Carlo-Prediction"><a href="#Monte-Carlo-Prediction" class="headerlink" title="Monte Carlo Prediction"></a>Monte Carlo Prediction</h3><p>一个状态的 value 是一个期望回报，即从该状态出发遵循一定的策略可以期望获得的累积未来折扣奖励。</p>
<p>MC 方法内在的想法就是，从 experience 中估计状态值函数，对访问一个状态之后获得的回报进行平均，随着 episode 次数的增多，这个平均值会收敛到期望值。\(v_{\pi}(s)\) 是在策略 \(\pi\) 下，状态 \(s\) 的 value，我们要做的就是在给定一些遵循策略 \(\pi\) 经历了状态 \(s\) 的 episode 时，对 \(v_{\pi}(s)\) 进行估计。在 episode 中的 \(s\) 每一次出现就称之为对状态 \(s\) 的一次访问。根据进行估计时使用 \(s\) 的第一访问还是每一次访问，MC 方法可以分为 First-visit MC 和 Every-visit MC。<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/first-visit-mc-prediction.png" alt="first-visit-mc-prediction"></p>
<h3 id="Monte-Carlo-Estimation-of-Action-Values"><a href="#Monte-Carlo-Estimation-of-Action-Values" class="headerlink" title="Monte Carlo Estimation of Action Values"></a>Monte Carlo Estimation of Action Values</h3><p>上面讲了在给定策略的条件下使用 MC 对状态值函数进行估计。</p>
<p>当环境模型未知时，对状态-动作值函数进行估计就比对状态值函数进行估计要有用了，这是因为在环境模型已知时，状态值函数对于确定（给出）一个策略就足够了，即在根据当前状态选择动作时，仅仅需要选择立即奖励和下一状态的 value 之和最大的那个动作。这样可行是因为，当环境模型已知时，一个动作带来的奖励是已知的，状态转移概率是已知的，即在当前状态下采取一个动作转移到下一状态及由下一状态带来的期望回报都是已知的。但是在没有环境模型时，状态值函数不足以给出一个策略，为了可以得到一个策略，就需要更加显式直接的状态-动作对值函数。</p>
<p>\(q_{\pi}(s,a)\) the expected return when starting in state \(s\), taking action \(a\), and thereafter following policy \(\pi\)（从状态 \(s\) 出发，采取动作 \(a\) 并在后续遵循策略 \(\pi\) 可以得到的期望回报）。在使用 MC 进行估计时，和状态值函数的估计很像，不做在这里我们考虑的不再是对状态 \(s\) 的访问而是对状态-动作对 \(s\)-\(a\) 的访问。</p>
<p>这里唯一的问题在于，很多的状态-动作对可能永远不会被访问到。如果策略是确定性的，那么在一个状态下做出的动作就是确定性的，有的状态-动作对就不会被访问到，就无法更新状态-动作对的 value, 这是一个严重的问题，因为学习状态-动作对的 value 的目的就是用来在一个状态下可用的动作中做出选择，这就需要我们能够对一个状态的所有可能动作的 value 做出估计，而不是仅仅估计我们目前策略所指定的状态-动作对的 value。这就是一个保持探索（maintaining exploration）的问题，一个解决方法就是 exploring starts 假设，即每一个状态-动作对都有不为零的概率被选择成为一个 episode 的开始。但是在“实际”环境中这通常是不可行的。另外一个确保所有的状态-动作对都被访问到的方法是使用在每一个状态下以非零概率选择各个动作的随机策略。</p>
<h3 id="Monte-Carlo-Control"><a href="#Monte-Carlo-Control" class="headerlink" title="Monte Carlo Control"></a>Monte Carlo Control</h3><p>遵循 GPI（generalized policy iteration 泛化策略迭代）的指导，即策略的评估（利用上面讲到的状态-动作对值函数对一个策略的好坏进行评估）和策略的提高（对得到的状态-动作对值函数进行利用从而提高策略）。<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/GPI.png" alt="GPI"><br>policy evaluation and policy improvement, beginning with an arbitrary policy \(\pi_{0}\) and ending with the optimal policy and optimal action-value function:<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/evaluation-improvement.png" alt="evaluation-improvement"><br>策略评估是通过多个 episode，用状态-动作对值函数的估计渐进地逼近真实的函数。<br>策略提升是通过对状态-动作对值函数进行利用得到的贪婪策略，即$$\pi(s)=argmax_{a}q(s,a)$$<br>策略提高之所以有效是因为：<br>$$q_{\pi_{k} }(s, \pi_{k+1}(s))=q_{\pi_{k} }(s, argmax_{a}q_{\pi_{k} }(s,a))$$<br>$$=max_{a}q_{\pi_{k} }(s,a)$$<br>$$\ge q_{\pi_{k} }(s, \pi_{k}(s))$$<br>上面的公式可以做如下解释：<br>\(q_{\pi_{k} }(s,a)\) 是在策略 \(\pi_{k}\) 指导下进行多次 episode 而得到的对状态-动作对值函数的估计，使用贪心的方式得到的新策略 \(\pi_{k+1}(s)\)，\(q_{\pi_{k} }(s, \pi_{k+1}(s))\) 是在新的策略 \(\pi_{k+1}(s)\)指导下可以得到的状态-动作对值函数的预期，实际上尚未在其指导下进行过一次 episode，可以看到这个预期是大于等于已经得到的状态-动作对值函数的估计的，因此新的策略 \(\pi_{k+1}(s)\) 相较于 \(\pi_{k}(s)\) 是提升了的。可以看到状态-动作对值函数的提高提升了策略，而策略的提高提升了状态-动作对值函数，两者交替上升。由此可见，MC 方法可以在对环境模型未知的情况下仅仅通过 episode 找到最优的策略。</p>
<p>We made two unlikely assumptions above in order to easily obtain this guarantee of convergence for the Monte Carlo method. One was that the episodes have exploring starts, and the other was that policy evaluation could be done with an infinite number of episodes.第二个假设相对而言比较容易移除，一种有效的避免策略评估需要无限次数的 episode 的途径是，我们放弃完整地完成策略评估，而是中途进入策略提升阶段，我们并不需要直达或者逼近真实的\(q_{\pi_{k} }\)，而只需要 move toward \(q_{\pi_{k} }\) step-by-step。</p>
<p>一个极端的例子就是值迭代，在两次策略提升之间是只有迭代策略评估的一次迭代；in-place 版本的值迭代更加极端，在提升和评估之间只有一个状态。</p>
<p>For Monte Carlo policy evaluation it is natural to alternate between evaluation and improvement on<br>an episode-by-episode basis. 在每次 episode 之后，观测到的回报被用于策略评估，然后策略会在此次 episode 中被访问的所有状态上进行提升。<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/MC-ES.png" alt="MC-ES"></p>
<h3 id="Monte-Carlo-Control-without-Exploring-Starts"><a href="#Monte-Carlo-Control-without-Exploring-Starts" class="headerlink" title="Monte Carlo Control without Exploring Starts"></a>Monte Carlo Control without Exploring Starts</h3><p>如何去掉 exploring starts 假设呢？有两种方式：</p>
<ul>
<li>on-policy：evaluate or improve the policy that is used to make decisions</li>
<li>off-policy：evaluate or improve a policy different from that used to generate the data<br>MC ES 是一种 on-policy 的例子。<br>在 on-policy 方法中的策略通常是软的，也就是$$\pi(a|s)&gt;0\quad for\quad all\quad s\quad \in\quad S\quad and\quad a\quad \in\quad A(s)$$<br>但是向确定性的最优策略偏移地越来越近。<br>\(\varepsilon-soft\) 中的一个例子就是 \(\varepsilon-greedy\)，在 \(\varepsilon-greedy\) 中所有的非贪婪动作被选择的概率为 \(\frac{\varepsilon}{|A(s)|}\)，贪婪动作被选择的概率为 \(1-\varepsilon+\frac{\varepsilon}{|A(s)|}\)，它是对于所有的状态和动作都有 \(\pi(a|s)\ge\frac{\varepsilon}{|A(s)|}\) 的 \(\varepsilon-soft\)。<br>在去掉 exploring starts 假设之后，不能简单地使用贪婪地方式进行策略提升，因为这会阻碍非贪婪动作的探索，相应的使用 \(\varepsilon-greedy\) 策略。<br><img src="/2017/12/12/RLAI-Monte-Carlo-Methods-学习笔记/.." alt="epsilon-soft"><br>关于算法有效性的证明请参考 Reinforcement Learning: An Introduction 5.4。</li>
</ul>
<h3 id="Off-policy-Prediction-via-Importance-Sampling"><a href="#Off-policy-Prediction-via-Importance-Sampling" class="headerlink" title="Off-policy Prediction via Importance Sampling"></a>Off-policy Prediction via Importance Sampling</h3><p>所有的 learning control 方式都有一个困境，那就是假设在后续最优动作的条件下得到状态-动作对的 value，但是又需要表现的非最优来探索所有的动作，即最优动作是确定的，但是又需要执行非最优动作来探索动作空间进行最优动作的寻找，使得最优策略的状态-动作对 value 的求解变得不可得。<br>How to learn about the optimal policy while behaving according to<br>an exploratory policy?<br>上面讲到的得 on-policy 是一种折中的方式，即学习到的并不是最优策略的状态-动作对的值，而是接近最优的仍在探索的策略的状态-动作对的值，即最优策略是固定的，但此处得到的策略是随机的非最优的。更直接的方式是使用两个策略，一个进行学习从而成为最优策略（target policy），另一个更具有探索性用来产出动作（behaviour policy），也就是 off-policy learning，这种方式方差较大，收敛地更慢。<br>下面首先进行策略的评估，也就是 prediction，从策略 \(b(b\ne\pi)\) 指导下的一些 episode 出发估计 \(v_{\pi}\) 或者 \(q_{\pi}\)。<br>收敛假设：$$\pi(a|s)&gt;0 implies b(a|s)&gt;0$$<br>另外，\(b\) 必须是随机的，而 \(\pi\) 可以是确定性的，在控制学里，典型的情形是 \(\pi\) 是在当前状态-动作值函数估计下的确定性策略，behaviour policy 保持随机，而 target policy 逐渐成为一个确定性的最优策略。<br>几乎所有的 off-policy 方法使用了重要性采样，a general technique for estimating expected<br>values under one distribution given samples from another。<br>在这里我们通过根据目标策略下的轨迹和行为策略下的轨迹之间的相对概率关系对回报进行加权的方式来将重要性采样应用到 off-policy 的学习中。<br>给定一个初始状态 \(S_{t}\)，\([A_{t}, S_{t+1}, A_{t+1},…,S_{T}]\)这样的一条轨迹出现的概率<br><img src="/2017/12/12/RLAI-Monte-Carlo-Methods-学习笔记/.." alt="trajectory_probability"><br>这样的一条轨迹在 target policy 和 behaviour policy 之间的相对概率也就是 importance-sampling ratio：<br><img src="/2017/12/12/RLAI-Monte-Carlo-Methods-学习笔记/.." alt="trajectory_importance_ratio"><br>Although the trajectory probabilities depend on the MDP’s transition probabilities, which are generally<br>unknown, they appear identically in both the numerator and denominator, and thus cancel. The<br>importance sampling ratio ends up depending only on the two policies and the sequence, not on the<br>MDP.<br>下面给出一个 MC 算法，该算法使用观察到的遵循策略 \(b\) 的一批次 episode 来估计 \(v_{\pi}(s)\).<br>给所有的 episode 中的时间步打破 episode 边界赋予一个唯一的递增的时间步序号，在所有的 episode 中，对于 every-visit 方式，定义对状态 \(s\) 进行了访问的时间步集合是 \(J(s)\)；对于 first-visit 方式，定义第一次对状态 \(s\)进行了访问的时间步集合是 \(J(s)\)。<br>let \(T(t)\) denote the first time<br>of termination following time \(t\), and \(G_{t}\) denote the return after \(t\) up through \(T(t)\)。<br>\(\lbrace G_{t}\rbrace_{t \in J(s)}\) 就是状态 \(s\) 的回报，\(\lbrace \rho_{t:T(t)-1} \rbrace_{t \in J(s)}\) 就是状态 \(s\) 的重要性采样比率。<br>To estimate \(v_{\pi}(s)\), we simply scale the returns by the ratios and average the results:<br><img src="/2017/12/12/RLAI-Monte-Carlo-Methods-学习笔记/.." alt="ordinary_scale_return_and_average"><br>通过简单平均完成的重要性采样叫做 ordinary importance sampling。<br><img src="/2017/12/12/RLAI-Monte-Carlo-Methods-学习笔记/.." alt="weighted_scale_return_and_average"><br>通过加权平均完成的重要性采样叫做 weighted importance sampling。<br>关于这两者的不同，请参考 Reinforcement Learning: An Introduction 5.5。</p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2017/12/09/强化学习-3-估值网络及其-TensorFlow-实现/">
                强化学习 3 估值网络及其 TensorFlow 实现
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2017-12-09</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h1 id="强化学习-3-估值网络及其-TensorFlow-实现"><a href="#强化学习-3-估值网络及其-TensorFlow-实现" class="headerlink" title="强化学习 3 估值网络及其 TensorFlow 实现"></a>强化学习 3 估值网络及其 TensorFlow 实现</h1><h2 id="估值网络"><a href="#估值网络" class="headerlink" title="估值网络"></a>估值网络</h2><h4 id="概念："><a href="#概念：" class="headerlink" title="概念："></a>概念：</h4><ol>
<li>Q-learning 中的期望价值指从当前的这一步到后续的所有步骤，总共可以期望获得的最大价值。也就说是对未来进行了断言，这一步采取这样的动作，不管未来采取什么样的动作，可以期望获得的价值不会超过这个Q值。Q值的意义就是最大价值，当前最大 = 当前 + gamma * 以后续一步为当前的最大。</li>
<li>Q-learning 的目的就是求解 \(Q(s_t, a_t)\)，可以将其看作一个表格，每一行是不同的状态，每一列是不同的动作，学习的过程就是把这个表用合适的值填充起来。</li>
<li>Q-learning 的训练思路也很简单，以（状态，动作，奖励，下一状态）四元组（\((s_t, a_t, r_{t+1}, s_{t+1})\)）作为样本进行训练。</li>
<li>学习目标 \(r_{t+1} + \gamma \cdot \max\limits_{a} Q(s_{t+1}, a)\)，即相当于 label。</li>
<li>学习过程 \(Q_{new}(s_t, a_t) \gets (1-\alpha) \cdot Q_{old}(s_t, a_t) + \alpha \cdot (r_{t+1} + \gamma \cdot \max\limits_{a} Q_{old}(s_{t+1}, a))\)</li>
<li>Q-learning 的模型是神经网络时，即将\(Q(s_t, a_t)\)表达为神经网络，就是估值网络。</li>
<li>DQN 是 DeepMind 在 Human-level control through deep reinforcement learning 中提出的。</li>
</ol>
<h4 id="Tricks："><a href="#Tricks：" class="headerlink" title="Tricks："></a>Tricks：</h4><ol>
<li>Experience Replay，因为深度学习需要大量的样本，所以传统的Q-learning 的 online update 的方法不太适合于 DQN，可以像训练CNN 那样进行多个 epoch 的训练，主要思想就是存储 Agent 的 Experience，并且每次训练时随机抽取一部分样本训练网络。这样就能比较稳定的完成学习任务，避免只短视地学习新接触地样本，而是综合地反复利用过往的大量样本进行学习。可以创建一个用来存储Experience的缓存buffer，缓存满了之后就使用新的样本进行替换，如此就可以保证样本都有相近的概率被抽到，如果不替换，那么一开始就有的老样本在整个训练过程中被抽中的概率就会比新样本大得多。</li>
<li>使用 target DQN 进行辅助训练，它的意义是用来计算目标 Q 值（label），即提供\(\max\limits_{a} Q(s_{t+1}, a)\)。之所以拆分为两个网络，一个用来制造学习目标，一个用来进行实际训练，是为了让 Q-learning 训练的目标平稳。强化学习的学习目标每次更新后都在变化，即相同的输入，目标Q值（label）并不相同，如果模型参数更新地很频繁，幅度很大，训练过程就会因为目标剧烈变化而非常不稳定，容易失控。为了降低这个影响，需要让目标Q值（label）尽量平稳，因此需要一个比较稳定的 target DQN 辅助计算目标Q值，让target DQN 进行低频率的学习，让它输出的目标Q值波动也小，可以减少对训练过程的影响。</li>
<li>Double DQN 是在拆分出target DQN 的基础上更近一步，DeepMind 的研究者在 Deep Reinforcement Learning with Double Q-learning 中指出，传统的DQN通常会高估 Action 的Q值，如果这中高估是不均匀的，可能会导致本来次优的 Action 总是被高估而超过最后的 Action，这样就可能永远发现不了最优的 Action。之前，target DQN 完全负责生成目标Q值，先产生\(Q(s_{t+1}, a)\),再通过\(\max\limits_{a}\)选择那个最大的Q值。Double DQN 改变了第二步，不是直接选择 target DQN 输出中的最大Q值，而是在主DQN的输出中找到最大Q值对应的 Action，再找到 target DQN 的输出中这个 Action 对应的Q值，这个Q值作为目标Q值（即label）。使用的目标Q值不一定总是 target DQN 输出中的最大Q值，这样就避免了总是从 target DQN 的输出中选择被高估的同一个 Action 对应的Q值作为 label。此时学习目标就可以写成 $$Target=r_{t+1} + \gamma \cdot Q_{target}(s_{t+1}, argmax_{a}(Q_{main}(s_{t+1}, a)))$$</li>
<li>Dueling DQN 是 DQN 一大改进，在 Dueling Network Architectures for Deep Reinforcement Learning 中提出，将Q值函数\(Q(s_{t}, a)\)拆分成两部分，一部分是静态的环境本身具有的价值\(V(s_{t})\)，称为 Value；另外一部分是通过执行 Action 额外带来的价值\(A(a_{t})\)，称为 Advantage，即 \(Q(s_{t}, a_{t})=V(s_{t})+A(a_{t})\)。Dueling 的目的就是让网络分别计算环境本身的 Value 和 Action 带来的 Advantage，Advantage 或好或坏，设计成零均值。<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/dueling.png" alt="传统DQN和Dueling DQN 的对比"><br>Dueling DQN 的输出是一个标量和向量，分别代表 Value 和每个 Action 的 Advantage。</li>
</ol>
<h4 id="仿真环境-GridWorld"><a href="#仿真环境-GridWorld" class="headerlink" title="仿真环境 GridWorld"></a>仿真环境 GridWorld</h4><p>grid_world.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> scipy.misc</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, coordinates, size, intensity, channel, reward, name)</span>:</span></div><div class="line">        self.x, self.y = coordinates[<span class="number">0</span>], coordinates[<span class="number">1</span>]</div><div class="line">        self.size = size</div><div class="line">        self.intensity = intensity</div><div class="line">        self.channel = channel</div><div class="line">        self.reward = reward</div><div class="line">        self.name = name</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">GridWorld</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size)</span>:</span></div><div class="line">        self.x_size = size</div><div class="line">        self.y_size = size</div><div class="line">        self.action_num = <span class="number">4</span></div><div class="line">        self.block_num = <span class="number">7</span></div><div class="line">        self.blocks = <span class="keyword">None</span></div><div class="line">        self.available_grids = <span class="keyword">None</span></div><div class="line">        self.state = <span class="keyword">None</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_positions</span><span class="params">(self, num)</span>:</span></div><div class="line">        indices = np.random.choice(np.arange(len(self.available_grids)), size=num, replace=<span class="keyword">False</span>)</div><div class="line">        grids = [self.available_grids[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</div><div class="line">        <span class="keyword">for</span> grid <span class="keyword">in</span> grids:</div><div class="line">            self.available_grids.remove(grid)</div><div class="line">        <span class="keyword">return</span> grids</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span><span class="params">(self)</span>:</span></div><div class="line">        self.available_grids = [(x, y) <span class="keyword">for</span> x <span class="keyword">in</span> range(self.x_size) <span class="keyword">for</span> y <span class="keyword">in</span> range(self.y_size)]</div><div class="line">        postions = self.get_positions(self.block_num)</div><div class="line">        size = [<span class="number">1</span>] * self.block_num</div><div class="line">        intensity = [<span class="number">1</span>] * self.block_num</div><div class="line">        channel = [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</div><div class="line">        reward = [<span class="keyword">None</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>]</div><div class="line">        name = [<span class="string">"hero"</span>, <span class="string">"goal"</span>, <span class="string">"fire"</span>, <span class="string">"goal"</span>, <span class="string">"fire"</span>, <span class="string">"goal"</span>, <span class="string">"goal"</span>]</div><div class="line">        self.blocks = [Block(*args) <span class="keyword">for</span> args <span class="keyword">in</span> zip(postions, size, intensity, channel, reward, name)]</div><div class="line">        self.state = self.render()</div><div class="line">        <span class="keyword">return</span> self.state</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">move</span><span class="params">(self, direction)</span>:</span></div><div class="line">        hero = self.blocks[<span class="number">0</span>]</div><div class="line">        <span class="keyword">if</span> direction == <span class="number">0</span> <span class="keyword">and</span> hero.y &gt;=<span class="number">1</span>:</div><div class="line">            hero.y -= <span class="number">1</span></div><div class="line">        <span class="keyword">elif</span> direction == <span class="number">1</span> <span class="keyword">and</span> hero.y &lt;= self.y_size - <span class="number">2</span>:</div><div class="line">            hero.y += <span class="number">1</span></div><div class="line">        <span class="keyword">elif</span> direction == <span class="number">2</span> <span class="keyword">and</span> hero.x &gt;= <span class="number">1</span>:</div><div class="line">            hero.x -= <span class="number">1</span></div><div class="line">        <span class="keyword">elif</span> direction == <span class="number">3</span> <span class="keyword">and</span> hero.x &lt;= self.x_size - <span class="number">2</span>:</div><div class="line">            hero.x += <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">pass</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_hit</span><span class="params">(self)</span>:</span></div><div class="line">        hero = self.blocks.pop(<span class="number">0</span>)</div><div class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self.blocks:</div><div class="line">            <span class="keyword">if</span> (hero.x == block.x) <span class="keyword">and</span> (hero.y == block.y):</div><div class="line">                self.available_grids.append([block.x, block.y])</div><div class="line">                self.blocks.remove(block)</div><div class="line">                <span class="keyword">if</span> block.name == <span class="string">"goal"</span>:</div><div class="line">                    self.blocks.append(Block(*self.get_positions(<span class="number">1</span>), <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="string">"goal"</span>))</div><div class="line">                <span class="keyword">elif</span> block.name == <span class="string">"fire"</span>:</div><div class="line">                    self.blocks.append(Block(*self.get_positions(<span class="number">1</span>), <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">-1</span>, <span class="string">"fire"</span>))</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    <span class="keyword">raise</span></div><div class="line">                self.blocks.insert(<span class="number">0</span>, hero)</div><div class="line">                <span class="keyword">return</span> block.reward, <span class="keyword">False</span></div><div class="line">        </div><div class="line">        self.blocks.insert(<span class="number">0</span>, hero)</div><div class="line">        <span class="keyword">return</span> <span class="number">0.0</span>, <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">render</span><span class="params">(self)</span>:</span></div><div class="line">        canvas = np.ones([self.y_size+<span class="number">2</span>, self.x_size+<span class="number">2</span>, <span class="number">3</span>])</div><div class="line">        canvas[<span class="number">1</span>:<span class="number">-1</span>,<span class="number">1</span>:<span class="number">-1</span>,:] = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self.blocks:</div><div class="line">            canvas[block.y+<span class="number">1</span>:block.y+block.size+<span class="number">1</span>,block.x+<span class="number">1</span>:block.x+block.size+<span class="number">1</span>,block.channel] = block.intensity</div><div class="line">        r = scipy.misc.imresize(canvas[:,:,<span class="number">0</span>], [<span class="number">84</span>,<span class="number">84</span>,<span class="number">1</span>], interp=<span class="string">'nearest'</span>)</div><div class="line">        g = scipy.misc.imresize(canvas[:,:,<span class="number">1</span>], [<span class="number">84</span>,<span class="number">84</span>,<span class="number">1</span>], interp=<span class="string">'nearest'</span>)</div><div class="line">        b = scipy.misc.imresize(canvas[:,:,<span class="number">2</span>], [<span class="number">84</span>,<span class="number">84</span>,<span class="number">1</span>], interp=<span class="string">'nearest'</span>)</div><div class="line">        <span class="keyword">return</span> np.stack([r, g, b],axis=<span class="number">2</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, action)</span>:</span></div><div class="line">        self.move(action)</div><div class="line">        reward, done = self.check_hit()</div><div class="line">        self.state = self.render()</div><div class="line">        <span class="keyword">return</span> self.state, reward, done</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(self)</span>:</span></div><div class="line">        plt.imshow(self.state, interpolation=<span class="string">"nearest"</span>)</div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    env = GridWorld(size=<span class="number">5</span>)</div><div class="line">    _ = env.reset()</div><div class="line">    env.plot()</div></pre></td></tr></table></figure></p>
<p>上面的模块定义了 GridWorld，效果图如下：<br><img src="http://oytnj8g2y.bkt.clouddn.com/gw.png" alt="gw"><br>其中蓝色是英雄，绿色是目标（奖励为1），红色是火焰（惩罚为1），英雄在这个 GridWorld 中游走获得尽可能多分数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding: utf-8</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">from</span> grid_world <span class="keyword">import</span> GridWorld</div><div class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">"1"</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQN</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, env, name)</span>:</span></div><div class="line">        self.name = name</div><div class="line">        self.env = env</div><div class="line">        <span class="comment">################## 网络结构 ##############</span></div><div class="line">        self.scalar_input =  tf.placeholder(shape=[<span class="keyword">None</span>,<span class="number">21168</span>],dtype=tf.float32)</div><div class="line">        self.image = tf.reshape(self.scalar_input,shape=[<span class="number">-1</span>,<span class="number">84</span>,<span class="number">84</span>,<span class="number">3</span>])</div><div class="line">        self.conv1 = tf.contrib.layers.convolution2d(inputs=self.image, num_outputs=<span class="number">32</span>, </div><div class="line">            kernel_size=[<span class="number">8</span>,<span class="number">8</span>], stride=[<span class="number">4</span>,<span class="number">4</span>], padding=<span class="string">'VALID'</span>, biases_initializer=<span class="keyword">None</span>)</div><div class="line">        self.conv2 = tf.contrib.layers.convolution2d(inputs=self.conv1, num_outputs=<span class="number">64</span>, </div><div class="line">            kernel_size=[<span class="number">4</span>,<span class="number">4</span>], stride=[<span class="number">2</span>,<span class="number">2</span>],padding=<span class="string">'VALID'</span>, biases_initializer=<span class="keyword">None</span>)</div><div class="line">        self.conv3 = tf.contrib.layers.convolution2d(inputs=self.conv2, num_outputs=<span class="number">64</span>, </div><div class="line">            kernel_size=[<span class="number">3</span>,<span class="number">3</span>], stride=[<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">'VALID'</span>, biases_initializer=<span class="keyword">None</span>)</div><div class="line">        self.conv4 = tf.contrib.layers.convolution2d(inputs=self.conv3, num_outputs=<span class="number">512</span>, </div><div class="line">            kernel_size=[<span class="number">7</span>,<span class="number">7</span>], stride=[<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">'VALID'</span>, biases_initializer=<span class="keyword">None</span>)</div><div class="line"></div><div class="line">        self.conv4_1, self.conv4_2 = tf.split(self.conv4, <span class="number">2</span>, <span class="number">3</span>)</div><div class="line">        self.flat_1 = tf.contrib.layers.flatten(self.conv4_1)</div><div class="line">        self.flat_2 = tf.contrib.layers.flatten(self.conv4_2)</div><div class="line">        self.advantage_weight = tf.Variable(tf.random_normal([<span class="number">256</span>, self.env.action_num]), </div><div class="line">            name=<span class="string">"advantage_weight"</span>)</div><div class="line">        self.value_weight = tf.Variable(tf.random_normal([<span class="number">256</span>, <span class="number">1</span>]), name=<span class="string">"value_weight"</span>)</div><div class="line">        self.advantage = tf.matmul(self.flat_1, self.advantage_weight)</div><div class="line">        self.value = tf.matmul(self.flat_2, self.value_weight)</div><div class="line">        </div><div class="line">        self.q_value = self.value + tf.subtract(self.advantage, </div><div class="line">            tf.reduce_mean(self.advantage, reduction_indices=<span class="number">1</span>, keep_dims=<span class="keyword">True</span>))</div><div class="line">        self.action = tf.argmax(self.q_value, <span class="number">1</span>)</div><div class="line"></div><div class="line">        <span class="comment"># self.actions 是一维的，类似于[1,1,2,3,1,0.....]</span></div><div class="line">        self.actions = tf.placeholder(shape=[<span class="keyword">None</span>], dtype=tf.int32)</div><div class="line">        <span class="comment"># self.target_q 是一维的，是self.actions 中每个动作对应的q值</span></div><div class="line">        self.target_q = tf.placeholder(shape=[<span class="keyword">None</span>], dtype=tf.float32)</div><div class="line">        <span class="comment"># self.q_value_of_action 是一维的，是self.actions 中每个动作对应的预测q值</span></div><div class="line">        self.actions_onehot = tf.one_hot(self.actions, self.env.action_num, dtype=tf.float32)</div><div class="line">        self.q_value_on_action = tf.reduce_sum(tf.multiply(self.q_value, self.actions_onehot), </div><div class="line">            reduction_indices=<span class="number">1</span>)</div><div class="line">        </div><div class="line">        self.loss = tf.reduce_mean(tf.square(self.target_q - self.q_value_on_action))</div><div class="line">        self.trainer = tf.train.AdamOptimizer(learning_rate=<span class="number">0.00001</span>)</div><div class="line">        self.update_model = self.trainer.minimize(self.loss)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExperienceBuffer</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, buffer_size=<span class="number">50000</span>)</span>:</span></div><div class="line">        self.buffer = []</div><div class="line">        self.buffer_size = buffer_size</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, experience)</span>:</span></div><div class="line">        num_to_remove = len(self.buffer) + len(experience) - self.buffer_size</div><div class="line">        <span class="keyword">if</span> num_to_remove &gt; <span class="number">0</span>:</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(num_to_remove):</div><div class="line">                self.buffer.pop(<span class="number">0</span>)</div><div class="line">        self.buffer.extend(experience)</div><div class="line">            </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, size)</span>:</span></div><div class="line">        indices = np.random.choice(np.arange(len(self.buffer)), size=size)</div><div class="line">        <span class="keyword">return</span> [self.buffer[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</div><div class="line"></div><div class="line">     </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_update_target_ops</span><span class="params">(variables, tau)</span>:</span></div><div class="line">    main_DQN_vars = variables[<span class="number">0</span>:len(variables)//<span class="number">2</span>]</div><div class="line">    target_DQN_vars = variables[len(variables)//<span class="number">2</span>:]</div><div class="line">    ops = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(target_DQN_vars)):</div><div class="line">        ops.append(target_DQN_vars[i].assign(</div><div class="line">            main_DQN_vars[i].value() * tau + (<span class="number">1</span> - tau) * target_DQN_vars[i].value()</div><div class="line">            ))</div><div class="line">    <span class="keyword">return</span> ops</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_target</span><span class="params">(ops, sess)</span>:</span></div><div class="line">    <span class="keyword">for</span> op <span class="keyword">in</span> ops:</div><div class="line">        sess.run(op)</div><div class="line"></div><div class="line"><span class="comment">########################## 超参数 #####################</span></div><div class="line">batch_size = <span class="number">32</span></div><div class="line"></div><div class="line"><span class="comment">#How often to perform a training step.</span></div><div class="line">update_freq = <span class="number">4</span></div><div class="line">gamma = <span class="number">.99</span></div><div class="line">random_upper_bound = <span class="number">1</span></div><div class="line">random_lower_bound = <span class="number">0.1</span></div><div class="line">annealing_steps = <span class="number">10000</span></div><div class="line">random_threshold = random_upper_bound</div><div class="line">drop_step = (random_upper_bound - random_lower_bound) / annealing_steps</div><div class="line"></div><div class="line"><span class="comment">#How many episodes of game environment to train network with.</span></div><div class="line">num_episodes = <span class="number">10000</span></div><div class="line"><span class="comment">#How many steps of random actions before training begins.</span></div><div class="line">pre_train_steps = <span class="number">10000</span></div><div class="line"><span class="comment">#The max allowed length for one episode.</span></div><div class="line">max_episode_length = <span class="number">50</span></div><div class="line">load_model = <span class="keyword">False</span> </div><div class="line">path = <span class="string">"./dqn"</span></div><div class="line"><span class="comment">#Rate to update target network toward primary network</span></div><div class="line">tau = <span class="number">0.001</span></div><div class="line"></div><div class="line"><span class="comment">########################## 训练 #####################</span></div><div class="line">env = GridWorld(size=<span class="number">5</span>)</div><div class="line">tf.reset_default_graph()</div><div class="line">main_DQN = DQN(env, <span class="string">"main"</span>)</div><div class="line">target_DQN = DQN(env, <span class="string">"target"</span>)</div><div class="line"></div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line">update_target_ops_1 = get_update_target_ops(tf.trainable_variables(), <span class="number">1</span>)</div><div class="line">update_target_ops_2 = get_update_target_ops(tf.trainable_variables(), tau)</div><div class="line"></div><div class="line">saver = tf.train.Saver()</div><div class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(path):</div><div class="line">    os.makedirs(path)</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="keyword">if</span> load_model == <span class="keyword">True</span>:</div><div class="line">        print(<span class="string">'Loading Model...'</span>)</div><div class="line">        ckpt = tf.train.get_checkpoint_state(path)</div><div class="line">        saver.restore(sess,ckpt.model_checkpoint_path)</div><div class="line">    sess.run(init_op)</div><div class="line">    </div><div class="line">    <span class="comment">#Set the target network to be equal to the main network.</span></div><div class="line">    update_target(update_target_ops_2, sess)</div><div class="line">    global_experience_buffer = ExperienceBuffer()</div><div class="line">    </div><div class="line">    <span class="comment">#create list to contain total rewards per episode</span></div><div class="line">    total_reward_list = []</div><div class="line">    steps = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_episodes + <span class="number">1</span>):</div><div class="line">        episode_buffer = ExperienceBuffer()</div><div class="line">        observation = env.reset()</div><div class="line">        observation = np.reshape(observation, [<span class="number">21168</span>])</div><div class="line">        done = <span class="keyword">False</span></div><div class="line">        total_reward_in_episode = <span class="number">0</span></div><div class="line">        steps_in_episode = <span class="number">0</span></div><div class="line">        <span class="keyword">while</span> steps_in_episode &lt; max_episode_length:</div><div class="line">            <span class="comment"># 积累样本</span></div><div class="line">            steps_in_episode += <span class="number">1</span></div><div class="line">            steps += <span class="number">1</span></div><div class="line">            <span class="keyword">if</span> np.random.rand(<span class="number">1</span>) &lt; random_threshold <span class="keyword">or</span> steps &lt; pre_train_steps:</div><div class="line">                action = np.random.randint(<span class="number">0</span>, <span class="number">4</span>)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                action = sess.run(main_DQN.action, feed_dict=&#123;main_DQN.scalar_input: [observation]&#125;)[<span class="number">0</span>]</div><div class="line">            new_observation, reward, done = env.step(action)</div><div class="line">            new_observation = np.reshape(new_observation, [<span class="number">21168</span>])</div><div class="line">            <span class="comment">#Save the experience to episode buffer</span></div><div class="line">            episode_buffer.add([[observation, action, reward, new_observation, done]])</div><div class="line">            </div><div class="line">            total_reward_in_episode += reward</div><div class="line">            observation = new_observation</div><div class="line">            <span class="comment"># 积累了 pre_train_steps/max_episode_length 次经历的pre_train_steps个样本之后，</span></div><div class="line">            <span class="comment"># 才会第一次开始衰减随机门限，进行第一次训练。</span></div><div class="line">            <span class="keyword">if</span> steps &gt;= pre_train_steps:</div><div class="line">                <span class="keyword">if</span> random_threshold &gt; random_lower_bound:</div><div class="line">                    random_threshold -= drop_step</div><div class="line">                <span class="keyword">if</span> steps % (update_freq) == <span class="number">0</span>:</div><div class="line">                    train_batch = global_experience_buffer.sample(batch_size)</div><div class="line">                    <span class="comment">#Below we perform the Double-DQN update to the target Q-values</span></div><div class="line">                    observations = np.vstack([record[<span class="number">0</span>] <span class="keyword">for</span> record <span class="keyword">in</span> train_batch])</div><div class="line">                    actions = np.array([record[<span class="number">1</span>] <span class="keyword">for</span> record <span class="keyword">in</span> train_batch])</div><div class="line">                    instant_rewards = np.array([record[<span class="number">2</span>] <span class="keyword">for</span> record <span class="keyword">in</span> train_batch])</div><div class="line">                    new_observations = np.vstack([record[<span class="number">3</span>] <span class="keyword">for</span> record <span class="keyword">in</span> train_batch])</div><div class="line">                    </div><div class="line">                    action = sess.run(main_DQN.action, feed_dict=&#123;main_DQN.scalar_input: new_observations&#125;)</div><div class="line">                    q_value, value, advantage = sess.run([target_DQN.q_value, target_DQN.value, target_DQN.advantage], </div><div class="line">                        feed_dict=&#123;target_DQN.scalar_input: new_observations&#125;)</div><div class="line">                    labels = instant_rewards + gamma * q_value[range(batch_size) ,action]</div><div class="line"></div><div class="line">                    <span class="comment"># Update the network with our target values.</span></div><div class="line">                    _ = sess.run(main_DQN.update_model, feed_dict=&#123;</div><div class="line">                        main_DQN.scalar_input: observations,</div><div class="line">                        main_DQN.target_q: labels,</div><div class="line">                        main_DQN.actions: actions&#125;)</div><div class="line"></div><div class="line">                    <span class="comment">#update the target network towards the main network.</span></div><div class="line">                    update_target(update_target_ops_2, sess)</div><div class="line">        </div><div class="line">        <span class="comment">#Get all experiences from this episode</span></div><div class="line">        global_experience_buffer.add(episode_buffer.buffer)</div><div class="line">        total_reward_list.append(total_reward_in_episode)</div><div class="line">        </div><div class="line">        <span class="keyword">if</span> i&gt;<span class="number">0</span> <span class="keyword">and</span> i % <span class="number">25</span> == <span class="number">0</span>:</div><div class="line">            print(<span class="string">'episode'</span>,i,<span class="string">', average reward of last 25 episode'</span>, np.mean(total_reward_list[<span class="number">-25</span>:]))</div><div class="line">        <span class="comment">#Periodically save the model.</span></div><div class="line">        <span class="keyword">if</span> i&gt;<span class="number">0</span> <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</div><div class="line">            saver.save(sess,path+<span class="string">'/model-'</span>+str(i)+<span class="string">'.cptk'</span>)</div><div class="line">            print(<span class="string">"Saved Model"</span>)</div><div class="line">    saver.save(sess,path+<span class="string">'/model-'</span>+str(i)+<span class="string">'.cptk'</span>)</div></pre></td></tr></table></figure></p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2017/12/09/强化学习-2-策略网络及其-TensorFlow-实现/">
                强化学习 2 策略网络及其 TensorFlow 实现
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2017-12-09</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h1 id="强化学习-2-策略网络及其-TensorFlow-实现"><a href="#强化学习-2-策略网络及其-TensorFlow-实现" class="headerlink" title="强化学习 2 策略网络及其 TensorFlow 实现"></a>强化学习 2 策略网络及其 TensorFlow 实现</h1><h2 id="策略网络"><a href="#策略网络" class="headerlink" title="策略网络"></a>策略网络</h2><p>在强化学习中，我们并不知道标签，对于某一个特定的环境状态，我们并不知道它对应的最好的动作是什么，只知道当前动作获得的奖励和试验后获得的未来奖励。我们需要让模型通过试验自己学习什么动作才是某一个特定环境状态下的较优动作，而不是告诉模型较优动作是什么，因为我们也不知道正确的答案。<br>为了让策略网络更好的理解未来，不仅仅要考虑动作的当前奖励，还要动作的考虑未来的、潜在的奖励，即奖励为 Discounted Reward。在训练过程中，模型会接触到好的动作以及它们带来的高奖励，差的动作以及它们带来的低奖励，通过学习，模型会逐渐增加选择好的动作的概率，降低选择差动作的概率，这样就逐渐完成了策略的学习。策略网络是一个End-to-End（端到端）的方法，直接产生动作。</p>
<h2 id="Gym"><a href="#Gym" class="headerlink" title="Gym"></a>Gym</h2><p>核心概念</p>
<ol>
<li>Environment，</li>
<li>Agent（编写的算法/模型）<br>Agent 观察环境状态（Observation），执行动作（Action）改变环境（Environment），得到奖励（Reward），再观察环境状态执行动作得到奖励，周而复始。即 Observation-Action-Reward 循环。<h2 id="策略网络解决-CartPole-任务"><a href="#策略网络解决-CartPole-任务" class="headerlink" title="策略网络解决 CartPole 任务"></a>策略网络解决 CartPole 任务</h2></li>
<li>Observation 四元组（车的位置，车的速度，杆的角度，杆的速度）</li>
<li>Action Space 给小车施加正向或者反向的力（0， 1）</li>
<li>任务目标时尽可能保持杆竖直不倒，当小车偏离中心超过2.4个单位或者杆的倾角大于15度时，任务失败。</li>
<li>在每坚持一步就会获得+1的奖励，模型要坚持尽可能长的时间而不导致任务失败。</li>
<li>模型为了获得尽可能多的奖励就要有远见，不能仅考虑当前的奖励，也要考虑未来长远的奖励<br>采取随机动作的 Agent：<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> gym</div><div class="line">env = gym.make(<span class="string">"CartPole-v0"</span>)</div><div class="line"></div><div class="line">env.reset()</div><div class="line">random_episodes = <span class="number">0</span></div><div class="line">reward_sum = <span class="number">0</span></div><div class="line"><span class="keyword">while</span> random_episodes &lt; <span class="number">10</span>:</div><div class="line">    observation, reward, done, _ = env.step(np.random.choice([<span class="number">0</span>, <span class="number">1</span>]))</div><div class="line">    reward_sum += reward</div><div class="line">    <span class="keyword">if</span> done:</div><div class="line">        random_episodes += <span class="number">1</span></div><div class="line">        print(<span class="string">"Reward for this episode was: "</span>, reward_sum)</div><div class="line">        reward_sum = <span class="number">0</span></div><div class="line">        env.reset()</div></pre></td></tr></table></figure>
</li>
</ol>
<p>MLP Policy Network：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!coding: utf-8</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </div><div class="line"><span class="keyword">import</span> gym </div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </div><div class="line">env = gym.make(<span class="string">"CartPole-v0"</span>)</div><div class="line"></div><div class="line"><span class="comment">################################# 网络结构 #####################</span></div><div class="line">hidden_n = <span class="number">50</span></div><div class="line">batch_size = <span class="number">25</span></div><div class="line">learning_rate = <span class="number">1e-1</span></div><div class="line">input_dim = <span class="number">4</span></div><div class="line">output_dim = <span class="number">1</span></div><div class="line">gamma = <span class="number">0.99</span></div><div class="line"></div><div class="line">observation_placeholder = tf.placeholder(tf.float32, [<span class="keyword">None</span>, input_dim], name=<span class="string">"observation_placeholder"</span>)</div><div class="line">w_1 = tf.get_variable(<span class="string">"w_1"</span>, shape=[input_dim, hidden_n], initializer=tf.contrib.layers.xavier_initializer())</div><div class="line">layer_1 = tf.nn.relu(tf.matmul(observation_placeholder, w_1))</div><div class="line">w_2 = tf.get_variable(<span class="string">"w_2"</span>, shape=[hidden_n, output_dim], initializer=tf.contrib.layers.xavier_initializer())</div><div class="line">probability = tf.nn.sigmoid(tf.matmul(layer_1, w_2))</div><div class="line"></div><div class="line"><span class="comment">################################# 批优化 #####################</span></div><div class="line"><span class="string">'''</span></div><div class="line"><span class="string">深度强化学习的训练也是采用 batch training，不逐个“样本”的更新参数，而累计 batch_size 个经历（episode）再更新参数，防止</span></div><div class="line"><span class="string">单一经历（episode）随机扰动噪声对模型训练带来的不利影响。</span></div><div class="line"><span class="string">不像 CNN 那样一个 batch 样本 feed 进模型，直接产生一个梯度平均值就可以进行一次参数的更新，</span></div><div class="line"><span class="string">这里需要进行 batch_size 次play从而得到 batch_size 个经历（episode），需要存储每经历（episode）中的平均梯度，并进行平均，最终更新一次参数。</span></div><div class="line"><span class="string">'''</span></div><div class="line">adam = tf.train.AdamOptimizer(learning_rate=learning_rate)</div><div class="line">w_1_grad = tf.placeholder(tf.float32, name=<span class="string">"batch_grad_1"</span>)</div><div class="line">w_2_grad = tf.placeholder(tf.float32, name=<span class="string">"batch_grad_2"</span>)</div><div class="line">batch_grad = [w_1_grad, w_2_grad]</div><div class="line">update = adam.apply_gradients(zip(batch_grad, tf.trainable_variables()))</div><div class="line"></div><div class="line"><span class="comment">################################# 损失 #####################</span></div><div class="line"><span class="string">'''</span></div><div class="line"><span class="string">在 CartPole 问题中，每一次得到的奖励和这次奖励之前的所有动作都有关，也就说之前的所有动作导致了这次奖励的得到，</span></div><div class="line"><span class="string">为了计算一个动作带来的奖励我们要计算这个动作之后全部奖励的折扣和。</span></div><div class="line"><span class="string">我们倒推求解每一个动作带来的奖励。在 CartPole 任务中除了导致任务失败的那次动作之外，所有动作的即时奖励都是 1。</span></div><div class="line"><span class="string">一个动作带来的奖励是后一时间步动作带来的奖励的折扣加上这个动作的即时奖励。</span></div><div class="line"><span class="string">'''</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">discount_reward</span><span class="params">(r)</span>:</span></div><div class="line">    d_r = np.zeros_like(r)</div><div class="line">    d_r[<span class="number">-1</span>] = r[<span class="number">-1</span>]</div><div class="line">    <span class="keyword">for</span> time <span class="keyword">in</span> range(len(r)<span class="number">-1</span>)[::<span class="number">-1</span>]:</div><div class="line">        d_r[time] =  d_r[time+<span class="number">1</span>] * gamma + r[time]</div><div class="line">    <span class="keyword">return</span> d_r</div><div class="line"></div><div class="line"><span class="comment"># 模型做出的动作的反动作，学习目标，相当于 label，是来自于模型的概率输出与一个来自均匀分布的随机数的比较</span></div><div class="line">opposite_action = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>], name=<span class="string">"opposite_action"</span>)</div><div class="line"><span class="comment"># 动作带来的奖励</span></div><div class="line">advantage = tf.placeholder(tf.float32, name=<span class="string">"reward_signal"</span>)</div><div class="line"><span class="string">'''</span></div><div class="line"><span class="string">模型得到这份奖励的概率：</span></div><div class="line"><span class="string">opposite_action=1 时，即 action=0，1- probability</span></div><div class="line"><span class="string">opposite_action=0 时，即 action=1，probability</span></div><div class="line"><span class="string">综合上面两个式子，模型得到这份奖励的概率：</span></div><div class="line"><span class="string">opposite_action * (opposite_action - probability) + (1 - opposite_action) * (opposite_action + probability)</span></div><div class="line"><span class="string">'''</span></div><div class="line"><span class="comment"># 模型得到这份奖励的概率的对数：</span></div><div class="line">log_prob = tf.log(opposite_action * (opposite_action - probability) </div><div class="line">    + (<span class="number">1</span> - opposite_action) * (opposite_action + probability))</div><div class="line">loss = -tf.reduce_mean(log_prob * advantage)</div><div class="line">gradients = tf.gradients(loss, tf.trainable_variables())</div><div class="line"></div><div class="line"><span class="comment">################################# 训练 #####################</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    init = tf.global_variables_initializer()</div><div class="line">    sess.run(init)</div><div class="line"></div><div class="line">    observation = env.reset()</div><div class="line">    xs, ys, rewards = [], [], []</div><div class="line">    reward_sum = <span class="number">0</span></div><div class="line">    episode = <span class="number">1</span></div><div class="line">    total_episodes = <span class="number">10000</span></div><div class="line"></div><div class="line">    grad_buffer = sess.run(tf.trainable_variables())</div><div class="line">    <span class="keyword">for</span> i, grad <span class="keyword">in</span> enumerate(grad_buffer):</div><div class="line">        grad_buffer[i] = grad * <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="keyword">while</span> episode &lt;= total_episodes:</div><div class="line">        x = np.reshape(observation, [<span class="number">1</span>, input_dim])</div><div class="line">        prob = sess.run(probability, feed_dict=&#123;observation_placeholder: x&#125;)</div><div class="line">        action = <span class="number">1</span> <span class="keyword">if</span> np.random.uniform() &lt; prob <span class="keyword">else</span> <span class="number">0</span></div><div class="line">        xs.append(x)</div><div class="line">        ys.append(<span class="number">1</span> - action)</div><div class="line">        observation, reward, done, info = env.step(action)</div><div class="line">        reward_sum += reward</div><div class="line">        rewards.append(reward)</div><div class="line">        <span class="keyword">if</span> done:</div><div class="line">            <span class="comment">### 构建一个经历（episode）的样本，计算该经历（episode）中所有样本上的梯度和，并加入到此批次的梯度buffer中 #####</span></div><div class="line">            episode += <span class="number">1</span></div><div class="line">            xs_per_episode = np.vstack(xs)</div><div class="line">            ys_per_episode = np.vstack(ys)</div><div class="line">            rewards_per_episode = np.vstack(rewards)</div><div class="line">            xs, ys, rewards = [], [], []</div><div class="line">            d_r = discount_reward(rewards_per_episode)</div><div class="line">            d_r -= np.mean(d_r)</div><div class="line">            d_r /= np.std(d_r)</div><div class="line">            grad_per_episode = sess.run(gradients, feed_dict=&#123;observation_placeholder: xs_per_episode,</div><div class="line">                opposite_action: ys_per_episode,</div><div class="line">                advantage: d_r&#125;)</div><div class="line">            <span class="keyword">for</span> i, grad <span class="keyword">in</span> enumerate(grad_per_episode):</div><div class="line">                grad_buffer[i] += grad</div><div class="line"></div><div class="line">            <span class="keyword">if</span> episode % batch_size == <span class="number">0</span>:</div><div class="line">                sess.run(update, feed_dict=&#123;w_1_grad: grad_buffer[<span class="number">0</span>], w_2_grad: grad_buffer[<span class="number">1</span>]&#125;)</div><div class="line"></div><div class="line">                <span class="keyword">for</span> i, grad <span class="keyword">in</span> enumerate(grad_buffer):</div><div class="line">                    grad_buffer[i] = grad * <span class="number">0</span></div><div class="line"></div><div class="line">                print(<span class="string">"Average reward for episode %d : %.2f"</span> %(episode, reward_sum/batch_size))</div><div class="line"></div><div class="line">                <span class="keyword">if</span> reward_sum/batch_size &gt;= <span class="number">200</span>:</div><div class="line">                    print(<span class="string">"Task solved in %d episodes"</span> % episode)</div><div class="line">                    <span class="keyword">break</span></div><div class="line">                reward_sum =<span class="number">0</span></div><div class="line">            observation = env.reset()</div></pre></td></tr></table></figure></p>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2017/12/07/强化学习-1-Basic/">
                强化学习 1 Basic
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2017-12-07</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h1 id="强化学习基础"><a href="#强化学习基础" class="headerlink" title="强化学习基础"></a>强化学习基础</h1><h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p>主要用于解决连续决策问题<br>三个主要概念：</p>
<ol>
<li>环境（Environment）</li>
<li>动作（Action）</li>
<li>奖励（Reward）<br>强化学习的目标就是让模型根据环境，动作和奖励学习出最佳的策略。<br>强化学习不像监督学习那样具有明确的标签（目标），也不像无监督学习那样完全没有标签（目标），强化学习的目标一般是变化的，不明确的，甚至不是绝对正确的。<br>Google 的 DQN（Deep Q-Network）结合了深度学习和强化学习。<br>DeepMind 的 AlphaGo 结合了策略网络（Policy Network）、估值网络（Value Network 即DQN）与蒙特卡洛搜索树。<br>无人驾驶传感，CNN，RNN 对环境信息进行处理，再结合强化学习做出决策。</li>
</ol>
<h2 id="两类方法"><a href="#两类方法" class="headerlink" title="两类方法"></a>两类方法</h2><ol>
<li>Policy-Based （Policy Gradients） <ul>
<li>直接给出某个环境状态下应该采取的动作</li>
<li>适用于动作种类非常多或者连续取值的动作</li>
</ul>
</li>
<li>Value-Based（Q-Learning）<ul>
<li>给出某个环境状态下所有动作的Q值，之后可以选择Q值最高的动作。</li>
<li>少量离散取值的动作</li>
</ul>
</li>
</ol>
<h2 id="Model-Based-or-Model-Free"><a href="#Model-Based-or-Model-Free" class="headerlink" title="Model-Based or Model-Free"></a>Model-Based or Model-Free</h2><p>是否可以对环境进行建模，在复杂环境下一般使用 Model-Free 强化学习，同时给予更多的样本，弥补对于环境建模的缺失。</p>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2017/12/07/Learning-TensorFlow-2-计算加速/">
                Learning TensorFlow 2 计算加速
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2017-12-07</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2017/12/07/循环神经网络的原理及-TensorFlow-实现/">
                循环神经网络的原理及 TensorFlow 实现
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2017-12-07</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h2 id="循环神经网络的原理及-TensorFlow-实现"><a href="#循环神经网络的原理及-TensorFlow-实现" class="headerlink" title="循环神经网络的原理及 TensorFlow 实现"></a>循环神经网络的原理及 TensorFlow 实现</h2><h3 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><p><strong>循环体</strong>的网络结构的参数在不同时刻是共享的, 循环体的网络结构的输出不仅提供给下一时刻作为输入，同时也提供了当前时刻的输出。为了将循环体的输出转化为最终的输出，需要一个全连接来完成这个过程，这和卷积神经网络中最后的全连接层意义是一样的，不同时刻用于输出的全连接的参数也是一样的。<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/RNN1.jpg" alt="RNN的前向传播的计算过程示意图"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">#!coding: utf-8</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">X = [1, 2]</div><div class="line">state = [0.0, 0.0]</div><div class="line"># 分开定义不同输入部分的权重以方便操作</div><div class="line">w_cell_state = np.matrix([[0.1, 0.2], [0.3, 0.4]])</div><div class="line">w_cell_input = np.matrix([0.5, 0.6])</div><div class="line">b_cell = [0.1, -0.1]</div><div class="line"></div><div class="line">w_output = np.matrix([[1.0], [2.0]])</div><div class="line">b_output = 0.1</div><div class="line"></div><div class="line"># 按照时间顺序执行循环神经网络的前向传播过程</div><div class="line">for i in range(len(X)):</div><div class="line">     before_activation = state * w_cell_state + X[i] * w_cell_input + b_cell</div><div class="line">     state = np.tanh(before_activation)</div><div class="line">     final_output = state * w_output + b_output</div><div class="line">     print &quot;before activation: &quot;, before_activation</div><div class="line">     print &quot;state: &quot;, state</div><div class="line">     print &quot;output: &quot;, final_output</div></pre></td></tr></table></figure></p>
<p>输出结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">before activation:  [[ 0.6  0.5]]</div><div class="line">state:  [[ 0.53704957  0.46211716]]</div><div class="line">output:  [[ 1.56128388]]</div><div class="line">before activation:  [[ 1.2923401   1.39225678]]</div><div class="line">state:  [[ 0.85973818  0.88366641]]</div><div class="line">output:  [[ 2.72707101]]</div></pre></td></tr></table></figure></p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>上面的RNN并没有记忆的功能，仅仅是将上一时刻的输出输入到该时刻，仅仅就是一个“循环”神经网络，LSTM 不仅仅将上一时刻的输出输入到该时刻，而且内部还有一个“记忆单元”可以长久保持细胞的状态。即将上一时刻细胞的状态保持到该时刻，并在该时刻进行更新，从而可以继续保持到下一时刻。<br>LSTM中有三种门，输入门，遗忘门，输出门。门是通过 sigmoid 产生的，产生的门要去 pointwise 地乘才能发挥作用，门都需要去乘以一个东西。三种门的产生所需的构造材料是不一样的，作用的对象也是不一样的。<code>h</code> 是循环体的输出（不一定是这一时刻网络的输出，因为可能还要经过一个全连接层才能成为这一时刻网络的输出；但是是可能经过激活函数激活的），<code>x</code> 是循环体的输入，<code>c</code>是一个细胞状态，通过 <code>h</code> 和 <code>x</code> 以及上一时刻的细胞状态进行更新，它的意义就是对过往的一切经过一系列遗忘之后还剩下的东西，是对过往的概括。<br>遗忘门的生成：<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/forget_gate.png" alt="forget_gate"><br>输入门的生成：<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/input_gate.png" alt="input_gate"><br>遗忘门和输入门发挥作用：<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/forget_input_gate.png" alt="forget_input_gate"><br>输出门的生成及发挥作用：<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/output_gate.png" alt="output_gate"><br>LSTM cell 输入、输出、状态、门之间的交互<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/RNN2.jpg" alt="RNN2"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">lstm = rnn_cell.BasicLSTMCell(lstm_hidden_size)</div><div class="line"># 将LSTM中的状态初始化为全零数组</div><div class="line">state = lstm.zero_state(batch_size, tf.float32)</div><div class="line">loss = 0.0</div><div class="line"># 虽然理论上循环神经网络可以处理任意长度的序列，但是训练时为了避免梯度消散，会规定一个最大的序列长度，用 num_steps 来表示</div><div class="line">for i in range(num_steps):</div><div class="line">    if i &gt; 0:</div><div class="line">        tf.get_variable_scope().reuse_variables()</div><div class="line">    lstm_output, state = lstm(current_input, state)</div><div class="line">    final_output = fully_connected(lstm_output)</div><div class="line">    # 计算当前时刻 batch 上的损失</div><div class="line">    loss += calc_loss(final_output, expected_output)</div><div class="line"># 使用前面类似的方法训练模型</div></pre></td></tr></table></figure></p>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/RNN3.jpg" alt="RNN3"></p>
<h3 id="循环神经网络的dropout"><a href="#循环神经网络的dropout" class="headerlink" title="循环神经网络的dropout"></a>循环神经网络的dropout</h3><p>在卷积神经网络上，通过dropout可以让神经网络更加的健壮，类似的在循环神经网络中也可以使用dropout，而且类似卷积神经网络只在最后的全连接层使用dropout，循环神经网络一般只在循环体不同层之间使用dropout，而不在同一层的循环体之间使用dropout。<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/RNN4.jpg" alt="RNN4"></p>
<h3 id="循环神经网络的变种"><a href="#循环神经网络的变种" class="headerlink" title="循环神经网络的变种"></a>循环神经网络的变种</h3><h4 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h4><p>在一些问题中，当前时刻的输出不仅与之前的状态有关还和之后的状态有关，这时就需要使用双向循环神经网络。例如预测一个句子中缺失的单词，就需要前后文相结合。</p>
<h4 id="深层循环神经网络"><a href="#深层循环神经网络" class="headerlink" title="深层循环神经网络"></a>深层循环神经网络</h4><p>为了增强模型的表达能力，在一个时刻上堆叠多个循环体，和卷积神经网络一样，每一层的循环体参数是一致的共享的，不同层之间的参数是不一样的。<br>为了支持深层循环神经网络，TensorFlow 提供了 MultiRNNCell。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"># 定义一个循环体作为组件</div><div class="line">lstm = rnn_cell.BasicLSTMCell(lstm_size)</div><div class="line">stacked_lstm = rnn_cell.MultiRNNCell([lstm] * number_of_layers)</div><div class="line"></div><div class="line"># 与经典的循环神经网络一样，通过zero_state来初始化初始状态</div><div class="line">state = stacked_lstm.zero_state(batch_size, tf.float32)</div><div class="line"></div><div class="line">for i in range(num_steps):</div><div class="line">    if i &gt; 0:</div><div class="line">        tf.get_variable_scope().reuse_variables()</div><div class="line">    lstm_output, state = stacked_lstm(current_input, state)</div><div class="line">    final_output = fully_connected(lstm_output)</div><div class="line">    # 计算当前时刻 batch 上的损失</div><div class="line">    loss += calc_loss(final_output, expected_output)</div></pre></td></tr></table></figure></p>

        </div>
    

</div>
            
        </section>
    </div>
</div>



    <div class="row">
        <div class="col-sm-12">
            <div class="wrap-pagination">
                <a class="" href="/page/3/">
                    <i class="fa fa-chevron-left" aria-hidden="true"></i>
                </a>
                <a class="" href="/page/5/">
                    <i class="fa fa-chevron-right" aria-hidden="true"></i>
                </a>
            </div>
        </div>
    </div>




</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This is Robert Lexis (FengCun Li). To see the world, things dangerous to come to, to see behind walls, to draw closer, to find each other and to feel. That is the purpose of LIFE.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2018/08/13/消失的梯度/">消失的梯度</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/11/MobileNet-V1-and-V2-带来的卷积结构革命/">MobileNet V1 and V2 带来的卷积</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/09/卷积转置卷积关系在-TensorFLow-中的验证/">卷积转置卷积关系在 TensorFLow 中的验证</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/08/转置卷积-Transposed-Convolution/">转置卷积 Transposed Convoluti</a>
            </li>
            
        </ul>
    </div>



            
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/RobertLexis">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:robert_lexis@163.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Robert Lexis Loves Wenny
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



</body>

</html>