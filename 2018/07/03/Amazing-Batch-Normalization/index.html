<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="è¿™ç¯‡åšæ–‡ä¸»è¦æ€»ç»“äº† batch normalization çš„å…¬å¼ã€ä¸ºä»€ä¹ˆæœ‰æ•ˆçš„ç›´è§‚è§£é‡Šã€‚åœ¨åšæ–‡çš„æœ€åï¼ŒéªŒè¯äº†é€šè¿‡ä½¿ç”¨ batch normalization å¯ä»¥åœ¨æ›´å°‘çš„ epoch ä¸‹è·å¾—ç²¾åº¦ä¸Šçš„æ˜¾è‘—æå‡ã€‚">
<meta name="keywords" content="DL">
<meta property="og:type" content="article">
<meta property="og:title" content="Amazing Batch Normalization">
<meta property="og:url" content="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/index.html">
<meta property="og:site_name" content="The next stop - Antarctica">
<meta property="og:description" content="è¿™ç¯‡åšæ–‡ä¸»è¦æ€»ç»“äº† batch normalization çš„å…¬å¼ã€ä¸ºä»€ä¹ˆæœ‰æ•ˆçš„ç›´è§‚è§£é‡Šã€‚åœ¨åšæ–‡çš„æœ€åï¼ŒéªŒè¯äº†é€šè¿‡ä½¿ç”¨ batch normalization å¯ä»¥åœ¨æ›´å°‘çš„ epoch ä¸‹è·å¾—ç²¾åº¦ä¸Šçš„æ˜¾è‘—æå‡ã€‚">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/BN.png">
<meta property="og:image" content="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/covariate_shift.png">
<meta property="og:image" content="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/why_BN_works.png">
<meta property="og:image" content="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/ModelAccuracy.png">
<meta property="og:image" content="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/val_loss.png">
<meta property="og:updated_time" content="2018-09-03T14:06:25.922Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Amazing Batch Normalization">
<meta name="twitter:description" content="è¿™ç¯‡åšæ–‡ä¸»è¦æ€»ç»“äº† batch normalization çš„å…¬å¼ã€ä¸ºä»€ä¹ˆæœ‰æ•ˆçš„ç›´è§‚è§£é‡Šã€‚åœ¨åšæ–‡çš„æœ€åï¼ŒéªŒè¯äº†é€šè¿‡ä½¿ç”¨ batch normalization å¯ä»¥åœ¨æ›´å°‘çš„ epoch ä¸‹è·å¾—ç²¾åº¦ä¸Šçš„æ˜¾è‘—æå‡ã€‚">
<meta name="twitter:image" content="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/BN.png">

<link rel="canonical" href="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Amazing Batch Normalization | The next stop - Antarctica</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ ">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">The next stop - Antarctica</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">To see behind walls. To draw closer.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>æ ‡ç­¾</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>åˆ†ç±»</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Fengcun Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="The next stop - Antarctica">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Amazing Batch Normalization
        </h1>

        <div class="post-meta">
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2018-09-03 22:06:25" itemprop="dateModified" datetime="2018-09-03T22:06:25+08:00">2018-09-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>è¿™ç¯‡åšæ–‡ä¸»è¦æ€»ç»“äº† batch normalization çš„å…¬å¼ã€ä¸ºä»€ä¹ˆæœ‰æ•ˆçš„ç›´è§‚è§£é‡Šã€‚<br>åœ¨åšæ–‡çš„æœ€åï¼ŒéªŒè¯äº†é€šè¿‡ä½¿ç”¨ batch normalization å¯ä»¥åœ¨æ›´å°‘çš„ epoch ä¸‹è·å¾—ç²¾åº¦ä¸Šçš„æ˜¾è‘—æå‡ã€‚<br><a id="more"></a></p>
<blockquote>
<p>Batch Normalization was first introduced by two researchers at Google, Sergey Ioffe and Christian Szegedy in their paper â€œBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shiftâ€ in 2015. The authors showed that batch normalization improved the top result of ImageNet (2014) by a significant margin using only 7% of the training steps. Today, Batch Normalization is used in almost all CNN architectures.</p>
</blockquote>
<p>å¯¹è¾“å…¥ç‰¹å¾åšå½’ä¸€åŒ–èƒ½å¤Ÿå¸®åŠ©ç¥ç»ç½‘ç»œçš„è®­ç»ƒï¼ŒBatch Normalization å°±æ˜¯åœ¨éšè—å±‚ä¸Šåšå½’ä¸€åŒ–ã€‚ä½†æ˜¯è¾“å…¥å±‚å’Œéšè—å±‚ä¸Šçš„å½’ä¸€åŒ–è¿˜æ˜¯æœ‰æ‰€ä¸åŒçš„ï¼Œå¯¹éšè—å±‚ Batch Norm ä¹‹åå¹¶ä¸è¦æ±‚ä¸€å®šæ˜¯é›¶å‡å€¼å’Œå•ä½æ–¹å·®ï¼Œè€Œæ˜¯é€šè¿‡ learnable parameters \( \gamma \) å’Œ \( \beta \) ä½¿å¾—éšè—å±‚å…·æœ‰å¯æ§çš„ã€ä¿®æ­£çš„å‡å€¼å’Œæ–¹å·®ï¼Œè¿™ä¸ªå‡å€¼å’Œæ–¹å·®å¯ä»¥æ˜¯ 0 å’Œ 1ï¼Œä¹Ÿå¯ä»¥æ˜¯ç”± \( \beta \) å’Œ \( \gamma \) æ§åˆ¶çš„å…¶ä»–å€¼ã€‚<br>Batch Norm å¯ä»¥è®©ç¥ç»ç½‘ç»œçš„è®­ç»ƒå¯¹äºè¶…å‚æ•°çš„é€‰æ‹©ä¸é‚£ä¹ˆæ•æ„Ÿï¼Œä¹Ÿè®©å¾ˆæ·±çš„ç¥ç»ç½‘ç»œçš„è®­ç»ƒå˜çš„æ›´åŠ å®¹æ˜“ã€‚</p>
<h5 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h5><p>\(a_{n}^{[l](m)}\) æ˜¯ ç¬¬ \( l \) å±‚ä¸Šçš„ç¬¬ \( n \) ä¸ªç¥ç»å…ƒåœ¨ç¬¬ \( m \) ä¸ªæ ·æœ¬ä¸Šçš„æ¿€æ´»å€¼ã€‚</p>
<h3 id="å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–"><a href="#å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–" class="headerlink" title="å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–"></a>å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–</h3><p>\[<br>\mu = \frac{1}{m} \sum_{i}X^{(i)} \\<br>X = X - \mu \\<br>\sigma^2 = \frac{1}{m} \sum_{i} (X^{(i)})^2 \\<br>X = \frac{X}{\sigma}<br>\]</p>
<h3 id="Can-we-normalize-z-2-so-we-can-train-w-3-and-b-3-more-efficiently-and-faster"><a href="#Can-we-normalize-z-2-so-we-can-train-w-3-and-b-3-more-efficiently-and-faster" class="headerlink" title="Can we normalize \( z^{[2]} \) so we can train \(w^{[3]}\) and \( b^{[3]} \) more efficiently and faster ?"></a>Can we normalize \( z^{[2]} \) so we can train \(w^{[3]}\) and \( b^{[3]} \) more efficiently and faster ?</h3><p>Do normalization before activation.<br>Given some intermediate values in NN \( z^{[l](i)} \)ï¼Œshape æ˜¯ \( (n^{[l]}, 1) \)ï¼Œ\( n^{[l]} \) æ˜¯ layer \( l \) çš„ç¥ç»å…ƒä¸ªæ•°ï¼Œç®€å†™ \( z^{[l](i)} \) ä¸º \(z^{(i)}\)ã€‚<br><img src="/2018/07/03/Amazing-Batch-Normalization/BN.png" alt="BN"><br>åŠ åç½®ä¼šè¢«å‡å‡å€¼æ‰€æŠµæ¶ˆï¼Œå› æ­¤å°±æ²¡æœ‰åŠ åç½®çš„å¿…è¦äº†ã€‚<br>\[<br>z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}  \longrightarrow z^{[l]} = W^{[l]}a^{[l-1]}<br>\]<br>parameters for layer \( l \): </p>
<ul>
<li>\( W^{[l]} \), \( \beta^{[l]} \), \( \gamma^{[l]} \).</li>
<li>\( \beta^{[l]} \), \( \gamma^{[l]} \) çš„ shape å‡ä¸º \( (n^{[l]}, 1) \)ã€‚</li>
</ul>
<p>åœ¨ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­ä¸­ replace \( z^{[l]} \) with \( \tilde{z}^{[l]} \).</p>
<h3 id="Why-does-Batch-Norm-work"><a href="#Why-does-Batch-Norm-work" class="headerlink" title="Why does Batch Norm work ?"></a>Why does Batch Norm work ?</h3><p>åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒä¸­ï¼Œä¸è®©è¾“å…¥çš„ä¸åŒç‰¹å¾å…·æœ‰ä¸åŒçš„å°ºåº¦ç›¸æ¯”è¾ƒï¼Œè®©è¾“å…¥çš„ä¸åŒç‰¹å¾åœ¨ç›¸åŒçš„ range å†…å¯ä»¥æå¤§çš„åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚</p>
<h5 id="learning-on-shifting-input-distribution"><a href="#learning-on-shifting-input-distribution" class="headerlink" title="learning on shifting input distribution"></a>learning on shifting input distribution</h5><p><img src="/2018/07/03/Amazing-Batch-Normalization/covariate_shift.png" alt="covariate_shift"><br>åœ¨ä¸Šå›¾ä¸­å·¦ä¾§æ•°æ®é›†ä¸Šè®­ç»ƒå¾—åˆ°çš„ç¥ç»ç½‘ç»œæ¨¡å‹å¾ˆéš¾åœ¨å³ä¾§çš„æ•°æ®é›†ä¸Šå¾—åˆ°å¥½çš„ç»“æœï¼Œè¿™æ˜¯å› ä¸ºæ•°æ®é›†çš„åˆ†å¸ƒå‘ç”Ÿäº†æ”¹å˜ã€‚<br>å½“æ•°æ®åˆ†å¸ƒéšç€æŸç§ä¸œè¥¿åœ¨å˜ï¼ˆåœ¨ä¸Šé¢çš„ä¾‹å­ä¸­å°±æ˜¯æ•°æ®åˆ†å¸ƒéšç€æ ·æœ¬é›†åˆçš„ä¸åŒè€Œå˜åŒ–ï¼‰ï¼Œæˆ‘ä»¬å¿…é¡»é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œè¿™å°±å«åš covariate shiftã€‚<br>\( X \longrightarrow y \)ï¼Œif the distribution of \( X \) changesï¼Œwe must retrain the model.</p>
<h5 id="covariate-shift-æ˜¯å¦‚ä½•å½±å“ç¥ç»ç½‘ç»œçš„"><a href="#covariate-shift-æ˜¯å¦‚ä½•å½±å“ç¥ç»ç½‘ç»œçš„" class="headerlink" title="covariate shift æ˜¯å¦‚ä½•å½±å“ç¥ç»ç½‘ç»œçš„?"></a>covariate shift æ˜¯å¦‚ä½•å½±å“ç¥ç»ç½‘ç»œçš„?</h5><p><img src="/2018/07/03/Amazing-Batch-Normalization/why_BN_works.png" alt="why_BN_works"><br>ç¬¬äºŒå±‚æ¿€æ´»å€¼çš„åˆ†å¸ƒéšç€è®­ç»ƒè¿‡ç¨‹ä¼šå‘ç”Ÿå˜åŒ–ï¼Œè¿™ä½¿å¾—ç¬¬ä¸‰å±‚çš„æ¨¡å‹å‚æ•°éœ€è¦é‡æ–°è®­ç»ƒã€‚</p>
<blockquote>
<p>In a neural network, each hidden unitâ€™s input distribution changes every time there is a parameter update in the previous layer. This is called <strong>internal covariate shift</strong>. This makes training slow and requires a very small learning rate and a good parameter initialization. This problem can be solved by normalizing the layerâ€™s inputs over a mini-batch.<br>å‡è®¾åœ¨ä¸€å®šçš„ \( [a_{1}^{[2]}, a_{2}^{[2]}, a_{3}^{[2]}, a_{4}^{[2]}] \) çš„è¾“å…¥ä¸Šï¼Œ\(<br>W^{[3]}, b^{[3]}, W^{[4]}, b^{[4]}, W^{[5]}, b^{[5]} \) å¾ˆå¥½åœ°å®Œæˆäº†ä»»åŠ¡ï¼ˆå¾—åˆ°äº†æ­£ç¡®çš„æ ‡ç­¾ \( \hat{y} \)ï¼‰ï¼Œä½†æ˜¯å½“ \(<br>W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]} \) æ”¹å˜äº†ï¼Œ\( [a_{1}^{[2]}, a_{2}^{[2]}, a_{3}^{[2]}, a_{4}^{[2]}] \) ä¹Ÿä¼šå‘ç”Ÿå˜åŒ–ï¼Œå› æ­¤ä»ç¬¬ä¸‰å±‚çš„è§’åº¦çœ‹ï¼Œç¬¬äºŒå±‚çš„è¾“å‡ºçš„åˆ†å¸ƒä¸€ç›´åœ¨å˜ï¼Œå› æ­¤ä¼šå—åˆ° covariate shift çš„å½±å“ä½¿å¾—ä¹‹å‰è¿­ä»£æ­¥éª¤è®­ç»ƒå¾—åˆ°çš„ \(<br>W^{[3]}, b^{[3]}, W^{[4]}, b^{[4]}, W^{[5]}, b^{[5]} \) ä¸å†æœ‰æ•ˆã€‚<br>Batch Normalization å¯ä»¥å‡å°‘éšè—å•å…ƒå€¼çš„åˆ†å¸ƒçš„ä¸ç¨³å®šæ€§ã€‚<br>\( z^{[2]} \) å¯ä»¥å‘ç”Ÿå˜åŒ–ï¼Œå®é™…ä¸Šéšç€ç¥ç»ç½‘ç»œæ›´æ–°å‰å‡ å±‚çš„å‚æ•°ï¼Œå®ƒä»¬ç¡®å®ä¼šå‘ç”Ÿå˜åŒ–ï¼Œä½†æ˜¯ BN ä¿è¯å®ƒä»¬çš„å‡å€¼å’Œæ–¹å·®æ˜¯ç¨³å®šçš„ã€å¯æ§çš„ï¼Œç”± \( \beta^{[2]} \) å’Œ \( \gamma^{[2]} \) æ§åˆ¶ã€‚<br>BN é™åˆ¶äº†å…ˆå‰å±‚å‚æ•°æ›´æ–°å¯¹ç¬¬ä¸‰å±‚çš„è¾“å…¥çš„åˆ†å¸ƒçš„å½±å“ï¼Œä½¿å¾—æ•°æ®åˆ†å¸ƒæ›´åŠ ç¨³å®šï¼Œè®©ç¥ç»ç½‘ç»œåç»§å±‚åœ¨æ›´åŠ ç¨³å›ºçš„åŸºç¡€ä¸Šè¿›è¡Œå­¦ä¹ ï¼Œå‰Šå¼±äº†å„ä¸ªå±‚æ¬¡å‚æ•°å­¦ä¹ è¿‡ç¨‹çš„è€¦åˆï¼Œè®©æ¯ä¸€å±‚å‚æ•°çš„å­¦ä¹ æ›´åŠ ç‹¬ç«‹ï¼Œæœ‰æ•ˆæé«˜æ•´ä¸ªç¥ç»ç½‘ç»œçš„è®­ç»ƒæ•ˆç‡ã€‚</p>
</blockquote>
<h3 id="Other-benefits-of-Batch-Normalization"><a href="#Other-benefits-of-Batch-Normalization" class="headerlink" title="Other benefits of Batch Normalization"></a>Other benefits of Batch Normalization</h3><h5 id="Batch-Norm-as-regularization"><a href="#Batch-Norm-as-regularization" class="headerlink" title="Batch Norm as regularization"></a>Batch Norm as regularization</h5><ul>
<li>Each mini-batch is scaled by the mean/variance computed on just that mini-batch.</li>
<li>This adds some noise to the values \( z^{[ğ‘™]} \) within that minibatch. So similar to dropout, it adds some noise to each hidden layerâ€™s activations. </li>
<li>This has a slight regularization effect. </li>
</ul>
<h5 id="Higher-learning-rate"><a href="#Higher-learning-rate" class="headerlink" title="Higher learning rate"></a>Higher learning rate</h5><p>If we use a high learning rate in a traditional neural network, then the gradients could explode or vanish. Large learning rates can scale the parameters which could amplify the gradients, thus leading to an explosion. But if we do batch normalization, small changes in parameter to one layer do not get propagated to other layers. This makes it possible to use higher learning rates for the optimizers, which otherwise would not have been possible. It also makes gradient propagation in the network more stable.</p>
<h3 id="Batch-Norm-at-test-time"><a href="#Batch-Norm-at-test-time" class="headerlink" title="Batch Norm at test time"></a>Batch Norm at test time</h3><p>åœ¨æ¨ç†/æµ‹è¯•çš„æ—¶å€™ï¼Œä½¿ç”¨ across mini-batches çš„æŒ‡æ•°åŠ æƒæ»‘åŠ¨å¹³å‡ä¼°è®¡ \( \mu \)ï¼Œ\( \sigma^{2} \)ã€‚</p>
<h3 id="Batch-Normalization-in-Keras"><a href="#Batch-Normalization-in-Keras" class="headerlink" title="Batch Normalization in Keras"></a>Batch Normalization in Keras</h3><p>Use the Keras code provided here as a baseline for showing how batch normalizations can improve the accuracy by a large margin. The baseline code does not use batch normalization and the following changes were made to it to add on batch normalization.</p>
<ul>
<li><p>Import the BatchNormalization modules from keras.layers</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import BatchNormalization</span><br></pre></td></tr></table></figure>
</li>
<li><p>Batch Normalization calls were added in keras after the Conv2D or Dense function calls, but before the following Activation function calls. Here are some examples.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model.add(Conv2D(32, (3, 3)))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(Activation(&apos;relu&apos;))</span><br><span class="line"></span><br><span class="line">model.add(Dense(32, (3, 3)))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(Activation(&apos;relu&apos;))</span><br></pre></td></tr></table></figure>
</li>
<li><p>Dropout calls were removed. Batch Normalization itself has some regularization effect. So usually dropout can be reduced. In our case removing it altogether also did not reduce accuracy.</p>
</li>
<li>Learning rate of the optimizer was increased 10 times from 0.0001 to 0.001. RMSprop optimizer was used as in the baseline code, but this would be true for other optimizers too.</li>
<li>We used the same numpy seed to both the baseline and batch normalization versions in order to make a fair comparison between the two output versions. Since most of the deep learning algorithms are stochastic, their outputs are not exactly same in different runs, but the batch normalization version outperformed the baseline version by large margins in all the runs, with the same numpy seed as well as without giving the same numpy seed.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from numpy.random import seed</span><br><span class="line">seed(7)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="/2018/07/03/Amazing-Batch-Normalization/ModelAccuracy.png" alt="ModelAccuracy"><br>Both the versions have roughly the same run time needed for each epoch (24-25 s), but as we see in the above plot, the accuracy attains a much higher value much faster if we use the batch normalization. The keras team reports an accuracy of  79% after 50 epochs, as seen in their Github code. Our run of the same code gave a maximum accuracy of 77%, which seems fair, given it is in the same range as theirs and the stochastic nature of the keras runs. But with batch normalization, it increases to 87%. All these runs were done on a GeForce GTX 1080.</p>
<p><img src="/2018/07/03/Amazing-Batch-Normalization/val_loss.png" alt="val_loss"><br>Above is the plot of the validation vs training losses for both the versions. As we can see the validation loss reduces substantially with batch normalization.</p>
<p><a href="https://github.com/spmallick/learnopencv/tree/master/BatchNormalization" target="_blank" rel="noopener">ä»£ç é“¾æ¥</a></p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/DL/" rel="tag"># DL</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/07/02/Softmax-cross-entropy-æ¨å¯¼åŠæ±‚å¯¼/" rel="prev" title="Softmax Cross Entropy æ¨å¯¼åŠæ±‚å¯¼">
      <i class="fa fa-chevron-left"></i> Softmax Cross Entropy æ¨å¯¼åŠæ±‚å¯¼
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/07/09/Faster-R-CNN-s-RPN/" rel="next" title="Faster R-CNN's RPN">
      Faster R-CNN's RPN <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-5"><a class="nav-link" href="#Notations"><span class="nav-number">1.</span> <span class="nav-text">Notations</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–"><span class="nav-number"></span> <span class="nav-text">å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Can-we-normalize-z-2-so-we-can-train-w-3-and-b-3-more-efficiently-and-faster"><span class="nav-number"></span> <span class="nav-text">Can we normalize \( z^{[2]} \) so we can train \(w^{[3]}\) and \( b^{[3]} \) more efficiently and faster ?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Why-does-Batch-Norm-work"><span class="nav-number"></span> <span class="nav-text">Why does Batch Norm work ?</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#learning-on-shifting-input-distribution"><span class="nav-number">1.</span> <span class="nav-text">learning on shifting input distribution</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#covariate-shift-æ˜¯å¦‚ä½•å½±å“ç¥ç»ç½‘ç»œçš„"><span class="nav-number">2.</span> <span class="nav-text">covariate shift æ˜¯å¦‚ä½•å½±å“ç¥ç»ç½‘ç»œçš„?</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Other-benefits-of-Batch-Normalization"><span class="nav-number"></span> <span class="nav-text">Other benefits of Batch Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Batch-Norm-as-regularization"><span class="nav-number">1.</span> <span class="nav-text">Batch Norm as regularization</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Higher-learning-rate"><span class="nav-number">2.</span> <span class="nav-text">Higher learning rate</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Batch-Norm-at-test-time"><span class="nav-number"></span> <span class="nav-text">Batch Norm at test time</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Batch-Normalization-in-Keras"><span class="nav-number"></span> <span class="nav-text">Batch Normalization in Keras</span></a></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Fengcun Li"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Fengcun Li</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">111</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Fengcun Li</span>
</div>
  <div class="powered-by">ç”± <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> å¼ºåŠ›é©±åŠ¨
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  











<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
