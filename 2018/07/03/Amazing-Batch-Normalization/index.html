<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="这篇博文主要总结了 batch normalization 的公式、为什么有效的直观解释。在博文的最后，验证了通过使用 batch normalization 可以在更少的 epoch 下获得精度上的显著提升。">
<meta name="keywords" content="DL">
<meta property="og:type" content="article">
<meta property="og:title" content="Amazing Batch Normalization">
<meta property="og:url" content="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/index.html">
<meta property="og:site_name" content="The next stop - Antarctica">
<meta property="og:description" content="这篇博文主要总结了 batch normalization 的公式、为什么有效的直观解释。在博文的最后，验证了通过使用 batch normalization 可以在更少的 epoch 下获得精度上的显著提升。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/BN.png">
<meta property="og:image" content="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/covariate_shift.png">
<meta property="og:image" content="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/why_BN_works.png">
<meta property="og:image" content="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/ModelAccuracy.png">
<meta property="og:image" content="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/val_loss.png">
<meta property="og:updated_time" content="2018-09-03T14:06:25.922Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Amazing Batch Normalization">
<meta name="twitter:description" content="这篇博文主要总结了 batch normalization 的公式、为什么有效的直观解释。在博文的最后，验证了通过使用 batch normalization 可以在更少的 epoch 下获得精度上的显著提升。">
<meta name="twitter:image" content="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/BN.png">

<link rel="canonical" href="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Amazing Batch Normalization | The next stop - Antarctica</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">The next stop - Antarctica</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">To see behind walls. To draw closer.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/03/Amazing-Batch-Normalization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Fengcun Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="The next stop - Antarctica">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Amazing Batch Normalization
        </h1>

        <div class="post-meta">
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2018-09-03 22:06:25" itemprop="dateModified" datetime="2018-09-03T22:06:25+08:00">2018-09-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>这篇博文主要总结了 batch normalization 的公式、为什么有效的直观解释。<br>在博文的最后，验证了通过使用 batch normalization 可以在更少的 epoch 下获得精度上的显著提升。<br><a id="more"></a></p>
<blockquote>
<p>Batch Normalization was first introduced by two researchers at Google, Sergey Ioffe and Christian Szegedy in their paper “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift” in 2015. The authors showed that batch normalization improved the top result of ImageNet (2014) by a significant margin using only 7% of the training steps. Today, Batch Normalization is used in almost all CNN architectures.</p>
</blockquote>
<p>对输入特征做归一化能够帮助神经网络的训练，Batch Normalization 就是在隐藏层上做归一化。但是输入层和隐藏层上的归一化还是有所不同的，对隐藏层 Batch Norm 之后并不要求一定是零均值和单位方差，而是通过 learnable parameters \( \gamma \) 和 \( \beta \) 使得隐藏层具有可控的、修正的均值和方差，这个均值和方差可以是 0 和 1，也可以是由 \( \beta \) 和 \( \gamma \) 控制的其他值。<br>Batch Norm 可以让神经网络的训练对于超参数的选择不那么敏感，也让很深的神经网络的训练变的更加容易。</p>
<h5 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h5><p>\(a_{n}^{[l](m)}\) 是 第 \( l \) 层上的第 \( n \) 个神经元在第 \( m \) 个样本上的激活值。</p>
<h3 id="对输入进行归一化"><a href="#对输入进行归一化" class="headerlink" title="对输入进行归一化"></a>对输入进行归一化</h3><p>\[<br>\mu = \frac{1}{m} \sum_{i}X^{(i)} \\<br>X = X - \mu \\<br>\sigma^2 = \frac{1}{m} \sum_{i} (X^{(i)})^2 \\<br>X = \frac{X}{\sigma}<br>\]</p>
<h3 id="Can-we-normalize-z-2-so-we-can-train-w-3-and-b-3-more-efficiently-and-faster"><a href="#Can-we-normalize-z-2-so-we-can-train-w-3-and-b-3-more-efficiently-and-faster" class="headerlink" title="Can we normalize \( z^{[2]} \) so we can train \(w^{[3]}\) and \( b^{[3]} \) more efficiently and faster ?"></a>Can we normalize \( z^{[2]} \) so we can train \(w^{[3]}\) and \( b^{[3]} \) more efficiently and faster ?</h3><p>Do normalization before activation.<br>Given some intermediate values in NN \( z^{[l](i)} \)，shape 是 \( (n^{[l]}, 1) \)，\( n^{[l]} \) 是 layer \( l \) 的神经元个数，简写 \( z^{[l](i)} \) 为 \(z^{(i)}\)。<br><img src="/2018/07/03/Amazing-Batch-Normalization/BN.png" alt="BN"><br>加偏置会被减均值所抵消，因此就没有加偏置的必要了。<br>\[<br>z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}  \longrightarrow z^{[l]} = W^{[l]}a^{[l-1]}<br>\]<br>parameters for layer \( l \): </p>
<ul>
<li>\( W^{[l]} \), \( \beta^{[l]} \), \( \gamma^{[l]} \).</li>
<li>\( \beta^{[l]} \), \( \gamma^{[l]} \) 的 shape 均为 \( (n^{[l]}, 1) \)。</li>
</ul>
<p>在神经网络的前向传播中 replace \( z^{[l]} \) with \( \tilde{z}^{[l]} \).</p>
<h3 id="Why-does-Batch-Norm-work"><a href="#Why-does-Batch-Norm-work" class="headerlink" title="Why does Batch Norm work ?"></a>Why does Batch Norm work ?</h3><p>在深度学习模型的训练中，与让输入的不同特征具有不同的尺度相比较，让输入的不同特征在相同的 range 内可以极大的加速训练过程。</p>
<h5 id="learning-on-shifting-input-distribution"><a href="#learning-on-shifting-input-distribution" class="headerlink" title="learning on shifting input distribution"></a>learning on shifting input distribution</h5><p><img src="/2018/07/03/Amazing-Batch-Normalization/covariate_shift.png" alt="covariate_shift"><br>在上图中左侧数据集上训练得到的神经网络模型很难在右侧的数据集上得到好的结果，这是因为数据集的分布发生了改变。<br>当数据分布随着某种东西在变（在上面的例子中就是数据分布随着样本集合的不同而变化），我们必须重新训练模型，这就叫做 covariate shift。<br>\( X \longrightarrow y \)，if the distribution of \( X \) changes，we must retrain the model.</p>
<h5 id="covariate-shift-是如何影响神经网络的"><a href="#covariate-shift-是如何影响神经网络的" class="headerlink" title="covariate shift 是如何影响神经网络的?"></a>covariate shift 是如何影响神经网络的?</h5><p><img src="/2018/07/03/Amazing-Batch-Normalization/why_BN_works.png" alt="why_BN_works"><br>第二层激活值的分布随着训练过程会发生变化，这使得第三层的模型参数需要重新训练。</p>
<blockquote>
<p>In a neural network, each hidden unit’s input distribution changes every time there is a parameter update in the previous layer. This is called <strong>internal covariate shift</strong>. This makes training slow and requires a very small learning rate and a good parameter initialization. This problem can be solved by normalizing the layer’s inputs over a mini-batch.<br>假设在一定的 \( [a_{1}^{[2]}, a_{2}^{[2]}, a_{3}^{[2]}, a_{4}^{[2]}] \) 的输入上，\(<br>W^{[3]}, b^{[3]}, W^{[4]}, b^{[4]}, W^{[5]}, b^{[5]} \) 很好地完成了任务（得到了正确的标签 \( \hat{y} \)），但是当 \(<br>W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]} \) 改变了，\( [a_{1}^{[2]}, a_{2}^{[2]}, a_{3}^{[2]}, a_{4}^{[2]}] \) 也会发生变化，因此从第三层的角度看，第二层的输出的分布一直在变，因此会受到 covariate shift 的影响使得之前迭代步骤训练得到的 \(<br>W^{[3]}, b^{[3]}, W^{[4]}, b^{[4]}, W^{[5]}, b^{[5]} \) 不再有效。<br>Batch Normalization 可以减少隐藏单元值的分布的不稳定性。<br>\( z^{[2]} \) 可以发生变化，实际上随着神经网络更新前几层的参数，它们确实会发生变化，但是 BN 保证它们的均值和方差是稳定的、可控的，由 \( \beta^{[2]} \) 和 \( \gamma^{[2]} \) 控制。<br>BN 限制了先前层参数更新对第三层的输入的分布的影响，使得数据分布更加稳定，让神经网络后继层在更加稳固的基础上进行学习，削弱了各个层次参数学习过程的耦合，让每一层参数的学习更加独立，有效提高整个神经网络的训练效率。</p>
</blockquote>
<h3 id="Other-benefits-of-Batch-Normalization"><a href="#Other-benefits-of-Batch-Normalization" class="headerlink" title="Other benefits of Batch Normalization"></a>Other benefits of Batch Normalization</h3><h5 id="Batch-Norm-as-regularization"><a href="#Batch-Norm-as-regularization" class="headerlink" title="Batch Norm as regularization"></a>Batch Norm as regularization</h5><ul>
<li>Each mini-batch is scaled by the mean/variance computed on just that mini-batch.</li>
<li>This adds some noise to the values \( z^{[𝑙]} \) within that minibatch. So similar to dropout, it adds some noise to each hidden layer’s activations. </li>
<li>This has a slight regularization effect. </li>
</ul>
<h5 id="Higher-learning-rate"><a href="#Higher-learning-rate" class="headerlink" title="Higher learning rate"></a>Higher learning rate</h5><p>If we use a high learning rate in a traditional neural network, then the gradients could explode or vanish. Large learning rates can scale the parameters which could amplify the gradients, thus leading to an explosion. But if we do batch normalization, small changes in parameter to one layer do not get propagated to other layers. This makes it possible to use higher learning rates for the optimizers, which otherwise would not have been possible. It also makes gradient propagation in the network more stable.</p>
<h3 id="Batch-Norm-at-test-time"><a href="#Batch-Norm-at-test-time" class="headerlink" title="Batch Norm at test time"></a>Batch Norm at test time</h3><p>在推理/测试的时候，使用 across mini-batches 的指数加权滑动平均估计 \( \mu \)，\( \sigma^{2} \)。</p>
<h3 id="Batch-Normalization-in-Keras"><a href="#Batch-Normalization-in-Keras" class="headerlink" title="Batch Normalization in Keras"></a>Batch Normalization in Keras</h3><p>Use the Keras code provided here as a baseline for showing how batch normalizations can improve the accuracy by a large margin. The baseline code does not use batch normalization and the following changes were made to it to add on batch normalization.</p>
<ul>
<li><p>Import the BatchNormalization modules from keras.layers</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import BatchNormalization</span><br></pre></td></tr></table></figure>
</li>
<li><p>Batch Normalization calls were added in keras after the Conv2D or Dense function calls, but before the following Activation function calls. Here are some examples.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model.add(Conv2D(32, (3, 3)))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(Activation(&apos;relu&apos;))</span><br><span class="line"></span><br><span class="line">model.add(Dense(32, (3, 3)))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(Activation(&apos;relu&apos;))</span><br></pre></td></tr></table></figure>
</li>
<li><p>Dropout calls were removed. Batch Normalization itself has some regularization effect. So usually dropout can be reduced. In our case removing it altogether also did not reduce accuracy.</p>
</li>
<li>Learning rate of the optimizer was increased 10 times from 0.0001 to 0.001. RMSprop optimizer was used as in the baseline code, but this would be true for other optimizers too.</li>
<li>We used the same numpy seed to both the baseline and batch normalization versions in order to make a fair comparison between the two output versions. Since most of the deep learning algorithms are stochastic, their outputs are not exactly same in different runs, but the batch normalization version outperformed the baseline version by large margins in all the runs, with the same numpy seed as well as without giving the same numpy seed.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from numpy.random import seed</span><br><span class="line">seed(7)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="/2018/07/03/Amazing-Batch-Normalization/ModelAccuracy.png" alt="ModelAccuracy"><br>Both the versions have roughly the same run time needed for each epoch (24-25 s), but as we see in the above plot, the accuracy attains a much higher value much faster if we use the batch normalization. The keras team reports an accuracy of  79% after 50 epochs, as seen in their Github code. Our run of the same code gave a maximum accuracy of 77%, which seems fair, given it is in the same range as theirs and the stochastic nature of the keras runs. But with batch normalization, it increases to 87%. All these runs were done on a GeForce GTX 1080.</p>
<p><img src="/2018/07/03/Amazing-Batch-Normalization/val_loss.png" alt="val_loss"><br>Above is the plot of the validation vs training losses for both the versions. As we can see the validation loss reduces substantially with batch normalization.</p>
<p><a href="https://github.com/spmallick/learnopencv/tree/master/BatchNormalization" target="_blank" rel="noopener">代码链接</a></p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/DL/" rel="tag"># DL</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/07/02/Softmax-cross-entropy-推导及求导/" rel="prev" title="Softmax Cross Entropy 推导及求导">
      <i class="fa fa-chevron-left"></i> Softmax Cross Entropy 推导及求导
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/07/09/Faster-R-CNN-s-RPN/" rel="next" title="Faster R-CNN's RPN">
      Faster R-CNN's RPN <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-5"><a class="nav-link" href="#Notations"><span class="nav-number">1.</span> <span class="nav-text">Notations</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对输入进行归一化"><span class="nav-number"></span> <span class="nav-text">对输入进行归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Can-we-normalize-z-2-so-we-can-train-w-3-and-b-3-more-efficiently-and-faster"><span class="nav-number"></span> <span class="nav-text">Can we normalize \( z^{[2]} \) so we can train \(w^{[3]}\) and \( b^{[3]} \) more efficiently and faster ?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Why-does-Batch-Norm-work"><span class="nav-number"></span> <span class="nav-text">Why does Batch Norm work ?</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#learning-on-shifting-input-distribution"><span class="nav-number">1.</span> <span class="nav-text">learning on shifting input distribution</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#covariate-shift-是如何影响神经网络的"><span class="nav-number">2.</span> <span class="nav-text">covariate shift 是如何影响神经网络的?</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Other-benefits-of-Batch-Normalization"><span class="nav-number"></span> <span class="nav-text">Other benefits of Batch Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Batch-Norm-as-regularization"><span class="nav-number">1.</span> <span class="nav-text">Batch Norm as regularization</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Higher-learning-rate"><span class="nav-number">2.</span> <span class="nav-text">Higher learning rate</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Batch-Norm-at-test-time"><span class="nav-number"></span> <span class="nav-text">Batch Norm at test time</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Batch-Normalization-in-Keras"><span class="nav-number"></span> <span class="nav-text">Batch Normalization in Keras</span></a></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Fengcun Li"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Fengcun Li</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">111</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Fengcun Li</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  











<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
