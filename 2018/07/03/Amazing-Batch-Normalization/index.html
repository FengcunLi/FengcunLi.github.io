<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="è¿™ç¯‡åšæ–‡ä¸»è¦æ€»ç»“äº† batch normalization çš„å…¬å¼ã€ä¸ºä»€ä¹ˆæœ‰æ•ˆçš„ç›´è§‚è§£é‡Šã€‚åœ¨åšæ–‡çš„æœ€åï¼ŒéªŒè¯äº†é€šè¿‡ä½¿ç”¨ batch normalization å¯ä»¥åœ¨æ›´å°‘çš„ epoch ä¸‹è·å¾—ç²¾åº¦ä¸Šçš„æ˜¾è‘—æå‡ã€‚">
    

    <!--Author-->
    
        <meta name="author" content="Robert Lexis">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Amazing Batch Normalization"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="This is Robert Lexis."/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>Amazing Batch Normalization - This is Robert Lexis.</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
    </div>
</header>

        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/2018/07/03/Amazing-Batch-Normalization/">
                Amazing Batch Normalization
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-07-03</span>
            
            
            
        </div>
    </div>

    <div class="content">

        <!-- Gallery -->
        

        <!-- Post Content -->
        <p>è¿™ç¯‡åšæ–‡ä¸»è¦æ€»ç»“äº† batch normalization çš„å…¬å¼ã€ä¸ºä»€ä¹ˆæœ‰æ•ˆçš„ç›´è§‚è§£é‡Šã€‚<br>åœ¨åšæ–‡çš„æœ€åï¼ŒéªŒè¯äº†é€šè¿‡ä½¿ç”¨ batch normalization å¯ä»¥åœ¨æ›´å°‘çš„ epoch ä¸‹è·å¾—ç²¾åº¦ä¸Šçš„æ˜¾è‘—æå‡ã€‚<br><a id="more"></a></p>
<blockquote>
<p>Batch Normalization was first introduced by two researchers at Google, Sergey Ioffe and Christian Szegedy in their paper â€œBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shiftâ€ in 2015. The authors showed that batch normalization improved the top result of ImageNet (2014) by a significant margin using only 7% of the training steps. Today, Batch Normalization is used in almost all CNN architectures.</p>
</blockquote>
<p>å¯¹è¾“å…¥ç‰¹å¾åšå½’ä¸€åŒ–èƒ½å¤Ÿå¸®åŠ©ç¥ç»ç½‘ç»œçš„è®­ç»ƒï¼ŒBatch Normalization å°±æ˜¯åœ¨éšè—å±‚ä¸Šåšå½’ä¸€åŒ–ã€‚ä½†æ˜¯è¾“å…¥å±‚å’Œéšè—å±‚ä¸Šçš„å½’ä¸€åŒ–è¿˜æ˜¯æœ‰æ‰€ä¸åŒçš„ï¼Œå¯¹éšè—å±‚ Batch Norm ä¹‹åå¹¶ä¸è¦æ±‚ä¸€å®šæ˜¯é›¶å‡å€¼å’Œå•ä½æ–¹å·®ï¼Œè€Œæ˜¯é€šè¿‡ learnable parameters \( \gamma \) å’Œ \( \beta \) ä½¿å¾—éšè—å±‚å…·æœ‰å¯æ§çš„ã€ä¿®æ­£çš„å‡å€¼å’Œæ–¹å·®ï¼Œè¿™ä¸ªå‡å€¼å’Œæ–¹å·®å¯ä»¥æ˜¯ 0 å’Œ 1ï¼Œä¹Ÿå¯ä»¥æ˜¯ç”± \( \beta \) å’Œ \( \gamma \) æ§åˆ¶çš„å…¶ä»–å€¼ã€‚<br>Batch Norm å¯ä»¥è®©ç¥ç»ç½‘ç»œçš„è®­ç»ƒå¯¹äºè¶…å‚æ•°çš„é€‰æ‹©ä¸é‚£ä¹ˆæ•æ„Ÿï¼Œä¹Ÿè®©å¾ˆæ·±çš„ç¥ç»ç½‘ç»œçš„è®­ç»ƒå˜çš„æ›´åŠ å®¹æ˜“ã€‚</p>
<h5 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h5><p>\(a_{n}^{[l](m)}\) æ˜¯ ç¬¬ \( l \) å±‚ä¸Šçš„ç¬¬ \( n \) ä¸ªç¥ç»å…ƒåœ¨ç¬¬ \( m \) ä¸ªæ ·æœ¬ä¸Šçš„æ¿€æ´»å€¼ã€‚</p>
<h3 id="å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–"><a href="#å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–" class="headerlink" title="å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–"></a>å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–</h3><p>\[<br>\mu = \frac{1}{m} \sum_{i}X^{(i)} \\\\<br>X = X - \mu \\\\<br>\sigma^2 = \frac{1}{m} \sum_{i} (X^{(i)})^2 \\\\<br>X = \frac{X}{\sigma}<br>\]</p>
<h3 id="Can-we-normalize-z-2-so-we-can-train-w-3-and-b-3-more-efficiently-and-faster"><a href="#Can-we-normalize-z-2-so-we-can-train-w-3-and-b-3-more-efficiently-and-faster" class="headerlink" title="Can we normalize \( z^{[2]} \) so we can train \(w^{[3]}\) and \( b^{[3]} \) more efficiently and faster ?"></a>Can we normalize \( z^{[2]} \) so we can train \(w^{[3]}\) and \( b^{[3]} \) more efficiently and faster ?</h3><p>Do normalization before activation.<br>Given some intermediate values in NN \( z^{[l](i)} \)ï¼Œshape æ˜¯ \( (n^{[l]}, 1) \)ï¼Œ\( n^{[l]} \) æ˜¯ layer \( l \) çš„ç¥ç»å…ƒä¸ªæ•°ï¼Œç®€å†™ \( z^{[l](i)} \) ä¸º \(z^{(i)}\)ã€‚<br><img src="/2018/07/03/Amazing-Batch-Normalization/BN.png" alt="BN"><br>åŠ åç½®ä¼šè¢«å‡å‡å€¼æ‰€æŠµæ¶ˆï¼Œå› æ­¤å°±æ²¡æœ‰åŠ åç½®çš„å¿…è¦äº†ã€‚<br>\[<br>z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}  \longrightarrow z^{[l]} = W^{[l]}a^{[l-1]}<br>\]<br>parameters for layer \( l \): </p>
<ul>
<li>\( W^{[l]} \), \( \beta^{[l]} \), \( \gamma^{[l]} \).</li>
<li>\( \beta^{[l]} \), \( \gamma^{[l]} \) çš„ shape å‡ä¸º \( (n^{[l]}, 1) \)ã€‚</li>
</ul>
<p>åœ¨ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­ä¸­ replace \( z^{[l]} \) with \( \tilde{z}^{[l]} \).</p>
<h3 id="Why-does-Batch-Norm-work"><a href="#Why-does-Batch-Norm-work" class="headerlink" title="Why does Batch Norm work ?"></a>Why does Batch Norm work ?</h3><p>åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒä¸­ï¼Œä¸è®©è¾“å…¥çš„ä¸åŒç‰¹å¾å…·æœ‰ä¸åŒçš„å°ºåº¦ç›¸æ¯”è¾ƒï¼Œè®©è¾“å…¥çš„ä¸åŒç‰¹å¾åœ¨ç›¸åŒçš„ range å†…å¯ä»¥æå¤§çš„åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚</p>
<h5 id="learning-on-shifting-input-distribution"><a href="#learning-on-shifting-input-distribution" class="headerlink" title="learning on shifting input distribution"></a>learning on shifting input distribution</h5><p><img src="/2018/07/03/Amazing-Batch-Normalization/covariate_shift.png" alt="covariate_shift"><br>åœ¨ä¸Šå›¾ä¸­å·¦ä¾§æ•°æ®é›†ä¸Šè®­ç»ƒå¾—åˆ°çš„ç¥ç»ç½‘ç»œæ¨¡å‹å¾ˆéš¾åœ¨å³ä¾§çš„æ•°æ®é›†ä¸Šå¾—åˆ°å¥½çš„ç»“æœï¼Œè¿™æ˜¯å› ä¸ºæ•°æ®é›†çš„åˆ†å¸ƒå‘ç”Ÿäº†æ”¹å˜ã€‚<br>å½“æ•°æ®åˆ†å¸ƒéšç€æŸç§ä¸œè¥¿åœ¨å˜ï¼ˆåœ¨ä¸Šé¢çš„ä¾‹å­ä¸­å°±æ˜¯æ•°æ®åˆ†å¸ƒéšç€æ ·æœ¬é›†åˆçš„ä¸åŒè€Œå˜åŒ–ï¼‰ï¼Œæˆ‘ä»¬å¿…é¡»é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œè¿™å°±å«åš covariate shiftã€‚<br>\( X \longrightarrow y \)ï¼Œif the distribution of \( X \) changesï¼Œwe must retrain the model.</p>
<h5 id="covariate-shift-æ˜¯å¦‚ä½•å½±å“ç¥ç»ç½‘ç»œçš„"><a href="#covariate-shift-æ˜¯å¦‚ä½•å½±å“ç¥ç»ç½‘ç»œçš„" class="headerlink" title="covariate shift æ˜¯å¦‚ä½•å½±å“ç¥ç»ç½‘ç»œçš„?"></a>covariate shift æ˜¯å¦‚ä½•å½±å“ç¥ç»ç½‘ç»œçš„?</h5><p><img src="/2018/07/03/Amazing-Batch-Normalization/why_BN_works.png" alt="why_BN_works"><br>ç¬¬äºŒå±‚æ¿€æ´»å€¼çš„åˆ†å¸ƒéšç€è®­ç»ƒè¿‡ç¨‹ä¼šå‘ç”Ÿå˜åŒ–ï¼Œè¿™ä½¿å¾—ç¬¬ä¸‰å±‚çš„æ¨¡å‹å‚æ•°éœ€è¦é‡æ–°è®­ç»ƒã€‚</p>
<blockquote>
<p>In a neural network, each hidden unitâ€™s input distribution changes every time there is a parameter update in the previous layer. This is called <strong>internal covariate shift</strong>. This makes training slow and requires a very small learning rate and a good parameter initialization. This problem can be solved by normalizing the layerâ€™s inputs over a mini-batch.<br>å‡è®¾åœ¨ä¸€å®šçš„ \( [a_{1}^{[2]}, a_{2}^{[2]}, a_{3}^{[2]}, a_{4}^{[2]}] \) çš„è¾“å…¥ä¸Šï¼Œ\(<br>W^{[3]}, b^{[3]}, W^{[4]}, b^{[4]}, W^{[5]}, b^{[5]} \) å¾ˆå¥½åœ°å®Œæˆäº†ä»»åŠ¡ï¼ˆå¾—åˆ°äº†æ­£ç¡®çš„æ ‡ç­¾ \( \hat{y} \)ï¼‰ï¼Œä½†æ˜¯å½“ \(<br>W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]} \) æ”¹å˜äº†ï¼Œ\( [a_{1}^{[2]}, a_{2}^{[2]}, a_{3}^{[2]}, a_{4}^{[2]}] \) ä¹Ÿä¼šå‘ç”Ÿå˜åŒ–ï¼Œå› æ­¤ä»ç¬¬ä¸‰å±‚çš„è§’åº¦çœ‹ï¼Œç¬¬äºŒå±‚çš„è¾“å‡ºçš„åˆ†å¸ƒä¸€ç›´åœ¨å˜ï¼Œå› æ­¤ä¼šå—åˆ° covariate shift çš„å½±å“ä½¿å¾—ä¹‹å‰è¿­ä»£æ­¥éª¤è®­ç»ƒå¾—åˆ°çš„ \(<br>W^{[3]}, b^{[3]}, W^{[4]}, b^{[4]}, W^{[5]}, b^{[5]} \) ä¸å†æœ‰æ•ˆã€‚<br>Batch Normalization å¯ä»¥å‡å°‘éšè—å•å…ƒå€¼çš„åˆ†å¸ƒçš„ä¸ç¨³å®šæ€§ã€‚<br>\( z^{[2]} \) å¯ä»¥å‘ç”Ÿå˜åŒ–ï¼Œå®é™…ä¸Šéšç€ç¥ç»ç½‘ç»œæ›´æ–°å‰å‡ å±‚çš„å‚æ•°ï¼Œå®ƒä»¬ç¡®å®ä¼šå‘ç”Ÿå˜åŒ–ï¼Œä½†æ˜¯ BN ä¿è¯å®ƒä»¬çš„å‡å€¼å’Œæ–¹å·®æ˜¯ç¨³å®šçš„ã€å¯æ§çš„ï¼Œç”± \( \beta^{[2]} \) å’Œ \( \gamma^{[2]} \) æ§åˆ¶ã€‚<br>BN é™åˆ¶äº†å…ˆå‰å±‚å‚æ•°æ›´æ–°å¯¹ç¬¬ä¸‰å±‚çš„è¾“å…¥çš„åˆ†å¸ƒçš„å½±å“ï¼Œä½¿å¾—æ•°æ®åˆ†å¸ƒæ›´åŠ ç¨³å®šï¼Œè®©ç¥ç»ç½‘ç»œåç»§å±‚åœ¨æ›´åŠ ç¨³å›ºçš„åŸºç¡€ä¸Šè¿›è¡Œå­¦ä¹ ï¼Œå‰Šå¼±äº†å„ä¸ªå±‚æ¬¡å‚æ•°å­¦ä¹ è¿‡ç¨‹çš„è€¦åˆï¼Œè®©æ¯ä¸€å±‚å‚æ•°çš„å­¦ä¹ æ›´åŠ ç‹¬ç«‹ï¼Œæœ‰æ•ˆæé«˜æ•´ä¸ªç¥ç»ç½‘ç»œçš„è®­ç»ƒæ•ˆç‡ã€‚</p>
</blockquote>
<h3 id="Other-benefits-of-Batch-Normalization"><a href="#Other-benefits-of-Batch-Normalization" class="headerlink" title="Other benefits of Batch Normalization"></a>Other benefits of Batch Normalization</h3><h5 id="Batch-Norm-as-regularization"><a href="#Batch-Norm-as-regularization" class="headerlink" title="Batch Norm as regularization"></a>Batch Norm as regularization</h5><ul>
<li>Each mini-batch is scaled by the mean/variance computed on just that mini-batch.</li>
<li>This adds some noise to the values \( z^{[ğ‘™]} \) within that minibatch. So similar to dropout, it adds some noise to each hidden layerâ€™s activations. </li>
<li>This has a slight regularization effect. </li>
</ul>
<h5 id="Higher-learning-rate"><a href="#Higher-learning-rate" class="headerlink" title="Higher learning rate"></a>Higher learning rate</h5><p>If we use a high learning rate in a traditional neural network, then the gradients could explode or vanish. Large learning rates can scale the parameters which could amplify the gradients, thus leading to an explosion. But if we do batch normalization, small changes in parameter to one layer do not get propagated to other layers. This makes it possible to use higher learning rates for the optimizers, which otherwise would not have been possible. It also makes gradient propagation in the network more stable.</p>
<h3 id="Batch-Norm-at-test-time"><a href="#Batch-Norm-at-test-time" class="headerlink" title="Batch Norm at test time"></a>Batch Norm at test time</h3><p>åœ¨æ¨ç†/æµ‹è¯•çš„æ—¶å€™ï¼Œä½¿ç”¨ across mini-batches çš„æŒ‡æ•°åŠ æƒæ»‘åŠ¨å¹³å‡ä¼°è®¡ \( \mu \)ï¼Œ\( \sigma^{2} \)ã€‚</p>
<h3 id="Batch-Normalization-in-Keras"><a href="#Batch-Normalization-in-Keras" class="headerlink" title="Batch Normalization in Keras"></a>Batch Normalization in Keras</h3><p>Use the Keras code provided here as a baseline for showing how batch normalizations can improve the accuracy by a large margin. The baseline code does not use batch normalization and the following changes were made to it to add on batch normalization.</p>
<ul>
<li><p>Import the BatchNormalization modules from keras.layers</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">from keras.layers import BatchNormalization</div></pre></td></tr></table></figure>
</li>
<li><p>Batch Normalization calls were added in keras after the Conv2D or Dense function calls, but before the following Activation function calls. Here are some examples.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">model.add(Conv2D(32, (3, 3)))</div><div class="line">model.add(BatchNormalization())</div><div class="line">model.add(Activation(&apos;relu&apos;))</div><div class="line"></div><div class="line">model.add(Dense(32, (3, 3)))</div><div class="line">model.add(BatchNormalization())</div><div class="line">model.add(Activation(&apos;relu&apos;))</div></pre></td></tr></table></figure>
</li>
<li><p>Dropout calls were removed. Batch Normalization itself has some regularization effect. So usually dropout can be reduced. In our case removing it altogether also did not reduce accuracy.</p>
</li>
<li>Learning rate of the optimizer was increased 10 times from 0.0001 to 0.001. RMSprop optimizer was used as in the baseline code, but this would be true for other optimizers too.</li>
<li>We used the same numpy seed to both the baseline and batch normalization versions in order to make a fair comparison between the two output versions. Since most of the deep learning algorithms are stochastic, their outputs are not exactly same in different runs, but the batch normalization version outperformed the baseline version by large margins in all the runs, with the same numpy seed as well as without giving the same numpy seed.<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">from numpy.random import seed</div><div class="line">seed(7)</div></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="/2018/07/03/Amazing-Batch-Normalization/ModelAccuracy.png" alt="ModelAccuracy"><br>Both the versions have roughly the same run time needed for each epoch (24-25 s), but as we see in the above plot, the accuracy attains a much higher value much faster if we use the batch normalization. The keras team reports an accuracy of  79% after 50 epochs, as seen in their Github code. Our run of the same code gave a maximum accuracy of 77%, which seems fair, given it is in the same range as theirs and the stochastic nature of the keras runs. But with batch normalization, it increases to 87%. All these runs were done on a GeForce GTX 1080.</p>
<p><img src="/2018/07/03/Amazing-Batch-Normalization/val_loss.png" alt="val_loss"><br>Above is the plot of the validation vs training losses for both the versions. As we can see the validation loss reduces substantially with batch normalization.</p>
<p><a href="https://github.com/spmallick/learnopencv/tree/master/BatchNormalization" target="_blank" rel="external">ä»£ç é“¾æ¥</a></p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    </div>

    

    
        <div class="post-tags">
            <i class="fa fa-tags" aria-hidden="true"></i>
            <a href="/tags/DL/">#DL</a>
        </div>
    

    <!-- Comments -->
    

</div>
        </section>

    </div>
</div>


</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This is Robert Lexis (FengCun Li). To see the world, things dangerous to come to, to see behind walls, to draw closer, to find each other and to feel. That is the purpose of LIFE.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2018/09/03/Optimizers/">Optimizers</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/31/Train-Faster-R-CNN-s-RPN-as-a-standalone-app-using-TF-Object-Detecion-APIs/">Train Faster R-CNN&#39;s RPN </a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/28/Tensorflow-æ¨¡å‹æµ®ç‚¹æ•°è®¡ç®—é‡å’Œå‚æ•°é‡ç»Ÿè®¡/">TensorFlow æ¨¡å‹æµ®ç‚¹æ•°è®¡ç®—é‡å’Œå‚æ•°é‡ç»Ÿè®¡</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/26/YOLO/">YOLO</a>
            </li>
            
        </ul>
    </div>



            
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/RobertLexis">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:robert_lexis@163.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Robert Lexis Loves Wenny
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



</body>

</html>