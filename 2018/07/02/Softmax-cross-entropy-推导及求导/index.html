<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="主要分析了 binary_crossentropy 和 categorical_crossentropy 的定义， softmax 和 categorical_crossentropy 的求导。
binary_crossentropy适用于每个类别相互独立但互不排斥的情况，常见于单类别任务\( (batch size, 1) \)和多类别中的多标签任务\( (batch size, num classes) \)。
\[\begin{split}    p_{i, j} &amp;amp; = sigmoid(logits_{i, j}) \\\\    &amp;amp; = \frac{1}{1 + e^{-logits_{i, j} } } \\\\    loss_{i, j} &amp;amp; = -[y_{i, j} \times ln p_{i, j} + (1 - y_{i, j}) \times (1 - ln p_{i, j})]\end{split}\]">
    

    <!--Author-->
    
        <meta name="author" content="Robert Lexis">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Softmax cross entropy 推导及求导"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="This is Robert Lexis."/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>Softmax cross entropy 推导及求导 - This is Robert Lexis.</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
    </div>
</header>

        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/2018/07/02/Softmax-cross-entropy-推导及求导/">
                Softmax cross entropy 推导及求导
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-07-02</span>
            
            
            
        </div>
    </div>

    <div class="content">

        <!-- Gallery -->
        

        <!-- Post Content -->
        <p>主要分析了 binary_crossentropy 和 categorical_crossentropy 的定义， softmax 和 categorical_crossentropy 的求导。</p>
<h3 id="binary-crossentropy"><a href="#binary-crossentropy" class="headerlink" title="binary_crossentropy"></a>binary_crossentropy</h3><p>适用于每个类别相互独立但互不排斥的情况，常见于单类别任务\( (batch size, 1) \)和多类别中的多标签任务\( (batch size, num classes) \)。</p>
<p>\[<br>\begin{split}<br>    p_{i, j} &amp; = sigmoid(logits_{i, j}) \\\\<br>    &amp; = \frac{1}{1 + e^{-logits_{i, j} } } \\\\<br>    loss_{i, j} &amp; = -[y_{i, j} \times ln p_{i, j} + (1 - y_{i, j}) \times (1 - ln p_{i, j})]<br>\end{split}<br>\]<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_crossentropy</span><span class="params">(target, output, from_logits=False)</span>:</span></div><div class="line">    <span class="string">"""Binary crossentropy between an output tensor and a target tensor.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    # Arguments</span></div><div class="line"><span class="string">        target: A tensor with the same shape as `output`.</span></div><div class="line"><span class="string">        output: A tensor.</span></div><div class="line"><span class="string">        from_logits: Whether `output` is expected to be a logits tensor.</span></div><div class="line"><span class="string">            By default, we consider that `output`</span></div><div class="line"><span class="string">            encodes a probability distribution.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    # Returns</span></div><div class="line"><span class="string">        A tensor.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="comment"># Note: tf.nn.sigmoid_cross_entropy_with_logits</span></div><div class="line">    <span class="comment"># expects logits, Keras expects probabilities.</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> from_logits:</div><div class="line">        <span class="comment"># transform back to logits</span></div><div class="line">        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)</div><div class="line">        output = tf.clip_by_value(output, _epsilon, <span class="number">1</span> - _epsilon)</div><div class="line">        output = tf.log(output / (<span class="number">1</span> - output))</div><div class="line"></div><div class="line">    <span class="keyword">return</span> tf.nn.sigmoid_cross_entropy_with_logits(labels=target,</div><div class="line">                                                   logits=output)</div></pre></td></tr></table></figure>
<h3 id="categorical-crossentropy"><a href="#categorical-crossentropy" class="headerlink" title="categorical_crossentropy"></a>categorical_crossentropy</h3><p>适用于每个类别相互独立且排斥的情况（onehot），即多类别中的单标签任务\( (batch size, num classes) \)。<br>\[<br>\begin{split}<br>    p_{i, j} &amp; = \frac{e^{logits_{i, j} } }{\sum_{j=0}^{num classes - 1} e^{logits_{i, j} } } \\\\<br>    loss_i &amp; = - \sum_{j=0}^{num  classes - 1} y_{i, j} ln p_{i, j}<br>\end{split}<br>\]<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">categorical_crossentropy</span><span class="params">(target, output, from_logits=False, axis=<span class="number">-1</span>)</span>:</span></div><div class="line">    <span class="string">"""Categorical crossentropy between an output tensor and a target tensor.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    # Arguments</span></div><div class="line"><span class="string">        target: A tensor of the same shape as `output`.</span></div><div class="line"><span class="string">        output: A tensor resulting from a softmax</span></div><div class="line"><span class="string">            (unless `from_logits` is True, in which</span></div><div class="line"><span class="string">            case `output` is expected to be the logits).</span></div><div class="line"><span class="string">        from_logits: Boolean, whether `output` is the</span></div><div class="line"><span class="string">            result of a softmax, or is a tensor of logits.</span></div><div class="line"><span class="string">        axis: Int specifying the channels axis. `axis=-1`</span></div><div class="line"><span class="string">            corresponds to data format `channels_last`,</span></div><div class="line"><span class="string">            and `axis=1` corresponds to data format</span></div><div class="line"><span class="string">            `channels_first`.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    # Returns</span></div><div class="line"><span class="string">        Output tensor.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    # Raises</span></div><div class="line"><span class="string">        ValueError: if `axis` is neither -1 nor one of</span></div><div class="line"><span class="string">            the axes of `output`.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    output_dimensions = list(range(len(output.get_shape())))</div><div class="line">    <span class="keyword">if</span> axis != <span class="number">-1</span> <span class="keyword">and</span> axis <span class="keyword">not</span> <span class="keyword">in</span> output_dimensions:</div><div class="line">        <span class="keyword">raise</span> ValueError(</div><div class="line">            <span class="string">'&#123;&#125;&#123;&#125;&#123;&#125;'</span>.format(</div><div class="line">                <span class="string">'Unexpected channels axis &#123;&#125;. '</span>.format(axis),</div><div class="line">                <span class="string">'Expected to be -1 or one of the axes of `output`, '</span>,</div><div class="line">                <span class="string">'which has &#123;&#125; dimensions.'</span>.format(len(output.get_shape()))))</div><div class="line">    <span class="comment"># Note: tf.nn.softmax_cross_entropy_with_logits</span></div><div class="line">    <span class="comment"># expects logits, Keras expects probabilities.</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> from_logits:</div><div class="line">        <span class="comment"># scale preds so that the class probas of each sample sum to 1</span></div><div class="line">        output /= tf.reduce_sum(output, axis, <span class="keyword">True</span>)</div><div class="line">        <span class="comment"># manual computation of crossentropy</span></div><div class="line">        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)</div><div class="line">        output = tf.clip_by_value(output, _epsilon, <span class="number">1.</span> - _epsilon)</div><div class="line">        <span class="keyword">return</span> - tf.reduce_sum(target * tf.log(output), axis)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> tf.nn.softmax_cross_entropy_with_logits(labels=target,</div><div class="line">                                                       logits=output)</div></pre></td></tr></table></figure></p>
<h3 id="weighted-cross-entropy"><a href="#weighted-cross-entropy" class="headerlink" title="weighted_cross_entropy"></a>weighted_cross_entropy</h3><p>\[<br>\begin{split}<br>    p_{i, j} &amp; = sigmoid(logits_{i, j}) \\\\<br>    &amp; = \frac{1}{1 + e^{-logits_{i, j} } } \\\\<br>\end{split}<br>\]</p>
<p>\[<br>loss_{i, j} = -[ pos\_weight \times y_{i, j} \times ln p_{i, j} + (1 - y_{i, j}) \times (1 - ln p_{i, j})]<br>\]<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.nn.weighted_cross_entropy_with_logits(labels,logits, pos_weight, name=<span class="keyword">None</span>)</div></pre></td></tr></table></figure></p>
<h3 id="softmax-求导"><a href="#softmax-求导" class="headerlink" title="softmax 求导"></a>softmax 求导</h3><p>\[<br>\begin{split}<br>    y_i &amp; = \frac{e^{x_i} }{\sum_{j=0}^{m-1} e^{x_j} } \\\\<br>    \frac{\partial y_i}{\partial x_k} &amp; = \frac{\partial \frac{e^{x_i} }{\sum_{j=0}^{m-1} e^{x_j} } }{\partial x_k} \\\\<br>    &amp; = \frac{\frac{\partial e^{x_i} }{\partial x_k} \times \sum - e^{x_i} \times \frac{\partial \sum}{\partial x_k} }{ {\sum}^2 } \\\\<br>    &amp; = \frac{\frac{\partial e^{x_i} }{\partial x_k} \times \sum - e^{x_i} \times e^{x_k} }{ {\sum}^2 } \\\\<br>    &amp; = \begin{cases}<br>        \frac{e^{x_k} \times \sum - e^{x_i} \times e^{x_k} }{ {\sum}^2 }, \text{ if } i = k \\\\<br>        \frac{ - e^{x_i} \times e^{x_k} }{ {\sum}^2 }, \text{ if } i \neq k<br>    \end{cases} \\\\<br>    &amp; = \begin{cases}<br>    y_k(1 - y_i), \text{ if } i = k\\\\<br>    -y_k y_i, \text{ if } i \neq k<br>    \end{cases}<br>\end{split}<br>\]</p>
<p>\[<br>    \mathbf{y} = softmax(\mathbf{x})<br>\]</p>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/softmax/softmax.png" alt="softmax"></p>
<p>雅可比矩阵<br>\[<br>Jacobian_{\mathbf{y} }(\mathbf{x}) =<br>\left[<br> \begin{matrix}<br>   \frac{\partial y_0}{\partial x_0} &amp; \frac{\partial y_0}{\partial x_1} &amp; \dots &amp; \frac{\partial y_0}{\partial x_{m-1} } \\\\<br>   \vdots &amp;  \vdots &amp; \ddots &amp; \vdots \\\\<br>   \frac{\partial y_{m-1} }{\partial x_0} &amp; \frac{\partial y_{m-1} }{\partial x_1} &amp; \dots &amp; \frac{\partial y_{m-1}}{\partial x_{m-1} }<br>  \end{matrix}<br>  \right]<br>\]</p>
<h3 id="categorical-crossentropy-求导"><a href="#categorical-crossentropy-求导" class="headerlink" title="categorical_crossentropy 求导"></a>categorical_crossentropy 求导</h3><p>\[<br>\begin{split}<br>    \frac{\partial loss}{\partial logits_k} &amp; = \frac{\partial {- \sum_{j=0}^{num  classes - 1} y_{j} ln p_{j} } }{\partial logits_k} \\\\<br>    &amp; = - \sum_{j=0}^{num classes - 1} y_{j}  \frac{\partial ln p_{j} }{\partial logits_k} \\\\<br>    &amp; = - \sum_{j=0}^{num classes - 1} y_{j}  \frac{1}{p_{j} } \frac{\partial p_{j} }{\partial logits_k} \\\\<br>    &amp; = - \sum_{j=0}^{num classes - 1} y_{j}  \frac{1}{p_{j} } \begin{cases}<br>                                                            p_k (1 - p_j), \text{ if } j = k \\\\<br>                                                            -p_j p_k, \text{ if } j \neq k<br>                                                            \end{cases} \\\\<br>    &amp; = - y_{k}(1 - p_k) - \sum_{j=0, j \neq k}^{num classes - 1} y_{j} (-p_k) \\\\<br>    &amp; = - y_{k} + y_{k} p_k + \sum_{j=0, j \neq k}^{num classes - 1} y_{j} p_k \\\\<br>    &amp; = - y_{k} + p_k \sum_{j=0}^{num classes - 1} y_{j} \\\\<br>    &amp; = p_k - y_{k}<br>\end{split}<br>\]<br>可以看出 categorical_crossentropy 的导数很简洁，即预测概率与真实概率的差。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">tf.nn.softmax_cross_entropy_with_logits(</div><div class="line">    _sentinel=<span class="keyword">None</span>,</div><div class="line">    labels=<span class="keyword">None</span>,</div><div class="line">    logits=<span class="keyword">None</span>,</div><div class="line">    dim=<span class="number">-1</span>,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure></p>
<blockquote>
<p>WARNING: This op expects unscaled logits, since it performs a softmax on logits internally for efficiency. Do not call this op with the output of softmax, as it will produce incorrect results.</p>
<p>Backpropagation will happen only into logits. To calculate a cross entropy loss that allows backpropagation into both logits and labels, see tf.nn.softmax_cross_entropy_with_logits_v2.</p>
</blockquote>
<p>相较于相继调用 <code>softmax</code> 和 <code>cross_entropy</code> （正向传播，反向传播），<code>softmax_cross_entropy_with_logits</code> 的反向传播速度更快，原因就是可以直接使用上面的的推导结果\( \frac{\partial loss}{\partial logits_k} =  p_k - y_{k} \) 简单快速地求得 <code>logits</code> 上的梯度，并在此基础上继续反向传播，而不必对 <code>cross_entropy</code> 和 <code>softmax</code> 依次反向传播之后才得到 <code>logits</code> 上的梯度\( \sum_{j=0}^{m-1} \frac{\partial loss}{\partial p_j} \frac{\partial p_j}{\partial logits_k} \)[upstream gradients local gradients]。将这两个操作合为一个操作，正向传播速度一样，反向传播实现了加速。</p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    </div>

    

    
        <div class="post-tags">
            <i class="fa fa-tags" aria-hidden="true"></i>
            <a href="/tags/DL/">#DL</a>
        </div>
    

    <!-- Comments -->
    

</div>
        </section>

    </div>
</div>


</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This is Robert Lexis (FengCun Li). To see the world, things dangerous to come to, to see behind walls, to draw closer, to find each other and to feel. That is the purpose of LIFE.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2019/05/08/Static-variable-in-inlined-function/">Static variable in inline</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2019/04/23/Iterator-invalidation-rules/">Iterator invalidation rul</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2019/03/18/Emplace-back/">Emplace back</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2019/03/14/Perfect-forward/">Perfect forward</a>
            </li>
            
        </ul>
    </div>



            
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/RobertLexis">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:robert_lexis@163.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Robert Lexis Loves Wenny
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



</body>

</html>