<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="主要总结了轻量级深度卷积神经网络 MobileNet V1 &amp;amp; V2 的关键贡献 separable convolution 和 inverted resisual &amp;amp; linear bottlenecks。
MobileNet V1MobileNets: Efficient Con">
    

    <!--Author-->
    
        <meta name="author" content="Robert Lexis">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="MobileNet V1 and V2 带来的卷积结构革命"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="This is Robert Lexis."/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>MobileNet V1 and V2 带来的卷积结构革命 - This is Robert Lexis.</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
    </div>
</header>

        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/2018/08/11/MobileNet-V1-and-V2-带来的卷积结构革命/">
                MobileNet V1 and V2 带来的卷积结构革命
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-08-11</span>
            
            
            
        </div>
    </div>

    <div class="content">

        <!-- Gallery -->
        

        <!-- Post Content -->
        <p>主要总结了轻量级深度卷积神经网络 MobileNet V1 &amp; V2 的关键贡献 separable convolution 和 inverted resisual &amp; linear bottlenecks。</p>
<h3 id="MobileNet-V1"><a href="#MobileNet-V1" class="headerlink" title="MobileNet V1"></a>MobileNet V1</h3><p><a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="external">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a><br>提出了<strong>可分离卷积结构（separable convolution）</strong>来代替传统的卷积结构，可以在很小的精度损失下有效地减少网络参数，适合于在移动端和嵌入式环境下构建轻量级深度卷积神经网络。</p>
<blockquote>
<p>A standard convolution both filters and combines inputs into a new set of outputs in one step. The depthwise separable convolution splits this into two layers, a separate layer for filtering and a separate layer for combining.</p>
<p>Depthwise separable convolution are made up of two layers: depthwise convolutions and pointwise convolutions. We use depthwise convolutions to apply a single filter per each input channel (input depth). Pointwise convolution, a simple 1×1 convolution, is then used to create a linear combination of the output of the depthwise layer. MobileNets use both batchnorm and ReLU nonlinearities for both layers.</p>
<p>MobileNet uses 3 × 3 depthwise separable convolutions which uses between 8 to 9 times less computation than standard convolutions at only a small reduction in accuracy.</p>
</blockquote>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/mobilenet/separable_convolution.png" alt="separable_convolution"><br><strong>可分离卷积结构可以替代标准卷积结构在各种各样的卷积神经网络中作为基础卷积结构，实现卷积神经网络的轻量化。</strong></p>
<h5 id="MobileNet-可分离卷积的官方实现（去掉关于-namescope-的代码）"><a href="#MobileNet-可分离卷积的官方实现（去掉关于-namescope-的代码）" class="headerlink" title="MobileNet 可分离卷积的官方实现（去掉关于 namescope 的代码）"></a>MobileNet 可分离卷积的官方实现（去掉关于 namescope 的代码）</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_separable_conv2d</span><span class="params">(input_tensor,</span></span></div><div class="line"><span class="function"><span class="params">                           num_outputs,</span></span></div><div class="line"><span class="function"><span class="params">                           normalizer_fn=None,</span></span></div><div class="line"><span class="function"><span class="params">                           stride=<span class="number">1</span>,</span></span></div><div class="line"><span class="function"><span class="params">                           rate=<span class="number">1</span>)</span>:</span></div><div class="line">    <span class="string">"""Separable mobilenet V1 style convolution.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Depthwise convolution, with default non-linearity (relu),</span></div><div class="line"><span class="string">    followed by 1x1 depthwise convolution.  This is similar to</span></div><div class="line"><span class="string">    slim.separable_conv2d, but differs in that it applies batch</span></div><div class="line"><span class="string">    normalization and non-linearity to depthwise. This  matches</span></div><div class="line"><span class="string">    the basic building of Mobilenet Paper</span></div><div class="line"><span class="string">    (https://arxiv.org/abs/1704.04861)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Args:</span></div><div class="line"><span class="string">      input_tensor: input</span></div><div class="line"><span class="string">      num_outputs: number of outputs</span></div><div class="line"><span class="string">      normalizer_fn: which normalizer function to use for depthwise/pointwise</span></div><div class="line"><span class="string">      stride: stride</span></div><div class="line"><span class="string">      rate: output rate (also known as dilation rate)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">        output tesnor</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    kernel_size = [<span class="number">3</span>, <span class="number">3</span>]</div><div class="line">    padding = <span class="string">'SAME'</span></div><div class="line">    <span class="comment"># 设置 num_outputs = None，即仅做 depthwise convoltion，而不做 slim.separable_conv2d 中的 pointwise convolution，然后依次使用了 normalizer_fn 和 slim.separable_conv2d 的默认非线性激活函数（relu），符合论文给出的卷积结构。</span></div><div class="line">    net = slim.separable_conv2d(</div><div class="line">        input_tensor,</div><div class="line">        num_outputs=<span class="keyword">None</span>,</div><div class="line">        kernel_size,</div><div class="line">        depth_multiplier=<span class="number">1</span>,</div><div class="line">        stride=stride,</div><div class="line">        rate=rate,</div><div class="line">        normalizer_fn=normalizer_fn,</div><div class="line">        padding=padding)</div><div class="line"></div><div class="line">    <span class="comment"># 进行 pointwise convolution，然后依次使用了 normalizer_fn 和 slim.separable_conv2d 的默认非线性激活函数（relu），符合论文给出的卷积结构。</span></div><div class="line">    net = slim.conv2d(</div><div class="line">        net,</div><div class="line">        num_outputs, [<span class="number">1</span>, <span class="number">1</span>],</div><div class="line">        stride=<span class="number">1</span>,</div><div class="line">        normalizer_fn=normalizer_fn)</div><div class="line">  <span class="keyword">return</span> net</div></pre></td></tr></table></figure>
<p>slim.separable_conv2d 和 slim.conv2d 的<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py" target="_blank" rel="external">官方文档</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">slim.separable_conv2d(</div><div class="line">    inputs,</div><div class="line">    num_outputs,</div><div class="line">    kernel_size,</div><div class="line">    depth_multiplier=<span class="number">1</span>,</div><div class="line">    stride=<span class="number">1</span>,</div><div class="line">    padding=<span class="string">'SAME'</span>,</div><div class="line">    data_format=DATA_FORMAT_NHWC,</div><div class="line">    rate=<span class="number">1</span>,</div><div class="line">    activation_fn=nn.relu,</div><div class="line">    normalizer_fn=<span class="keyword">None</span>,</div><div class="line">    normalizer_params=<span class="keyword">None</span>,</div><div class="line">    ...)</div><div class="line"><span class="number">1.</span> This op first performs a depthwise convolution that acts separately on</div><div class="line">channels, creating a variable called `depthwise_weights`. </div><div class="line"><span class="number">2.</span> If `num_outputs` <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, it adds a pointwise convolution that mixes channels, creating a</div><div class="line">variable called `pointwise_weights`. </div><div class="line"><span class="number">3.</span> Then, <span class="keyword">if</span> `normalizer_fn` <span class="keyword">is</span> <span class="keyword">None</span>, it adds bias to the result, creating a variable called <span class="string">'biases'</span>, otherwise, the `normalizer_fn` <span class="keyword">is</span> applied. </div><div class="line"><span class="number">4.</span> It <span class="keyword">finally</span> applies an activation function to produce the end result.</div><div class="line"></div><div class="line">slim.conv2d</div><div class="line"><span class="number">1.</span> creates a variable called `weights`, representing the convolutional kernel, that <span class="keyword">is</span> convolved (actually cross-correlated) <span class="keyword">with</span> the `inputs`.</div><div class="line"><span class="number">2.</span> If a `normalizer_fn` <span class="keyword">is</span> provided (such <span class="keyword">as</span> `batch_norm`), it <span class="keyword">is</span> then applied. Otherwise, <span class="keyword">if</span></div><div class="line">`normalizer_fn` <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">and</span> a `biases_initializer` <span class="keyword">is</span> provided then a `biases`</div><div class="line">variable would be created <span class="keyword">and</span> added the activations. </div><div class="line"><span class="number">3.</span> Finally, <span class="keyword">if</span> `activation_fn` <span class="keyword">is</span> <span class="keyword">not</span> `<span class="keyword">None</span>`, it <span class="keyword">is</span> applied to the activations <span class="keyword">as</span> well.</div></pre></td></tr></table></figure></p>
<h3 id="MobileNet-V2"><a href="#MobileNet-V2" class="headerlink" title="MobileNet V2"></a>MobileNet V2</h3><p><a href="https://arxiv.org/abs/1801.04381" target="_blank" rel="external">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></p>
<blockquote>
<p>Main contribution:<br>A novel layer module: the inverted residual with linear bottleneck. This module takes as an input a low-dimensional compressed representation which is first expanded to high dimension and filtered with a lightweight depthwise convolution. Features are subsequently projected back to a low-dimensional representation with a <strong>linear convolution</strong>.<br>总结起来就是一句话，利用 MobileNet V1 提出的可分离卷积结构的变种（去掉可分离卷积结构中 pointwise 之后的 relu）作为基础卷积结构，对 residual block 的变种进行了“加速降参”。</p>
</blockquote>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/mobilenet/inverted_residual.png" alt="inverted_residual"><br><small>Diagonally hatched layers do not use non-linearities. The thickness of each block to indicate its relative number of channels. Note how classical residuals connects the layers with high number of channels, whereas the inverted residuals connect the bottlenecks. </small></p>
<ol>
<li>为什么输入和输出的通道数目都被限制在较小的值？<br> The manifolds of interest in neural networks could be embedded in low-dimensional subspaces.</li>
<li>为什么扩张通道数？<br> 由下图可以看出不应该在低维空间内应用 relu，因为会造成信息的丢失，而又需要 relu 提供非线性，就只能在扩张通道数（升维）之后再使用 relu。<blockquote>
<p>The bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the tensor. </p>
</blockquote>
</li>
</ol>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/mobilenet/why_remove_relu.png" alt="why_remove_relu"></p>
<ol>
<li>为什么去掉可分离卷积结构中 pointwise 之后的 relu？<br>不应该在低维空间内应用 relu，因为会造成信息的丢失</li>
</ol>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/mobilenet/inverted_residual_1.png" alt="inverted_residual_1"></p>
<p><strong>Inverted Residuals 指中间通道多两头通道少，Linear Bottlenecks 指两头都是线性激活之后得到的 feature map。</strong></p>
<h5 id="Inverted-Residuals-and-Linear-Bottlenecks-的官方实现（去掉关于-namescope-的代码）"><a href="#Inverted-Residuals-and-Linear-Bottlenecks-的官方实现（去掉关于-namescope-的代码）" class="headerlink" title="Inverted Residuals and Linear Bottlenecks 的官方实现（去掉关于 namescope 的代码）"></a>Inverted Residuals and Linear Bottlenecks 的官方实现（去掉关于 namescope 的代码）</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_make_divisible</span><span class="params">(v, divisor)</span>:</span></div><div class="line">    min_value = divisor</div><div class="line">    new_v = max(min_value, int(v + divisor / <span class="number">2</span>) // divisor * divisor)</div><div class="line">    <span class="comment"># Make sure that round down does not go down by more than 10%.</span></div><div class="line">    <span class="keyword">if</span> new_v &lt; <span class="number">0.9</span> * v:</div><div class="line">        new_v += divisor</div><div class="line">    <span class="keyword">return</span> new_v</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">expand_input_by_factor</span><span class="params">(n, divisible_by=<span class="number">8</span>)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">lambda</span> num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">expanded_conv</span><span class="params">(input_tensor,</span></span></div><div class="line"><span class="function"><span class="params">                  num_outputs,</span></span></div><div class="line"><span class="function"><span class="params">                  expansion_size=expand_input_by_factor<span class="params">(<span class="number">6</span>)</span>,</span></span></div><div class="line"><span class="function"><span class="params">                  stride=<span class="number">1</span>,</span></span></div><div class="line"><span class="function"><span class="params">                  rate=<span class="number">1</span>,</span></span></div><div class="line"><span class="function"><span class="params">                  kernel_size=<span class="params">(<span class="number">3</span>, <span class="number">3</span>)</span>,</span></span></div><div class="line"><span class="function"><span class="params">                  normalizer_fn=None,</span></span></div><div class="line"><span class="function"><span class="params">                  depthwise_channel_multiplier=<span class="number">1</span>,</span></span></div><div class="line"><span class="function"><span class="params">                  padding=<span class="string">'SAME'</span>)</span>:</span></div><div class="line">    <span class="string">"""Depthwise Convolution Block with expansion.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Builds a composite convolution that has the following structure</span></div><div class="line"><span class="string">    expansion (1x1) -&gt; depthwise (kernel_size) -&gt; projection (1x1)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Args:</span></div><div class="line"><span class="string">        input_tensor: input</span></div><div class="line"><span class="string">        num_outputs: number of outputs in the final layer.</span></div><div class="line"><span class="string">        expansion_size: the size of expansion, could be a constant or a callable.</span></div><div class="line"><span class="string">            If latter it will be provided 'num_inputs' as an input. For forward</span></div><div class="line"><span class="string">            compatibility it should accept arbitrary keyword arguments.</span></div><div class="line"><span class="string">            Default will expand the input by factor of 6.</span></div><div class="line"><span class="string">        stride: depthwise stride</span></div><div class="line"><span class="string">        rate: depthwise rate</span></div><div class="line"><span class="string">        kernel_size: depthwise kernel</span></div><div class="line"><span class="string">        normalizer_fn: batchnorm or otherwise</span></div><div class="line"><span class="string">        depthwise_channel_multiplier: depthwise channel multiplier:</span></div><div class="line"><span class="string">        padding: Padding type to use</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">        Tensor of depth num_outputs</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    prev_depth = input_tensor.get_shape().as_list()[<span class="number">3</span>]</div><div class="line"></div><div class="line">    <span class="comment"># 设置 num_outputs = None，即仅做 depthwise convoltion，而不做 slim.separable_conv2d 中的 pointwise convolution，然后依次使用了 normalizer_fn 和 slim.separable_conv2d 的默认非线性激活函数（relu），符合论文给出的卷积结构。</span></div><div class="line">    depthwise_func = functools.partial(</div><div class="line">        slim.separable_conv2d,</div><div class="line">        num_outputs=<span class="keyword">None</span>,</div><div class="line">        kernel_size=kernel_size,</div><div class="line">        depth_multiplier=depthwise_channel_multiplier,</div><div class="line">        stride=stride,</div><div class="line">        rate=rate,</div><div class="line">        normalizer_fn=normalizer_fn,</div><div class="line">        padding=padding)</div><div class="line"></div><div class="line">    net = input_tensor</div><div class="line"></div><div class="line">    <span class="comment"># expand</span></div><div class="line">    <span class="keyword">if</span> callable(expansion_size):</div><div class="line">      inner_size = expansion_size(num_inputs=prev_depth)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      inner_size = expansion_size</div><div class="line">    <span class="keyword">if</span> inner_size &gt; net.shape[<span class="number">3</span>]:</div><div class="line">        net = slim.conv2d(net, inner_size, [<span class="number">1</span>, <span class="number">1</span>], stride=<span class="number">1</span>, normalizer_fn=normalizer_fn, padding=padding)</div><div class="line"></div><div class="line">    <span class="comment"># separable convolution</span></div><div class="line">    net = depthwise_func(net)</div><div class="line">    net = slim.conv2d(net, num_outputs, [<span class="number">1</span>, <span class="number">1</span>], stride=<span class="number">1</span>, normalizer_fn=normalizer_fn, activation_fn=<span class="keyword">None</span>, padding=padding)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> residual <span class="keyword">and</span> stride == <span class="number">1</span> <span class="keyword">and</span> net.get_shape().as_list()[<span class="number">3</span>] == prev_depth: </div><div class="line">      net += input_tensor</div><div class="line">    <span class="keyword">return</span> net</div></pre></td></tr></table></figure>

    </div>

    

    

    <!-- Comments -->
    

</div>
        </section>

    </div>
</div>


</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This is Robert Lexis (FengCun Li). To see the world, things dangerous to come to, to see behind walls, to draw closer, to find each other and to feel. That is the purpose of LIFE.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2018/08/26/YOLO/">YOLO</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/26/非最大值抑制/">非最大值抑制</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/26/目标定位-vs-目标检测/">目标定位 vs 目标检测</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/13/消失的梯度/">消失的梯度</a>
            </li>
            
        </ul>
    </div>



            
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/RobertLexis">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:robert_lexis@163.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Robert Lexis Loves Wenny
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



</body>

</html>