<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="主要总结了轻量级深度卷积神经网络 MobileNet V1 &amp;amp; V2 的关键贡献 separable convolution 和 inverted resisual &amp;amp; linear bottlenecks。">
<meta property="og:type" content="article">
<meta property="og:title" content="MobileNet V1 and V2 带来的卷积结构革命">
<meta property="og:url" content="http://yoursite.com/2018/08/11/MobileNet-V1-and-V2-带来的卷积结构革命/index.html">
<meta property="og:site_name" content="The next stop - Antarctica">
<meta property="og:description" content="主要总结了轻量级深度卷积神经网络 MobileNet V1 &amp;amp; V2 的关键贡献 separable convolution 和 inverted resisual &amp;amp; linear bottlenecks。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://oytnj8g2y.bkt.clouddn.com/image/jpg/mobilenet/separable_convolution.png">
<meta property="og:image" content="http://oytnj8g2y.bkt.clouddn.com/image/jpg/mobilenet/inverted_residual.png">
<meta property="og:image" content="http://oytnj8g2y.bkt.clouddn.com/image/jpg/mobilenet/why_remove_relu.png">
<meta property="og:image" content="http://oytnj8g2y.bkt.clouddn.com/image/jpg/mobilenet/inverted_residual_1.png">
<meta property="og:updated_time" content="2020-11-09T04:33:37.427Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MobileNet V1 and V2 带来的卷积结构革命">
<meta name="twitter:description" content="主要总结了轻量级深度卷积神经网络 MobileNet V1 &amp;amp; V2 的关键贡献 separable convolution 和 inverted resisual &amp;amp; linear bottlenecks。">
<meta name="twitter:image" content="http://oytnj8g2y.bkt.clouddn.com/image/jpg/mobilenet/separable_convolution.png">

<link rel="canonical" href="http://yoursite.com/2018/08/11/MobileNet-V1-and-V2-带来的卷积结构革命/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>MobileNet V1 and V2 带来的卷积结构革命 | The next stop - Antarctica</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">The next stop - Antarctica</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">To see behind walls. To draw closer.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/11/MobileNet-V1-and-V2-带来的卷积结构革命/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Fengcun Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="The next stop - Antarctica">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MobileNet V1 and V2 带来的卷积结构革命
        </h1>

        <div class="post-meta">
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-11-09 12:33:37" itemprop="dateModified" datetime="2020-11-09T12:33:37+08:00">2020-11-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>主要总结了轻量级深度卷积神经网络 MobileNet V1 &amp; V2 的关键贡献 separable convolution 和 inverted resisual &amp; linear bottlenecks。</p>
<a id="more"></a>
<h3 id="MobileNet-V1"><a href="#MobileNet-V1" class="headerlink" title="MobileNet V1"></a>MobileNet V1</h3><p><a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a><br>提出了<strong>可分离卷积结构（separable convolution）</strong>来代替传统的卷积结构，可以在很小的精度损失下有效地减少网络参数，适合于在移动端和嵌入式环境下构建轻量级深度卷积神经网络。</p>
<blockquote>
<p>A standard convolution both filters and combines inputs into a new set of outputs in one step. The depthwise separable convolution splits this into two layers, a separate layer for filtering and a separate layer for combining.</p>
</blockquote>
<blockquote>
<p>Depthwise separable convolution are made up of two layers: depthwise convolutions and pointwise convolutions. We use depthwise convolutions to apply a single filter per each input channel (input depth). Pointwise convolution, a simple 1×1 convolution, is then used to create a linear combination of the output of the depthwise layer. MobileNets use both batchnorm and ReLU nonlinearities for both layers.</p>
</blockquote>
<blockquote>
<p>MobileNet uses 3 × 3 depthwise separable convolutions which uses between 8 to 9 times less computation than standard convolutions at only a small reduction in accuracy.</p>
</blockquote>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/mobilenet/separable_convolution.png" alt="separable_convolution"></p>
<p><strong>可分离卷积结构可以替代标准卷积结构在各种各样的卷积神经网络中作为基础卷积结构，实现卷积神经网络的轻量化。</strong></p>
<h5 id="MobileNet-可分离卷积的官方实现（去掉关于-namescope-的代码）"><a href="#MobileNet-可分离卷积的官方实现（去掉关于-namescope-的代码）" class="headerlink" title="MobileNet 可分离卷积的官方实现（去掉关于 namescope 的代码）"></a>MobileNet 可分离卷积的官方实现（去掉关于 namescope 的代码）</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_separable_conv2d</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                           num_outputs,</span></span></span><br><span class="line"><span class="function"><span class="params">                           normalizer_fn=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                           stride=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                           rate=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Separable mobilenet V1 style convolution.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Depthwise convolution, with default non-linearity (relu),</span></span><br><span class="line"><span class="string">    followed by 1x1 depthwise convolution.  This is similar to</span></span><br><span class="line"><span class="string">    slim.separable_conv2d, but differs in that it applies batch</span></span><br><span class="line"><span class="string">    normalization and non-linearity to depthwise. This  matches</span></span><br><span class="line"><span class="string">    the basic building of Mobilenet Paper</span></span><br><span class="line"><span class="string">    (https://arxiv.org/abs/1704.04861)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      input_tensor: input</span></span><br><span class="line"><span class="string">      num_outputs: number of outputs</span></span><br><span class="line"><span class="string">      normalizer_fn: which normalizer function to use for depthwise/pointwise</span></span><br><span class="line"><span class="string">      stride: stride</span></span><br><span class="line"><span class="string">      rate: output rate (also known as dilation rate)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        output tesnor</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    kernel_size = [<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">    padding = <span class="string">'SAME'</span></span><br><span class="line">    <span class="comment"># 设置 num_outputs = None，即仅做 depthwise convoltion，而不做 slim.separable_conv2d 中的 pointwise convolution，然后依次使用了 normalizer_fn 和 slim.separable_conv2d 的默认非线性激活函数（relu），符合论文给出的卷积结构。</span></span><br><span class="line">    net = slim.separable_conv2d(</span><br><span class="line">        input_tensor,</span><br><span class="line">        num_outputs=<span class="keyword">None</span>,</span><br><span class="line">        kernel_size,</span><br><span class="line">        depth_multiplier=<span class="number">1</span>,</span><br><span class="line">        stride=stride,</span><br><span class="line">        rate=rate,</span><br><span class="line">        normalizer_fn=normalizer_fn,</span><br><span class="line">        padding=padding)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行 pointwise convolution，然后依次使用了 normalizer_fn 和 slim.separable_conv2d 的默认非线性激活函数（relu），符合论文给出的卷积结构。</span></span><br><span class="line">    net = slim.conv2d(</span><br><span class="line">        net,</span><br><span class="line">        num_outputs, [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        stride=<span class="number">1</span>,</span><br><span class="line">        normalizer_fn=normalizer_fn)</span><br><span class="line">  <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure>
<p>slim.separable_conv2d 和 slim.conv2d 的<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py" target="_blank" rel="noopener">官方文档</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">slim.separable_conv2d(</span><br><span class="line">    inputs,</span><br><span class="line">    num_outputs,</span><br><span class="line">    kernel_size,</span><br><span class="line">    depth_multiplier=<span class="number">1</span>,</span><br><span class="line">    stride=<span class="number">1</span>,</span><br><span class="line">    padding=<span class="string">'SAME'</span>,</span><br><span class="line">    data_format=DATA_FORMAT_NHWC,</span><br><span class="line">    rate=<span class="number">1</span>,</span><br><span class="line">    activation_fn=nn.relu,</span><br><span class="line">    normalizer_fn=<span class="keyword">None</span>,</span><br><span class="line">    normalizer_params=<span class="keyword">None</span>,</span><br><span class="line">    ...)</span><br><span class="line"><span class="number">1.</span> This op first performs a depthwise convolution that acts separately on</span><br><span class="line">channels, creating a variable called `depthwise_weights`.</span><br><span class="line"><span class="number">2.</span> If `num_outputs` <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, it adds a pointwise convolution that mixes channels, creating a</span><br><span class="line">variable called `pointwise_weights`.</span><br><span class="line"><span class="number">3.</span> Then, <span class="keyword">if</span> `normalizer_fn` <span class="keyword">is</span> <span class="keyword">None</span>, it adds bias to the result, creating a variable called <span class="string">'biases'</span>, otherwise, the `normalizer_fn` <span class="keyword">is</span> applied.</span><br><span class="line"><span class="number">4.</span> It <span class="keyword">finally</span> applies an activation function to produce the end result.</span><br><span class="line"></span><br><span class="line">slim.conv2d</span><br><span class="line"><span class="number">1.</span> creates a variable called `weights`, representing the convolutional kernel, that <span class="keyword">is</span> convolved (actually cross-correlated) <span class="keyword">with</span> the `inputs`.</span><br><span class="line"><span class="number">2.</span> If a `normalizer_fn` <span class="keyword">is</span> provided (such <span class="keyword">as</span> `batch_norm`), it <span class="keyword">is</span> then applied. Otherwise, <span class="keyword">if</span></span><br><span class="line">`normalizer_fn` <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">and</span> a `biases_initializer` <span class="keyword">is</span> provided then a `biases`</span><br><span class="line">variable would be created <span class="keyword">and</span> added the activations.</span><br><span class="line"><span class="number">3.</span> Finally, <span class="keyword">if</span> `activation_fn` <span class="keyword">is</span> <span class="keyword">not</span> `<span class="keyword">None</span>`, it <span class="keyword">is</span> applied to the activations <span class="keyword">as</span> well.</span><br></pre></td></tr></table></figure>
<h3 id="MobileNet-V2"><a href="#MobileNet-V2" class="headerlink" title="MobileNet V2"></a>MobileNet V2</h3><p><a href="https://arxiv.org/abs/1801.04381" target="_blank" rel="noopener">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></p>
<blockquote>
<p>Main contribution:<br>A novel layer module: the inverted residual with linear bottleneck. This module takes as an input a low-dimensional compressed representation which is first expanded to high dimension and filtered with a lightweight depthwise convolution. Features are subsequently projected back to a low-dimensional representation with a <strong>linear convolution</strong>.<br>总结起来就是一句话，利用 MobileNet V1 提出的可分离卷积结构的变种（去掉可分离卷积结构中 pointwise 之后的 relu）作为基础卷积结构，对 residual block 的变种进行了“加速降参”。</p>
</blockquote>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/mobilenet/inverted_residual.png" alt="inverted_residual"><br><small>Diagonally hatched layers do not use non-linearities. The thickness of each block to indicate its relative number of channels. Note how classical residuals connects the layers with high number of channels, whereas the inverted residuals connect the bottlenecks. </small></p>
<ol>
<li>为什么输入和输出的通道数目都被限制在较小的值？<br>The manifolds of interest in neural networks could be embedded in low-dimensional subspaces.</li>
<li>为什么扩张通道数？<br>由下图可以看出不应该在低维空间内应用 relu，因为会造成信息的丢失，而又需要 relu 提供非线性，就只能在扩张通道数（升维）之后再使用 relu。<blockquote>
<p>The bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the tensor.</p>
</blockquote>
</li>
</ol>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/mobilenet/why_remove_relu.png" alt="why_remove_relu"> 3. 为什么去掉可分离卷积结构中 pointwise 之后的 relu？<br>不应该在低维空间内应用 relu，因为会造成信息的丢失</p>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/mobilenet/inverted_residual_1.png" alt="inverted_residual_1"></p>
<p><strong>Inverted Residuals 指中间通道多两头通道少，Linear Bottlenecks 指两头都是线性激活之后得到的 feature map。</strong></p>
<h5 id="Inverted-Residuals-and-Linear-Bottlenecks-的官方实现（去掉关于-namescope-的代码）"><a href="#Inverted-Residuals-and-Linear-Bottlenecks-的官方实现（去掉关于-namescope-的代码）" class="headerlink" title="Inverted Residuals and Linear Bottlenecks 的官方实现（去掉关于 namescope 的代码）"></a>Inverted Residuals and Linear Bottlenecks 的官方实现（去掉关于 namescope 的代码）</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_make_divisible</span><span class="params">(v, divisor)</span>:</span></span><br><span class="line">    min_value = divisor</span><br><span class="line">    new_v = max(min_value, int(v + divisor / <span class="number">2</span>) // divisor * divisor)</span><br><span class="line">    <span class="comment"># Make sure that round down does not go down by more than 10%.</span></span><br><span class="line">    <span class="keyword">if</span> new_v &lt; <span class="number">0.9</span> * v:</span><br><span class="line">        new_v += divisor</span><br><span class="line">    <span class="keyword">return</span> new_v</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expand_input_by_factor</span><span class="params">(n, divisible_by=<span class="number">8</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">lambda</span> num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expanded_conv</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                  num_outputs,</span></span></span><br><span class="line"><span class="function"><span class="params">                  expansion_size=expand_input_by_factor<span class="params">(<span class="number">6</span>)</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  stride=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  rate=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  kernel_size=<span class="params">(<span class="number">3</span>, <span class="number">3</span>)</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  normalizer_fn=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                  depthwise_channel_multiplier=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  padding=<span class="string">'SAME'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Depthwise Convolution Block with expansion.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Builds a composite convolution that has the following structure</span></span><br><span class="line"><span class="string">    expansion (1x1) -&gt; depthwise (kernel_size) -&gt; projection (1x1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        input_tensor: input</span></span><br><span class="line"><span class="string">        num_outputs: number of outputs in the final layer.</span></span><br><span class="line"><span class="string">        expansion_size: the size of expansion, could be a constant or a callable.</span></span><br><span class="line"><span class="string">            If latter it will be provided 'num_inputs' as an input. For forward</span></span><br><span class="line"><span class="string">            compatibility it should accept arbitrary keyword arguments.</span></span><br><span class="line"><span class="string">            Default will expand the input by factor of 6.</span></span><br><span class="line"><span class="string">        stride: depthwise stride</span></span><br><span class="line"><span class="string">        rate: depthwise rate</span></span><br><span class="line"><span class="string">        kernel_size: depthwise kernel</span></span><br><span class="line"><span class="string">        normalizer_fn: batchnorm or otherwise</span></span><br><span class="line"><span class="string">        depthwise_channel_multiplier: depthwise channel multiplier:</span></span><br><span class="line"><span class="string">        padding: Padding type to use</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Tensor of depth num_outputs</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    prev_depth = input_tensor.get_shape().as_list()[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置 num_outputs = None，即仅做 depthwise convoltion，而不做 slim.separable_conv2d 中的 pointwise convolution，然后依次使用了 normalizer_fn 和 slim.separable_conv2d 的默认非线性激活函数（relu），符合论文给出的卷积结构。</span></span><br><span class="line">    depthwise_func = functools.partial(</span><br><span class="line">        slim.separable_conv2d,</span><br><span class="line">        num_outputs=<span class="keyword">None</span>,</span><br><span class="line">        kernel_size=kernel_size,</span><br><span class="line">        depth_multiplier=depthwise_channel_multiplier,</span><br><span class="line">        stride=stride,</span><br><span class="line">        rate=rate,</span><br><span class="line">        normalizer_fn=normalizer_fn,</span><br><span class="line">        padding=padding)</span><br><span class="line"></span><br><span class="line">    net = input_tensor</span><br><span class="line"></span><br><span class="line">    <span class="comment"># expand</span></span><br><span class="line">    <span class="keyword">if</span> callable(expansion_size):</span><br><span class="line">      inner_size = expansion_size(num_inputs=prev_depth)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      inner_size = expansion_size</span><br><span class="line">    <span class="keyword">if</span> inner_size &gt; net.shape[<span class="number">3</span>]:</span><br><span class="line">        net = slim.conv2d(net, inner_size, [<span class="number">1</span>, <span class="number">1</span>], stride=<span class="number">1</span>, normalizer_fn=normalizer_fn, padding=padding)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># separable convolution</span></span><br><span class="line">    net = depthwise_func(net)</span><br><span class="line">    net = slim.conv2d(net, num_outputs, [<span class="number">1</span>, <span class="number">1</span>], stride=<span class="number">1</span>, normalizer_fn=normalizer_fn, activation_fn=<span class="keyword">None</span>, padding=padding)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> residual <span class="keyword">and</span> stride == <span class="number">1</span> <span class="keyword">and</span> net.get_shape().as_list()[<span class="number">3</span>] == prev_depth:</span><br><span class="line">      net += input_tensor</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/08/09/卷积转置卷积关系在-TensorFLow-中的验证/" rel="prev" title="卷积转置卷积关系在 TensorFLow 中的验证">
      <i class="fa fa-chevron-left"></i> 卷积转置卷积关系在 TensorFLow 中的验证
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/08/13/消失的梯度/" rel="next" title="消失的梯度">
      消失的梯度 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#MobileNet-V1"><span class="nav-number">1.</span> <span class="nav-text">MobileNet V1</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#MobileNet-可分离卷积的官方实现（去掉关于-namescope-的代码）"><span class="nav-number">1.0.1.</span> <span class="nav-text">MobileNet 可分离卷积的官方实现（去掉关于 namescope 的代码）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MobileNet-V2"><span class="nav-number">2.</span> <span class="nav-text">MobileNet V2</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Inverted-Residuals-and-Linear-Bottlenecks-的官方实现（去掉关于-namescope-的代码）"><span class="nav-number">2.0.1.</span> <span class="nav-text">Inverted Residuals and Linear Bottlenecks 的官方实现（去掉关于 namescope 的代码）</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Fengcun Li"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Fengcun Li</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">104</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Fengcun Li</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
