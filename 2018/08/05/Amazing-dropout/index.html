<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="本文主要采摘、翻译、修改自 P. Galeone’s blog Analysis of Dropout。 Overfitting is a problem in Deep Neural Networks (DNN): the model learns to classify only the training set, adapting itself to the training example">
<meta name="keywords" content="DL">
<meta property="og:type" content="article">
<meta property="og:title" content="Amazing Dropout">
<meta property="og:url" content="http://yoursite.com/2018/08/05/Amazing-dropout/index.html">
<meta property="og:site_name" content="The next stop - Antarctica">
<meta property="og:description" content="本文主要采摘、翻译、修改自 P. Galeone’s blog Analysis of Dropout。 Overfitting is a problem in Deep Neural Networks (DNN): the model learns to classify only the training set, adapting itself to the training example">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/2018/08/05/Amazing-dropout/dropout.jpg">
<meta property="og:image" content="http://yoursite.com/2018/08/05/Amazing-dropout/distributions.png">
<meta property="og:updated_time" content="2018-08-28T02:49:26.636Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Amazing Dropout">
<meta name="twitter:description" content="本文主要采摘、翻译、修改自 P. Galeone’s blog Analysis of Dropout。 Overfitting is a problem in Deep Neural Networks (DNN): the model learns to classify only the training set, adapting itself to the training example">
<meta name="twitter:image" content="http://yoursite.com/2018/08/05/Amazing-dropout/dropout.jpg">

<link rel="canonical" href="http://yoursite.com/2018/08/05/Amazing-dropout/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Amazing Dropout | The next stop - Antarctica</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">The next stop - Antarctica</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">To see behind walls. To draw closer.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/05/Amazing-dropout/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Fengcun Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="The next stop - Antarctica">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Amazing Dropout
        </h1>

        <div class="post-meta">
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2018-08-28 10:49:26" itemprop="dateModified" datetime="2018-08-28T10:49:26+08:00">2018-08-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文主要采摘、翻译、修改自 P. Galeone’s blog <a href="https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/" target="_blank" rel="noopener">Analysis of Dropout</a>。</p>
<p>Overfitting is a problem in Deep Neural Networks (DNN): the model learns to classify only the training set, adapting itself to the training examples instead of learning decision boundaries capable of classifying generic instances. Many solutions to the overfitting problem have been presented during these years; one of them have overwhelmed the others due to its simplicity and its empirical good results: <strong>Dropout</strong>.</p>
<p><img src="/2018/08/05/Amazing-dropout/dropout.jpg" alt="dropout"><br><a id="more"></a><br>Visual representation of Dropout, right from the paper. On the left there’s the network before applying Dropout, on the right the same network with Dropout applied.The network on the left it’s the same network used at test time, once the parameters have been learned.</p>
<p>The idea behind Dropout is to train an ensemble of DNNs and average the results of the whole ensemble instead of train a single DNN.<strong>(Dropout 背后的思想是训练一个含有多个 DNN 的集成学习器。)</strong></p>
<p>The DNNs are built dropping out neurons with \( p \) probability, therefore keeping the others on with probability \( q=1−p \). When a neuron is dropped out, its output is set to zero, no matter what the input or the associated learned parameter is.</p>
<p>The dropped neurons do not contribute to the training phase in both the forward and backward phases of back-propagation: for this reason every time a single neuron is dropped out it’s like the training phase is done on a new network.<strong>(被 dropout 的神经元不参与训练过程的前向传播和反向传播，每次进行一个 batch 的训练的时候，由于被 dropout 的神经元组合不同，就像是在训练一个新的神经网络。)</strong></p>
<p>Quoting the authors:</p>
<blockquote>
<p>In a standard neural network, the derivative received by each parameter tells it how it should change so the final loss function is reduced, given what all other units are doing. Therefore, units may change in a way that they fix up the mistakes of the other units. This may lead to complex co-adaptations. This in turn leads to overfitting because these co-adaptations do not generalize to unseen data. We hypothesize that for each hidden unit, Dropout prevents co-adaptation by making the presence of other hidden units unreliable. <strong>Therefore, a hidden unit cannot rely on other specific units to correct its mistakes.</strong></p>
</blockquote>
<p>In short: Dropout works well in practice because it prevents the co-adaption of neurons during the training phase.</p>
<p>Now that we got an intuitive idea behind Dropout, let’s analyze it in depth.</p>
<h3 id="How-Dropout-works"><a href="#How-Dropout-works" class="headerlink" title="How Dropout works"></a>How Dropout works</h3><p>As said before, Dropout turns off neurons with probability \( p \) and therefore let the others turned on with probability \( q=1−p \)<br><strong> Every single neuron has the same probability of being turned off.</strong> This means that:<br>Given</p>
<ul>
<li>\( h(x) = Wx + b \) a linear projection of a \( d_i \)-dimensional input x into a \( d_h \)-dimensional output space.</li>
<li>\( a(h) \) an activation function<br>it’s possible to model the application of Dropout, in the training phase only, to the given projection as a modified activation function (可以将 Dropout 建模为一个特殊的激活函数（element-wise）):<br>\[<br>f(h) = D \bigodot a(h)<br>\]<br>Where \( D=(X_1, …, X_{d_h}) \) is a \( d_h \)-dimensional vector of Bernoulli variables \( X_i \) （\( d_h \) 维伯努利随机变量的向量）.</li>
</ul>
<h5 id="伯努利随机变量"><a href="#伯努利随机变量" class="headerlink" title="伯努利随机变量"></a>伯努利随机变量</h5><p>A Bernoulli random variable(伯努利随机变量) has the following probability mass distribution（pdf，概率质量函数）:<br>\[<br>f(k; p) = p^k (1-p)^{(1-k)} =<br>\begin{cases}<br>    p \text{ if } k = 1 \\<br>    (1-p) \text{ if } k = 0<br>\end{cases}<br>\]<br>Where k are the possible outcomes.</p>
<blockquote>
<p>伯努利分布 Bernoulli distribution，又名两点分布或者0-1分布，是一个离散型概率分布；若伯努利试验成功，则伯努利随机变量取值为1。若伯努利试验失败，则伯努利随机变量取值为0。</p>
</blockquote>
<p><strong> 设 drop rate 为 \( p \)。</strong><br>It’s evident that this random variable perfectly models the Dropout application on a single neuron. In fact, the neuron is turned off with probability \( p = P(X_i = 0) \) and kept on otherwise.<br><strong> 伯努利试验成功的概率是 \( P( X_i = 1) = 1 - p \)。伯努利试验成功，则神经元被保留。伯努利试验失败，则神经元被关闭。</strong></p>
<p>It can be useful to see the application of Dropout on a generic i-th neuron:<br>\[<br>O_i = X_i a(\sum_{k=1}^{d_i} w_k x_k +b) = \begin{cases}<br>a(\sum_{k=1}^{d_i} w_k x_k +b) \text{ if } X_i = 1 \\<br>0 \text{ if } X_i = 0<br>\end{cases}<br>\]<br>where \( P(X_i = 0) = p \) （即 \(X_i \thicksim Bernoulli(1-p) \)，\( X_i \) 服从成功概率为 \( (1-p) \) 的伯努利分布。）</p>
<p>Since during train phase a neuron is kept on with probability \( q \), during the testing phase we have to emulate the behavior of the ensemble of networks used in the training phase.</p>
<p>To do this, the authors suggest <strong> scaling the activation function by a factor of \( q \) </strong> during the test phase in order to use the expected output produced in the training phase as the single output required in the test phase. Thus:</p>
<p>Train phase:<br>\[<br>O_i = X_i a(\sum_{k=1}^{d_i} w_k x_k + b)<br>\]<br>Test phase:<br>\[<br>O_i= q a(\sum_{k=1}^{d_i} w_k x_k + b)<br>\]</p>
<blockquote>
<p>其实可以看到当 \( X_i \thicksim Bernoulli(1-p) \) 即 \( P(X_i = 0) = p \) 时，\( E(X_i) = (1-p) = q \)。</p>
</blockquote>
<h3 id="Inverted-Dropout"><a href="#Inverted-Dropout" class="headerlink" title="Inverted Dropout"></a>Inverted Dropout</h3><p>A slightly different approach is to use Inverted Dropout. This approach consists in the scaling of the activations during the training phase, leaving the test phase untouched.</p>
<p>The scale factor is the inverse of the keep probability: \( \frac{1}{1-p} = \frac{1}{q} \), thus:<br>Train phase:<br>\[<br>O_i = \frac{1}{q} X_i a(\sum_{k=1}^{di} w_k x_k + b)<br>\]<br>Test phase:<br>\[<br>O_i = a(\sum_{k=1}^{d_i} w_k x_k + b)<br>\]</p>
<p><strong>Inverted Dropout is how Dropout is implemented in practice in the various deep learning frameworks</strong> because it helps to define the model once and just change a parameter (the keep/drop probability) to run train and test on the same model.</p>
<p>Direct Dropout, instead, force you to modify the network during the test phase because if you don’t multiply by \( q \) the output the neuron will produce values that are higher respect to the one expected by the successive neurons (thus the following neurons can saturate or explode): that’s why Inverted Dropout is the more common implementation.</p>
<h3 id="Dropout-of-a-set-of-neurons"><a href="#Dropout-of-a-set-of-neurons" class="headerlink" title="Dropout of a set of neurons"></a>Dropout of a set of neurons</h3><p><strong> 设 drop rate 为 \( p \)。</strong><br>It can be easily noticed that a layer \( h \) with \( n \)  neurons, in a single train step, can be seen as an ensemble of \( n \) Bernoulli experiments, each one with a probability of success equals to \( 1 - p \). (可以看成是 \( n \) 次成功概率为 \( 1 - p \) 伯努利试验)。<br>Thus, the output of the layer h have a number of dropped neurons equals to (被关闭的神经元的数目为):<br>\[<br>Y = \sum_{k=1}^{d_h} 1 - X_i<br>\]</p>
<p>Since every neuron is now modeled as a Bernoulli random variable and all these random variables are independent and identically distributed, the total number of dropped neuron is a random variable too, called Binomial(二项分布):<br>\[<br>Y \thicksim Bi(d_h, p)<br>\]<br>Where the probability of getting exactly k shutdown neurons in n is given by the probability mass distribution:<br>\[<br>f(k;n, p) = \left( \begin{matrix} k \\ n \end{matrix} \right) p^k (1-p)^{(n-k)}<br>\]</p>
<p>We can now use this distribution to analyze the probability of dropping a specified number of neurons.<br>When using Dropout, we define a fixed Dropout probability \( p \) for a chosen layer and we expect that a proportional number of neurons are dropped from it.<br>For example, if the layer we apply Dropout to has \( n=1024 \) neurons and \( p=0.5 \), we expect that 512 get dropped. Let’s verify this statement:<br>\[<br>Y = \sum_{i=1}^{1024} X_i \\<br>Y \thicksim Bi(1024, 0.5) \\<br>P(Y=512) = \left( \begin{matrix} 512 \\ 1024 \end{matrix} \right) 0.5^{512} \times (1−0.5)^{(1024−512)} \approx 0.025<br>\]<br>Thus, the probability of dropping out exactly \( np=512 \) neurons is of only 0.025!</p>
<p>A python 3 script can help us to visualize how neurons are dropped for different values of p and a fixed value of n. The code is commented.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> binom</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># number of neurons</span></span><br><span class="line">n = <span class="number">1024</span></span><br><span class="line"><span class="comment"># number of tests (input examples)</span></span><br><span class="line">size = <span class="number">500</span></span><br><span class="line"><span class="comment"># histogram bin width, for data visualization</span></span><br><span class="line">binwidth = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">10</span>):</span><br><span class="line">    <span class="comment"># per layer probability</span></span><br><span class="line">    prob = p / <span class="number">10</span></span><br><span class="line">    <span class="comment"># generate size values from a bi(n, prob)</span></span><br><span class="line">    rnd_values = binom.rvs(n, prob, size=size)</span><br><span class="line">    <span class="comment"># draw histogram of rnd values</span></span><br><span class="line">    plt.hist(</span><br><span class="line">        rnd_values,</span><br><span class="line">        bins=[x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">0</span>, n, binwidth)],</span><br><span class="line">        <span class="comment"># normalize = extract the probabilities</span></span><br><span class="line">        normed=<span class="number">1</span>,</span><br><span class="line">        <span class="comment"># pick a random color</span></span><br><span class="line">        color=np.random.rand(<span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="comment"># label the histogram with its probability</span></span><br><span class="line">        label=str(prob))</span><br><span class="line"></span><br><span class="line">plt.legend(loc=<span class="string">'upper right'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>The binomial distribution is very peaked around \( np \). As we can see from the image above no matter what the p value is, the number of neurons dropped on average is proportional to \( np \), in fact:<br>\[<br>E[Bi(n,p)]=np<br>\]<br><img src="/2018/08/05/Amazing-dropout/distributions.png" alt="distributions"><br>Moreover, we can notice that the distribution of values is almost symmetric around \( p=0.5 \) and the probability of dropping \( np \) neurons increase as the distance from \( p=0.5 \) increase.</p>
<p>The scaling factor has been added by the authors to compensate(补偿) the activation values, because they expect that during the training phase only a percentage of \( 1−p \) neurons have been kept. During the testing phase, instead, the 100% of neurons are kept on, thus the value should be scaled down accordingly.</p>
<h3 id="Dropout-amp-other-regularizers"><a href="#Dropout-amp-other-regularizers" class="headerlink" title="Dropout &amp; other regularizers"></a>Dropout &amp; other regularizers</h3><p>Dropout is often used with <strong>L2 normalization</strong> and other parameter constraint techniques (such as Max Norm), this is not a case. Normalizations help to keep model parameters value low, in this way a parameter can’t grow too much.（将权重参数保持在较小的数值。）</p>
<p>In brief, the L2 normalization (for example) is an additional term to the loss, where \( \lambda \in [0, 1] \) is an hyper-parameter called regularization strength, \( F(W;x) \) is the model and \( \varepsilon \) is the error function between the real \( y \) and the predicted \( \tilde{y} \).<br>\[<br>\mathcal{L} = \varepsilon(y, F(W;x)) + \frac{\lambda}{2} W^2<br>\]<br>It’s easy to understand that this additional term, when doing back-propagation via gradient descent, reduces the update amount (使得更新不会正的太大). If \( \eta \) is the learning rate, the update amount of the parameter \(w \in W\) is<br>\[<br>w \gets w − \eta (\frac{\partial F(W;x) }{\partial w} + \lambda w)<br>\]<br>Dropout alone, instead, does not have any way to prevent parameter values from becoming too large during this update phase. <strong>Moreover, the inverted implementation leads the update steps to become bigger, as showed below.</strong></p>
<h5 id="Inverted-Dropout-and-other-regularizers"><a href="#Inverted-Dropout-and-other-regularizers" class="headerlink" title="Inverted Dropout and other regularizers"></a>Inverted Dropout and other regularizers</h5><p>Since Dropout does not prevent parameters from growing and overwhelming each other, applying L2 regularization (or any other regularization technique that constraints the parameter values) can help.</p>
<p>Making explicit the scaling factor, the previous equation becomes:<br>\[<br>w \gets w − \eta (\frac{1}{q} \frac{\partial F(W;x) }{\partial w} + \lambda w)<br>\]<br>It can be easily seen that when using Inverted Dropout, the learning rate is scaled by a factor of q. q has values in [0,1].For this reason, from now on we’ll call \( q \) boosting factor because it boosts the learning rate. Moreover, we’ll call \( \frac{\eta}{q} \) the effective learning rate.</p>
<p>The effective learning rate, thus, is higher respect to the learning rate chosen: for this reason normalizations that constrain the parameter values can help to simplify the learning rate selection process.</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>Dropout exists in two versions: direct (not commonly implemented) and inverted</li>
<li>Dropout on a single neuron can be modeled using a <strong>Bernoulli</strong> random variable</li>
<li>Dropout on a set of neurons can be modeled using a <strong>Binomial</strong> random variable</li>
<li>Even if the probability of dropping exactly \( np \) neurons is low, \( np \) neurons are dropped on average on a layer of \( n \) neurons.</li>
<li>Inverted Dropout boost the learning rate</li>
<li>Inverted Dropout should be using together with other normalization techniques that constrain the parameter values in order to simplify the learning rate selection procedure</li>
<li>Dropout helps to prevent overfitting in deep neural networks.</li>
<li>Max Norm impose a constraint to the parameters size. Chosen a value for the hyper-parameter \( c \). It imposes the constraint \( |w| \leq c \).<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script></li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/DL/" rel="tag"># DL</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/08/03/TensorRT-加速-Keras-模型在-Jetson-上的推理/" rel="prev" title="TensorRT 加速 Keras 模型在 Jetson 上的推理">
      <i class="fa fa-chevron-left"></i> TensorRT 加速 Keras 模型在 Jetson 上的推理
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/08/08/OpenCV-Python-C-与Scikit-image在BGR-RGB转灰度图上的不同/" rel="next" title="OpenCV(Python & C++)与Scikit-image在BGR/RGB转灰度图上的不同">
      OpenCV(Python & C++)与Scikit-image在BGR/RGB转灰度图上的不同 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#How-Dropout-works"><span class="nav-number">1.</span> <span class="nav-text">How Dropout works</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#伯努利随机变量"><span class="nav-number">1.0.1.</span> <span class="nav-text">伯努利随机变量</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Inverted-Dropout"><span class="nav-number">2.</span> <span class="nav-text">Inverted Dropout</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout-of-a-set-of-neurons"><span class="nav-number">3.</span> <span class="nav-text">Dropout of a set of neurons</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout-amp-other-regularizers"><span class="nav-number">4.</span> <span class="nav-text">Dropout &amp; other regularizers</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Inverted-Dropout-and-other-regularizers"><span class="nav-number">4.0.1.</span> <span class="nav-text">Inverted Dropout and other regularizers</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary"><span class="nav-number">5.</span> <span class="nav-text">Summary</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Fengcun Li"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Fengcun Li</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">110</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Fengcun Li</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  











<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
