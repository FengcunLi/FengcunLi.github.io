<!DOCTYPE html>
<html lang="zh-CN">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="这篇博文的主要灵感是来自 sleebapaul 的 vanishing gradients，但是他的那篇博文存在大量的公式推导错误，因此主要借鉴了他的一些图片。
This is a discussion on an old problem that hindered the Machine Lear">
    

    <!--Author-->
    
        <meta name="author" content="Robert Lexis">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="消失的梯度"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="This is WHY."/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

        <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>消失的梯度 - This is WHY.</title>

    <!-- Bootstrap Core CSS -->
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"/>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Google Analytics -->
    


    <!-- favicon -->
    
	
</head>


<body>

    <!-- Menu -->
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">This is WHY.</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                
                    <li>
                        <a href="/">
                            
                                Home
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/archives">
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/tags">
                            
                                Tags
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/categories">
                            
                                Categories
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="https://github.com/RobertLexis">
                            
                                <i class="fa fa-github fa-stack-2x"></i>
                            
                        </a>
                    </li>
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

    <!-- Main Content -->
    <!-- Page Header -->
<!-- Set your background image for this header in your post front-matter: cover -->

<header class="intro-header" style="background-image: url('http://www.codeblocq.com/assets/projects/hexo-theme-clean-blog/img/home-bg.jpg')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>消失的梯度</h1>
                    
                    <span class="meta">
                        <!-- Date and Author -->
                        
                        
                            2018-08-13
                        
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Tags and categories -->
           
                <div class="col-lg-4 col-lg-offset-2 col-md-5 col-md-offset-1 post-tags">
                    
                        


<a href="/tags/DL/">#DL</a>


                    
                </div>
                <div class="col-lg-4 col-md-5 post-categories">
                    
                </div>
            

            <!-- Gallery -->
            

            <!-- Post Main Content -->
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <p>这篇博文的主要灵感是来自 sleebapaul 的 <a href="https://github.com/sleebapaul/vanishing_gradients" target="_blank" rel="external">vanishing gradients</a>，但是他的那篇博文存在大量的公式推导错误，因此主要借鉴了他的一些图片。</p>
<p>This is a discussion on an old problem that hindered the Machine Learning research for decades, now partially solved using various methods discovered in last 10 years. </p>
<center><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/neural_network_shallow.png" alt="neural_network_shallow"></center>

<h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>\[<br>\begin{split}<br>    Z_1 &amp; = W_1 \times X \\\\<br>    A_1 &amp; = g(Z_1) \\\\<br>    Z_2 &amp; = W_2 \times A_1 \\\\<br>    A_2 &amp; = g(Z_2) \\\\<br>    Z_3 &amp; = W_3 \times A_2 \\\\<br>    A_3 &amp; = g(Z_3) \\\\<br>    J &amp; = \sum A_3 - y<br>\end{split}<br>\]<br>\( X \) \( Z_1 \) \( A_1 \) \( Z_2 \) \( A_2 \) \( Z_3 \) \( A_3\) \( y \) 都是列向量，这样的假设不仅适用于全连接神经网络，也适用于卷积神经网络，因为卷积运算也可以展开为这样的形式，具体的可以看我的另外一篇博文<a href="https://robertlexis.github.io/2018/08/08/转置卷积-Transposed-Convolution/" target="_blank" rel="external">转置卷积 Transposed Convolution</a>。</p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>\( g \) 是激活函数，是 element-wise 地对输入进行激活的，因此其导数 \( g^\prime \) 也是 element-wise 地发挥作用的。<br>即 \( g(z) \)、\( g^\prime(z) \)、\( z \) 的 shape 是相同的。\( g^\prime(z) \) 在公式中与其他项是 element-wise 相乘（用 \( \cdot \) 表示）而不是矩阵相乘（用 \( \times \) 表示）。<br>\[<br>\begin{split}<br>    \frac{\partial J}{\partial J} &amp; = 1 \\\ <br>    \frac{\partial J}{\partial A_3} &amp; = \mathbf{1} \\\ <br>    \frac{\partial J}{\partial Z_3} &amp; = \frac{\partial J}{\partial A_3} \frac{\partial A_3}{\partial Z_3} \\\\<br>    &amp; = \mathbf{1} \cdot g^\prime(Z_3) \\\\<br>    \frac{\partial J}{\partial A_2} &amp; = \frac{\partial J}{\partial Z_3} \frac{\partial Z_3}{\partial A_2} \\\\<br>    &amp; = {W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3)) \\\\<br>    \frac{\partial J}{\partial Z_2} &amp; = \frac{\partial J}{\partial A_2} \frac{\partial A_2}{\partial Z_2} \\\\<br>    &amp; = ({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2) \\\\<br>    \frac{\partial J}{\partial A_1} &amp; = \frac{\partial J}{\partial Z_2} \frac{\partial Z_2}{\partial A_1} \\\\<br>    &amp; = {W_2}^T \times (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2)) \\\\<br>    \frac{\partial J}{\partial Z_1} &amp; = \frac{\partial J}{\partial A_1} \frac{\partial A_1}{\partial Z_1} \\\\<br>    &amp; = ({W_2}^T \times (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2))) \cdot g^\prime(Z_1) \\\\<br>    \frac{\partial J}{\partial X} &amp; = \frac{\partial J}{\partial Z_1} \frac{\partial Z_1}{\partial X} \\\\<br>    &amp; = {W_1}^T \times (({W_2}^T \times (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2))) \cdot g^\prime(Z_1)) \\\\<br>\end{split}<br>\]</p>
<p><hr><br>\[<br>\begin{split}<br>    \frac{\partial J}{\partial W_3} &amp; = \frac{\partial J}{\partial Z_3} \frac{\partial Z_3}{\partial W_3} \\\\<br>    &amp; = \frac{\partial J}{\partial Z_3} \times {A_2}^T \\\\<br>    &amp; = (\mathbf{1} \cdot g^\prime(Z_3)) \times {A_2}^T \\\\<br>    &amp; = (\mathbf{1} \cdot g^\prime(Z_3)) \times g(W_2 \times g(W_1 \times X))^T \\\\<br>    \frac{\partial J}{\partial W_2} &amp; = \frac{\partial J}{\partial Z_2} \frac{\partial Z_2}{\partial W_2} \\\\<br>    &amp; = \frac{\partial J}{\partial Z_2} \times {A_1}^T \\\\<br>    &amp; = (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2)) \times {A_1}^T \\\\<br>    &amp; = (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2)) \times {g(W_1 \times X)}^T \\\\<br>    \frac{\partial J}{\partial W_1} &amp; = \frac{\partial J}{\partial Z_1} \frac{\partial Z_1}{\partial W_1} \\\\<br>    &amp; = \frac{\partial J}{\partial Z_1} \times {X}^T \\\\<br>    &amp; = (({W_2}^T \times (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2))) \cdot g^\prime(Z_1)) \times {X}^T<br>\end{split}<br>\]<br>We call this Chain rule in calculus.<br>Now, what is essentially “back propagated”? It’s the gradients which represents the rate at which the error changes with respect to the change of parameters in each layer! 即每一层参数的变化引起误差变化的大小，每一层参数的变化对误差变化的贡献。</p>
<h3 id="梯度弥散"><a href="#梯度弥散" class="headerlink" title="梯度弥散"></a>梯度弥散</h3><center><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/sigmoid.png" alt="sigmoid"></center>

<p>\[<br>\begin{split}<br>    {0.99}^3 &amp; = 0.970299 \\\\<br>    {0.99}^7 &amp; = 0.9320653479 \\\\<br>    {0.99}^{100} &amp; = 0.36603234127 \\\\<br>    {0.99}^{300} &amp; = 0.04904089407 \\\\<br>\end{split}<br>\]<br>As we propagate away from final layer, the perturbation on network output will not reach the layers, even if reaches, the gradient will be too feeble to make an update. This issue get worsened when number of layers increases. Very deep networks often have a gradient signal that goes to zero quickly because of this reason, thus making gradient descent unbearably slow. </p>
<p>More specifically, during gradient descent, as we backprop from the final layer back to the first layer, we are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero. Gradient based methods learn a parameter’s value by understanding how a small change in the parameter’s value will affect the network’s output. If a change in the parameter’s value causes very small change in the network’s output-the network just can’t learn the parameter effectively.</p>
<center><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/vanishing_grad.png" alt="vanishing_grad"></center>

<p><small>The speed of learning decreases very rapidly for the early layers as the network trains.</small></p>
<h3 id="改进措施"><a href="#改进措施" class="headerlink" title="改进措施"></a>改进措施</h3><h5 id="Initializing-the-weights-with-standard-techniques-like-Xavier-Intialization"><a href="#Initializing-the-weights-with-standard-techniques-like-Xavier-Intialization" class="headerlink" title="Initializing the weights with standard techniques like Xavier Intialization"></a>Initializing the weights with standard techniques like Xavier Intialization</h5><p>Initializing weights properly can reduce to some extend since the fact both too high or too low weights can result in exploding/vanishing gradients.</p>
<h5 id="The-curse-of-Sigmoids-and-Tanhs"><a href="#The-curse-of-Sigmoids-and-Tanhs" class="headerlink" title="The curse of Sigmoids and Tanhs"></a>The curse of Sigmoids and Tanhs</h5><center><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/sigmoid_derivative.png" alt="sigmoid_derivative"></center><br>Let’s make some observations from these graphs.<br><br>1. Sigmoid is confined to the interval [0, 1], the output will be always between these interval for any input. If we consider to initialize the weights as big as say  [−400, 400], after the activation, that will a binay matrix. High values tend to 1 and low values to 0. This will make derivatives zero and thus vanishing gradients.<br>2. The maximum value of derivative of sigmoid function is 0.25. So by default, the input weights get \( {\frac{1}{4}}{th}\) of the original value on calculating the gradients.<br>Same are applicable to tanh which is an extended sigmoid.<br><br><strong>So the take away is don’t use them.</strong><br><br>##### Dying ReLUs<br><center><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/relu.png" alt="relu"></center>

<ol>
<li>ReLUs don’t suffer from limited interval problem like sigmoid. The minimum value is 0 but there is no limit for maximum value.</li>
<li>If the input value is less than zero, then output and derivative are zero.<br>This will eliminate all the negative inputs which is okay. But derivatives getting zero is a problem. This means, in the entire training, that neuron which can’t fire above zero will remain unlearned or dead.</li>
</ol>
<p>So, ReLU is also not the perfect solution, but provides a great relief.</p>
<h5 id="ResNets"><a href="#ResNets" class="headerlink" title="ResNets"></a>ResNets</h5><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>


                
            </div>

            <!-- Comments -->
            
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    


                </div>
            
        </div>
    </div>
</article>

    <!-- Footer -->
    <hr />

<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    

                    

                    
                        <li>
                            <a href="https://github.com/RobertLexis" target="_blank">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    

                    

                    

                    
                </ul>
                <p class="copyright text-muted">&copy; 2018 Robert Lexis<br></p>
                <p class="copyright text-muted">Original Theme <a target="_blank" href="http://startbootstrap.com/template-overviews/clean-blog/">Clean Blog</a> from <a href="http://startbootstrap.com/" target="_blank">Start Bootstrap</a></p>
                <p class="copyright text-muted">Adapted for <a target="_blank" href="https://hexo.io/">Hexo</a> by <a href="http://www.codeblocq.com/" target="_blank">Jonathan Klughertz</a></p>
            </div>
        </div>
    </div>
</footer>


    <!-- After footer scripts -->
    
<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Bootstrap -->
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->



</body>

</html>