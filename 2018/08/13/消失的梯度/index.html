<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="这篇博文的主要灵感是来自 sleebapaul 的 vanishing gradients，但是他的那篇博文存在大量的公式推导错误，因此主要借鉴了他的一些图片。 This is a discussion on an old problem that hindered the Machine Learning research for decades, now partially solved us">
<meta name="keywords" content="DL">
<meta property="og:type" content="article">
<meta property="og:title" content="消失的梯度">
<meta property="og:url" content="http://yoursite.com/2018/08/13/消失的梯度/index.html">
<meta property="og:site_name" content="The next stop - Antarctica">
<meta property="og:description" content="这篇博文的主要灵感是来自 sleebapaul 的 vanishing gradients，但是他的那篇博文存在大量的公式推导错误，因此主要借鉴了他的一些图片。 This is a discussion on an old problem that hindered the Machine Learning research for decades, now partially solved us">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/neural_network_shallow.png">
<meta property="og:image" content="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/sigmoid.png">
<meta property="og:image" content="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/vanishing_grad.png">
<meta property="og:image" content="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/sigmoid_derivative.png">
<meta property="og:image" content="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/relu.png">
<meta property="og:updated_time" content="2018-08-26T10:47:17.410Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="消失的梯度">
<meta name="twitter:description" content="这篇博文的主要灵感是来自 sleebapaul 的 vanishing gradients，但是他的那篇博文存在大量的公式推导错误，因此主要借鉴了他的一些图片。 This is a discussion on an old problem that hindered the Machine Learning research for decades, now partially solved us">
<meta name="twitter:image" content="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/neural_network_shallow.png">

<link rel="canonical" href="http://yoursite.com/2018/08/13/消失的梯度/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>消失的梯度 | The next stop - Antarctica</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">The next stop - Antarctica</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">To see behind walls. To draw closer.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/13/消失的梯度/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Fengcun Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="The next stop - Antarctica">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          消失的梯度
        </h1>

        <div class="post-meta">
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2018-08-26 18:47:17" itemprop="dateModified" datetime="2018-08-26T18:47:17+08:00">2018-08-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>这篇博文的主要灵感是来自 sleebapaul 的 <a href="https://github.com/sleebapaul/vanishing_gradients" target="_blank" rel="noopener">vanishing gradients</a>，但是他的那篇博文存在大量的公式推导错误，因此主要借鉴了他的一些图片。</p>
<p>This is a discussion on an old problem that hindered the Machine Learning research for decades, now partially solved using various methods discovered in last 10 years. </p>
<center><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/neural_network_shallow.png" alt="neural_network_shallow"></center><br><a id="more"></a><br><br>### 前向传播<br>\[<br>\begin{split}<br>    Z_1 &amp; = W_1 \times X \\<br>    A_1 &amp; = g(Z_1) \\<br>    Z_2 &amp; = W_2 \times A_1 \\<br>    A_2 &amp; = g(Z_2) \\<br>    Z_3 &amp; = W_3 \times A_2 \\<br>    A_3 &amp; = g(Z_3) \\<br>    J &amp; = \sum A_3 - y<br>\end{split}<br>\]<br>\( X \) \( Z_1 \) \( A_1 \) \( Z_2 \) \( A_2 \) \( Z_3 \) \( A_3\) \( y \) 都是列向量，这样的假设不仅适用于全连接神经网络，也适用于卷积神经网络，因为卷积运算也可以展开为这样的形式，具体的可以看我的另外一篇博文<a href="https://robertlexis.github.io/2018/08/08/转置卷积-Transposed-Convolution/" target="_blank" rel="noopener">转置卷积 Transposed Convolution</a>。<br>### 反向传播<br>\( g \) 是激活函数，是 element-wise 地对输入进行激活的，因此其导数 \( g^\prime \) 也是 element-wise 地发挥作用的。<br>即 \( g(z) \)、\( g^\prime(z) \)、\( z \) 的 shape 是相同的。\( g^\prime(z) \) 在公式中与其他项是 element-wise 相乘（用 \( \cdot \) 表示）而不是矩阵相乘（用 \( \times \) 表示）。<br>\[<br>\begin{split}<br>    \frac{\partial J}{\partial J} &amp; = 1 \\<br>    \frac{\partial J}{\partial A_3} &amp; = \mathbf{1} \\<br>    \frac{\partial J}{\partial Z_3} &amp; = \frac{\partial J}{\partial A_3} \frac{\partial A_3}{\partial Z_3} \\<br>    &amp; = \mathbf{1} \cdot g^\prime(Z_3) \\<br>    \frac{\partial J}{\partial A_2} &amp; = \frac{\partial J}{\partial Z_3} \frac{\partial Z_3}{\partial A_2} \\<br>    &amp; = {W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3)) \\<br>    \frac{\partial J}{\partial Z_2} &amp; = \frac{\partial J}{\partial A_2} \frac{\partial A_2}{\partial Z_2} \\<br>    &amp; = ({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2) \\<br>    \frac{\partial J}{\partial A_1} &amp; = \frac{\partial J}{\partial Z_2} \frac{\partial Z_2}{\partial A_1} \\<br>    &amp; = {W_2}^T \times (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2)) \\<br>    \frac{\partial J}{\partial Z_1} &amp; = \frac{\partial J}{\partial A_1} \frac{\partial A_1}{\partial Z_1} \\<br>    &amp; = ({W_2}^T \times (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2))) \cdot g^\prime(Z_1) \\<br>    \frac{\partial J}{\partial X} &amp; = \frac{\partial J}{\partial Z_1} \frac{\partial Z_1}{\partial X} \\<br>    &amp; = {W_1}^T \times (({W_2}^T \times (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2))) \cdot g^\prime(Z_1)) \\<br>\end{split}<br>\]<br><hr><br>\[<br>\begin{split}<br>    \frac{\partial J}{\partial W_3} &amp; = \frac{\partial J}{\partial Z_3} \frac{\partial Z_3}{\partial W_3} \\<br>    &amp; = \frac{\partial J}{\partial Z_3} \times {A_2}^T \\<br>    &amp; = (\mathbf{1} \cdot g^\prime(Z_3)) \times {A_2}^T \\<br>    &amp; = (\mathbf{1} \cdot g^\prime(Z_3)) \times g(W_2 \times g(W_1 \times X))^T \\<br>    \frac{\partial J}{\partial W_2} &amp; = \frac{\partial J}{\partial Z_2} \frac{\partial Z_2}{\partial W_2} \\<br>    &amp; = \frac{\partial J}{\partial Z_2} \times {A_1}^T \\<br>    &amp; = (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2)) \times {A_1}^T \\<br>    &amp; = (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2)) \times {g(W_1 \times X)}^T \\<br>    \frac{\partial J}{\partial W_1} &amp; = \frac{\partial J}{\partial Z_1} \frac{\partial Z_1}{\partial W_1} \\<br>    &amp; = \frac{\partial J}{\partial Z_1} \times {X}^T \\<br>    &amp; = (({W_2}^T \times (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2))) \cdot g^\prime(Z_1)) \times {X}^T<br>\end{split}<br>\]<br>We call this Chain rule in calculus.<br>Now, what is essentially “back propagated”? It’s the gradients which represents the rate at which the error changes with respect to the change of parameters in each layer! 即每一层参数的变化引起误差变化的大小，每一层参数的变化对误差变化的贡献。<br><br>### 梯度弥散<br><center><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/sigmoid.png" alt="sigmoid"></center>

<p>\[<br>\begin{split}<br>    {0.99}^3 &amp; = 0.970299 \\<br>    {0.99}^7 &amp; = 0.9320653479 \\<br>    {0.99}^{100} &amp; = 0.36603234127 \\<br>    {0.99}^{300} &amp; = 0.04904089407 \\<br>\end{split}<br>\]<br>As we propagate away from final layer, the perturbation on network output will not reach the layers, even if reaches, the gradient will be too feeble to make an update. This issue get worsened when number of layers increases. Very deep networks often have a gradient signal that goes to zero quickly because of this reason, thus making gradient descent unbearably slow. </p>
<p>More specifically, during gradient descent, as we backprop from the final layer back to the first layer, we are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero. Gradient based methods learn a parameter’s value by understanding how a small change in the parameter’s value will affect the network’s output. If a change in the parameter’s value causes very small change in the network’s output-the network just can’t learn the parameter effectively.</p>
<center><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/vanishing_grad.png" alt="vanishing_grad"></center>

<p><small>The speed of learning decreases very rapidly for the early layers as the network trains.</small></p>
<h3 id="改进措施"><a href="#改进措施" class="headerlink" title="改进措施"></a>改进措施</h3><h5 id="Initializing-the-weights-with-standard-techniques-like-Xavier-Intialization"><a href="#Initializing-the-weights-with-standard-techniques-like-Xavier-Intialization" class="headerlink" title="Initializing the weights with standard techniques like Xavier Intialization"></a>Initializing the weights with standard techniques like Xavier Intialization</h5><p>Initializing weights properly can reduce to some extend since the fact both too high or too low weights can result in exploding/vanishing gradients.</p>
<h5 id="The-curse-of-Sigmoids-and-Tanhs"><a href="#The-curse-of-Sigmoids-and-Tanhs" class="headerlink" title="The curse of Sigmoids and Tanhs"></a>The curse of Sigmoids and Tanhs</h5><p><center><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/sigmoid_derivative.png" alt="sigmoid_derivative"></center><br>Let’s make some observations from these graphs.</p>
<ol>
<li>Sigmoid is confined to the interval [0, 1], the output will be always between these interval for any input. If we consider to initialize the weights as big as say  [−400, 400], after the activation, that will a binay matrix. High values tend to 1 and low values to 0. This will make derivatives zero and thus vanishing gradients.</li>
<li>The maximum value of derivative of sigmoid function is 0.25. So by default, the input weights get \( {\frac{1}{4}}{th}\) of the original value on calculating the gradients.<br>Same are applicable to tanh which is an extended sigmoid.<br><strong>So the take away is don’t use them.</strong></li>
</ol>
<h5 id="Dying-ReLUs"><a href="#Dying-ReLUs" class="headerlink" title="Dying ReLUs"></a>Dying ReLUs</h5><p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/relu.png" alt="relu"></p>
<ol>
<li>ReLUs don’t suffer from limited interval problem like sigmoid. The minimum value is 0 but there is no limit for maximum value.</li>
<li>If the input value is less than zero, then output and derivative are zero.<br>This will eliminate all the negative inputs which is okay. But derivatives getting zero is a problem. This means, in the entire training, that neuron which can’t fire above zero will remain unlearned or dead.</li>
</ol>
<p>So, ReLU is also not the perfect solution, but provides a great relief.</p>
<h5 id="ResNets"><a href="#ResNets" class="headerlink" title="ResNets"></a>ResNets</h5><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/DL/" rel="tag"># DL</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/08/11/MobileNet-V1-and-V2-带来的卷积结构革命/" rel="prev" title="MobileNet V1 and V2 带来的卷积结构革命">
      <i class="fa fa-chevron-left"></i> MobileNet V1 and V2 带来的卷积结构革命
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/08/26/目标定位-vs-目标检测/" rel="next" title="目标定位 vs 目标检测">
      目标定位 vs 目标检测 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#改进措施"><span class="nav-number">1.</span> <span class="nav-text">改进措施</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Initializing-the-weights-with-standard-techniques-like-Xavier-Intialization"><span class="nav-number">1.0.1.</span> <span class="nav-text">Initializing the weights with standard techniques like Xavier Intialization</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#The-curse-of-Sigmoids-and-Tanhs"><span class="nav-number">1.0.2.</span> <span class="nav-text">The curse of Sigmoids and Tanhs</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Dying-ReLUs"><span class="nav-number">1.0.3.</span> <span class="nav-text">Dying ReLUs</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ResNets"><span class="nav-number">1.0.4.</span> <span class="nav-text">ResNets</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Fengcun Li"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Fengcun Li</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">110</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Fengcun Li</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  











<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
