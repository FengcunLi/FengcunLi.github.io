<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="这篇博文的主要灵感是来自 sleebapaul 的 vanishing gradients，但是他的那篇博文存在大量的公式推导错误，因此主要借鉴了他的一些图片。
This is a discussion on an old problem that hindered the Machine Learning research for decades, now partially solved using various methods discovered in last 10 years.">
    

    <!--Author-->
    
        <meta name="author" content="Robert Lexis">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="消失的梯度"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="This is Robert Lexis."/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>消失的梯度 - This is Robert Lexis.</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
    </div>
</header>

        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/2018/08/13/消失的梯度/">
                消失的梯度
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-08-13</span>
            
            
            
        </div>
    </div>

    <div class="content">

        <!-- Gallery -->
        

        <!-- Post Content -->
        <p>这篇博文的主要灵感是来自 sleebapaul 的 <a href="https://github.com/sleebapaul/vanishing_gradients" target="_blank" rel="external">vanishing gradients</a>，但是他的那篇博文存在大量的公式推导错误，因此主要借鉴了他的一些图片。</p>
<p>This is a discussion on an old problem that hindered the Machine Learning research for decades, now partially solved using various methods discovered in last 10 years. </p>
<center><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/neural_network_shallow.png" alt="neural_network_shallow"></center><br><a id="more"></a><br><br>### 前向传播<br>\[<br>\begin{split}<br>    Z_1 &amp; = W_1 \times X \\\\<br>    A_1 &amp; = g(Z_1) \\\\<br>    Z_2 &amp; = W_2 \times A_1 \\\\<br>    A_2 &amp; = g(Z_2) \\\\<br>    Z_3 &amp; = W_3 \times A_2 \\\\<br>    A_3 &amp; = g(Z_3) \\\\<br>    J &amp; = \sum A_3 - y<br>\end{split}<br>\]<br>\( X \) \( Z_1 \) \( A_1 \) \( Z_2 \) \( A_2 \) \( Z_3 \) \( A_3\) \( y \) 都是列向量，这样的假设不仅适用于全连接神经网络，也适用于卷积神经网络，因为卷积运算也可以展开为这样的形式，具体的可以看我的另外一篇博文<a href="https://robertlexis.github.io/2018/08/08/转置卷积-Transposed-Convolution/" target="_blank" rel="external">转置卷积 Transposed Convolution</a>。<br>### 反向传播<br>\( g \) 是激活函数，是 element-wise 地对输入进行激活的，因此其导数 \( g^\prime \) 也是 element-wise 地发挥作用的。<br>即 \( g(z) \)、\( g^\prime(z) \)、\( z \) 的 shape 是相同的。\( g^\prime(z) \) 在公式中与其他项是 element-wise 相乘（用 \( \cdot \) 表示）而不是矩阵相乘（用 \( \times \) 表示）。<br>\[<br>\begin{split}<br>    \frac{\partial J}{\partial J} &amp; = 1 \\\ <br>    \frac{\partial J}{\partial A_3} &amp; = \mathbf{1} \\\ <br>    \frac{\partial J}{\partial Z_3} &amp; = \frac{\partial J}{\partial A_3} \frac{\partial A_3}{\partial Z_3} \\\\<br>    &amp; = \mathbf{1} \cdot g^\prime(Z_3) \\\\<br>    \frac{\partial J}{\partial A_2} &amp; = \frac{\partial J}{\partial Z_3} \frac{\partial Z_3}{\partial A_2} \\\\<br>    &amp; = {W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3)) \\\\<br>    \frac{\partial J}{\partial Z_2} &amp; = \frac{\partial J}{\partial A_2} \frac{\partial A_2}{\partial Z_2} \\\\<br>    &amp; = ({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2) \\\\<br>    \frac{\partial J}{\partial A_1} &amp; = \frac{\partial J}{\partial Z_2} \frac{\partial Z_2}{\partial A_1} \\\\<br>    &amp; = {W_2}^T \times (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2)) \\\\<br>    \frac{\partial J}{\partial Z_1} &amp; = \frac{\partial J}{\partial A_1} \frac{\partial A_1}{\partial Z_1} \\\\<br>    &amp; = ({W_2}^T \times (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2))) \cdot g^\prime(Z_1) \\\\<br>    \frac{\partial J}{\partial X} &amp; = \frac{\partial J}{\partial Z_1} \frac{\partial Z_1}{\partial X} \\\\<br>    &amp; = {W_1}^T \times (({W_2}^T \times (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2))) \cdot g^\prime(Z_1)) \\\\<br>\end{split}<br>\]<br><hr><br>\[<br>\begin{split}<br>    \frac{\partial J}{\partial W_3} &amp; = \frac{\partial J}{\partial Z_3} \frac{\partial Z_3}{\partial W_3} \\\\<br>    &amp; = \frac{\partial J}{\partial Z_3} \times {A_2}^T \\\\<br>    &amp; = (\mathbf{1} \cdot g^\prime(Z_3)) \times {A_2}^T \\\\<br>    &amp; = (\mathbf{1} \cdot g^\prime(Z_3)) \times g(W_2 \times g(W_1 \times X))^T \\\\<br>    \frac{\partial J}{\partial W_2} &amp; = \frac{\partial J}{\partial Z_2} \frac{\partial Z_2}{\partial W_2} \\\\<br>    &amp; = \frac{\partial J}{\partial Z_2} \times {A_1}^T \\\\<br>    &amp; = (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2)) \times {A_1}^T \\\\<br>    &amp; = (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2)) \times {g(W_1 \times X)}^T \\\\<br>    \frac{\partial J}{\partial W_1} &amp; = \frac{\partial J}{\partial Z_1} \frac{\partial Z_1}{\partial W_1} \\\\<br>    &amp; = \frac{\partial J}{\partial Z_1} \times {X}^T \\\\<br>    &amp; = (({W_2}^T \times (({W_3}^T \times (\mathbf{1} \cdot g^\prime(Z_3))) \cdot g^\prime(Z_2))) \cdot g^\prime(Z_1)) \times {X}^T<br>\end{split}<br>\]<br>We call this Chain rule in calculus.<br>Now, what is essentially “back propagated”? It’s the gradients which represents the rate at which the error changes with respect to the change of parameters in each layer! 即每一层参数的变化引起误差变化的大小，每一层参数的变化对误差变化的贡献。<br><br>### 梯度弥散<br><center><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/sigmoid.png" alt="sigmoid"></center>

<p>\[<br>\begin{split}<br>    {0.99}^3 &amp; = 0.970299 \\\\<br>    {0.99}^7 &amp; = 0.9320653479 \\\\<br>    {0.99}^{100} &amp; = 0.36603234127 \\\\<br>    {0.99}^{300} &amp; = 0.04904089407 \\\\<br>\end{split}<br>\]<br>As we propagate away from final layer, the perturbation on network output will not reach the layers, even if reaches, the gradient will be too feeble to make an update. This issue get worsened when number of layers increases. Very deep networks often have a gradient signal that goes to zero quickly because of this reason, thus making gradient descent unbearably slow. </p>
<p>More specifically, during gradient descent, as we backprop from the final layer back to the first layer, we are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero. Gradient based methods learn a parameter’s value by understanding how a small change in the parameter’s value will affect the network’s output. If a change in the parameter’s value causes very small change in the network’s output-the network just can’t learn the parameter effectively.</p>
<center><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/vanishing_grad.png" alt="vanishing_grad"></center>

<p><small>The speed of learning decreases very rapidly for the early layers as the network trains.</small></p>
<h3 id="改进措施"><a href="#改进措施" class="headerlink" title="改进措施"></a>改进措施</h3><h5 id="Initializing-the-weights-with-standard-techniques-like-Xavier-Intialization"><a href="#Initializing-the-weights-with-standard-techniques-like-Xavier-Intialization" class="headerlink" title="Initializing the weights with standard techniques like Xavier Intialization"></a>Initializing the weights with standard techniques like Xavier Intialization</h5><p>Initializing weights properly can reduce to some extend since the fact both too high or too low weights can result in exploding/vanishing gradients.</p>
<h5 id="The-curse-of-Sigmoids-and-Tanhs"><a href="#The-curse-of-Sigmoids-and-Tanhs" class="headerlink" title="The curse of Sigmoids and Tanhs"></a>The curse of Sigmoids and Tanhs</h5><p><center><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/sigmoid_derivative.png" alt="sigmoid_derivative"></center><br>Let’s make some observations from these graphs.</p>
<ol>
<li>Sigmoid is confined to the interval [0, 1], the output will be always between these interval for any input. If we consider to initialize the weights as big as say  [−400, 400], after the activation, that will a binay matrix. High values tend to 1 and low values to 0. This will make derivatives zero and thus vanishing gradients.</li>
<li>The maximum value of derivative of sigmoid function is 0.25. So by default, the input weights get \( {\frac{1}{4}}{th}\) of the original value on calculating the gradients.<br>Same are applicable to tanh which is an extended sigmoid.<br><strong>So the take away is don’t use them.</strong></li>
</ol>
<h5 id="Dying-ReLUs"><a href="#Dying-ReLUs" class="headerlink" title="Dying ReLUs"></a>Dying ReLUs</h5><p><img src="http://oytnj8g2y.bkt.clouddn.com/image/jpg/vanishing_grad/relu.png" alt="relu"></p>
<ol>
<li>ReLUs don’t suffer from limited interval problem like sigmoid. The minimum value is 0 but there is no limit for maximum value.</li>
<li>If the input value is less than zero, then output and derivative are zero.<br>This will eliminate all the negative inputs which is okay. But derivatives getting zero is a problem. This means, in the entire training, that neuron which can’t fire above zero will remain unlearned or dead.</li>
</ol>
<p>So, ReLU is also not the perfect solution, but provides a great relief.</p>
<h5 id="ResNets"><a href="#ResNets" class="headerlink" title="ResNets"></a>ResNets</h5><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    </div>

    

    
        <div class="post-tags">
            <i class="fa fa-tags" aria-hidden="true"></i>
            <a href="/tags/DL/">#DL</a>
        </div>
    

    <!-- Comments -->
    

</div>
        </section>

    </div>
</div>


</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This is Robert Lexis (FengCun Li). To see the world, things dangerous to come to, to see behind walls, to draw closer, to find each other and to feel. That is the purpose of LIFE.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2018/09/03/Optimizers/">Optimizers</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/28/Tensorflow-模型浮点数计算量和参数量统计/">TensorFlow 模型浮点数计算量和参数量统计</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/26/YOLO/">YOLO</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/26/非最大值抑制/">非最大值抑制</a>
            </li>
            
        </ul>
    </div>



            
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/RobertLexis">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:robert_lexis@163.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Robert Lexis Loves Wenny
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



</body>

</html>