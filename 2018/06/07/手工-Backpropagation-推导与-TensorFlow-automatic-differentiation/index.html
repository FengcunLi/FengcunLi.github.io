<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="主要对比梯度反向传播手工推导与TensorFLow 自动差分关于梯度反向传播手工推导的学习可以参考斯坦福大学的 CS231n: Convolutional Neural Networks for Visual Recognition 的 Lecture 4。下面通过代码对这两者进行对比，代码的Jupyter NoteBook 可以在我的 GitHub 仓库中找到。
imports123import osos.environ[&amp;apos;TF_CPP_MIN_LOG_LEVEL&amp;apos;]=&amp;apos;3&amp;apos;import tensorflow as tf
定义变量123with tf.variable_scope(&amp;quot;params&amp;quot;, reuse=tf.AUTO_REUSE):    w = tf.get_variable(&amp;quot;w&amp;quot;, initializer=tf.constant([2.0]))    b = tf.get_variable(&amp;quot;b&amp;quot;, initializer=tf.constant([0.0]))
定义占位符12x = tf.placeholder(tf.float32, shape=[None], name=&amp;quot;x&amp;quot;)y = tf.placeholder(tf.float32, shape=[None], name=&amp;quot;y&amp;quot;)
定义表达式1y_pred = w*x + b
定义代价函数1loss = tf.reduce_mean(tf.square(y_pred - y))
定义优化器123optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)grads_and_vars = optimizer.compute_gradients(loss)train_op = optimizer.apply_gradients(grads_and_vars)
创建会话并初始化变量1234init_op = tf.global_variables_initializer()sess = tf.InteractiveSession()sess.run(init_op)
根据函数 y = 4x + 3 给出训练数据对于单个数据点输入，对比 TensorFlow automatic differentiation 与手工推导123# y_ = 4 * 2 + 3x_ = [2]y_ = [11] # y_ = 4 * 2 + 3
手工推导 Backpropagation 过程">
    

    <!--Author-->
    
        <meta name="author" content="Robert Lexis">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="手工 Backpropagation 推导与 TensorFlow automatic differentiation"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="This is Robert Lexis."/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>手工 Backpropagation 推导与 TensorFlow automatic differentiation - This is Robert Lexis.</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
    </div>
</header>

        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/2018/06/07/手工-Backpropagation-推导与-TensorFlow-automatic-differentiation/">
                手工 Backpropagation 推导与 TensorFlow automatic differentiation
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-06-07</span>
            
            
            
        </div>
    </div>

    <div class="content">

        <!-- Gallery -->
        

        <!-- Post Content -->
        <h3 id="主要对比梯度反向传播手工推导与TensorFLow-自动差分"><a href="#主要对比梯度反向传播手工推导与TensorFLow-自动差分" class="headerlink" title="主要对比梯度反向传播手工推导与TensorFLow 自动差分"></a>主要对比梯度反向传播手工推导与TensorFLow 自动差分</h3><p>关于梯度反向传播手工推导的学习可以参考斯坦福大学的 <a href="http://cs231n.stanford.edu/" target="_blank" rel="external">CS231n: Convolutional Neural Networks for Visual Recognition</a> 的 Lecture 4。<br>下面通过代码对这两者进行对比，代码的Jupyter NoteBook 可以在我的 <a href="https://github.com/RobertLexis/TensorFlow-automatic-differentiation" target="_blank" rel="external">GitHub 仓库</a>中找到。</p>
<h4 id="imports"><a href="#imports" class="headerlink" title="imports"></a>imports</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">import os</div><div class="line">os.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;]=&apos;3&apos;</div><div class="line">import tensorflow as tf</div></pre></td></tr></table></figure>
<h4 id="定义变量"><a href="#定义变量" class="headerlink" title="定义变量"></a>定义变量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">with tf.variable_scope(&quot;params&quot;, reuse=tf.AUTO_REUSE):</div><div class="line">    w = tf.get_variable(&quot;w&quot;, initializer=tf.constant([2.0]))</div><div class="line">    b = tf.get_variable(&quot;b&quot;, initializer=tf.constant([0.0]))</div></pre></td></tr></table></figure>
<h4 id="定义占位符"><a href="#定义占位符" class="headerlink" title="定义占位符"></a>定义占位符</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = tf.placeholder(tf.float32, shape=[None], name=&quot;x&quot;)</div><div class="line">y = tf.placeholder(tf.float32, shape=[None], name=&quot;y&quot;)</div></pre></td></tr></table></figure>
<h4 id="定义表达式"><a href="#定义表达式" class="headerlink" title="定义表达式"></a>定义表达式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">y_pred = w*x + b</div></pre></td></tr></table></figure>
<h4 id="定义代价函数"><a href="#定义代价函数" class="headerlink" title="定义代价函数"></a>定义代价函数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loss = tf.reduce_mean(tf.square(y_pred - y))</div></pre></td></tr></table></figure>
<h4 id="定义优化器"><a href="#定义优化器" class="headerlink" title="定义优化器"></a>定义优化器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)</div><div class="line">grads_and_vars = optimizer.compute_gradients(loss)</div><div class="line">train_op = optimizer.apply_gradients(grads_and_vars)</div></pre></td></tr></table></figure>
<h4 id="创建会话并初始化变量"><a href="#创建会话并初始化变量" class="headerlink" title="创建会话并初始化变量"></a>创建会话并初始化变量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">init_op = tf.global_variables_initializer()</div><div class="line"></div><div class="line">sess = tf.InteractiveSession()</div><div class="line">sess.run(init_op)</div></pre></td></tr></table></figure>
<h4 id="根据函数-y-4x-3-给出训练数据"><a href="#根据函数-y-4x-3-给出训练数据" class="headerlink" title="根据函数 y = 4x + 3 给出训练数据"></a>根据函数 y = 4x + 3 给出训练数据</h4><h4 id="对于单个数据点输入，对比-TensorFlow-automatic-differentiation-与手工推导"><a href="#对于单个数据点输入，对比-TensorFlow-automatic-differentiation-与手工推导" class="headerlink" title="对于单个数据点输入，对比 TensorFlow automatic differentiation 与手工推导"></a>对于单个数据点输入，对比 TensorFlow automatic differentiation 与手工推导</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># y_ = 4 * 2 + 3</div><div class="line">x_ = [2]</div><div class="line">y_ = [11] # y_ = 4 * 2 + 3</div></pre></td></tr></table></figure>
<h6 id="手工推导-Backpropagation-过程"><a href="#手工推导-Backpropagation-过程" class="headerlink" title="手工推导 Backpropagation 过程"></a>手工推导 Backpropagation 过程</h6><p><img src="http://oytnj8g2y.bkt.clouddn.com/backwards/backward_1.png" alt="backward_1"><br><a id="more"></a></p>
<h6 id="用-TensorFlow-计算-loss-及梯度"><a href="#用-TensorFlow-计算-loss-及梯度" class="headerlink" title="用 TensorFlow 计算 loss 及梯度"></a>用 TensorFlow 计算 loss 及梯度</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sess.run([loss, grads_and_vars], feed_dict=&#123;x: x_, y: y_&#125;)</div></pre></td></tr></table></figure>
<p>Output:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[49.0,</div><div class="line"> [(array([-28.], dtype=float32), array([2.], dtype=float32)),</div><div class="line">  (array([-14.], dtype=float32), array([0.], dtype=float32))]]</div></pre></td></tr></table></figure></p>
<h4 id="对于-batch-输入，对比-TensorFlow-automatic-differentiation-与手工推导"><a href="#对于-batch-输入，对比-TensorFlow-automatic-differentiation-与手工推导" class="headerlink" title="对于 batch 输入，对比 TensorFlow automatic differentiation 与手工推导"></a>对于 batch 输入，对比 TensorFlow automatic differentiation 与手工推导</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x_ = [2, 3]</div><div class="line">y_ = [11, 15]</div></pre></td></tr></table></figure>
<p><img src="http://oytnj8g2y.bkt.clouddn.com/backwards/backward_1.png" alt="backward_1"><br><img src="http://oytnj8g2y.bkt.clouddn.com/backwards/backward_2.png" alt="backward_2"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sess.run([loss, grads_and_vars], feed_dict=&#123;x: x_, y: y_&#125;)</div></pre></td></tr></table></figure>
<p>Output:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[65.0,</div><div class="line"> [(array([-41.], dtype=float32), array([2.], dtype=float32)),</div><div class="line">  (array([-16.], dtype=float32), array([0.], dtype=float32))]]</div></pre></td></tr></table></figure></p>
<h6 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">65 == (81+49)/2</div><div class="line">-41 == (-28-54)/2</div><div class="line">-16 == (-14-18)/2</div></pre></td></tr></table></figure>
<p>可以看出 TensorFlow 计算出的 loss 是两次 loss 的均值，两个变量上的梯度也是各自两次梯度值的均值。</p>
<h4 id="以-learning-rate-为步幅对-w，b进行一次更新"><a href="#以-learning-rate-为步幅对-w，b进行一次更新" class="headerlink" title="以 learning_rate 为步幅对 w，b进行一次更新"></a>以 learning_rate 为步幅对 w，b进行一次更新</h4><h6 id="手工计算"><a href="#手工计算" class="headerlink" title="手工计算"></a>手工计算</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">2 - 0.001 * (-41) = 2.041</div><div class="line">0 - 0.001 * (-16) = 0.016</div></pre></td></tr></table></figure>
<h6 id="TensorFlow-计算结果"><a href="#TensorFlow-计算结果" class="headerlink" title="TensorFlow 计算结果"></a>TensorFlow 计算结果</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div></pre></td></tr></table></figure>
<p>Output:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[array([2.041], dtype=float32), array([0.016], dtype=float32)]</div></pre></td></tr></table></figure></p>
<h4 id="下面对模型进行训练并观察-w，b-的变化过程"><a href="#下面对模型进行训练并观察-w，b-的变化过程" class="headerlink" title="下面对模型进行训练并观察 w，b 的变化过程"></a>下面对模型进行训练并观察 w，b 的变化过程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">for _ in range(1000):</div><div class="line">    sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div><div class="line">[array([4.700393], dtype=float32), array([1.1883683], dtype=float32)]</div><div class="line"></div><div class="line"></div><div class="line">for _ in range(1000):</div><div class="line">    sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div><div class="line">[array([4.6550403], dtype=float32), array([1.3056908], dtype=float32)]</div><div class="line"></div><div class="line"></div><div class="line">for _ in range(1000):</div><div class="line">    sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div><div class="line">[array([4.6126213], dtype=float32), array([1.4154125], dtype=float32)]</div><div class="line"></div><div class="line"></div><div class="line">for _ in range(10000):</div><div class="line">    sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div><div class="line">[array([4.3136806], dtype=float32), array([2.1886632], dtype=float32)]</div><div class="line"></div><div class="line"></div><div class="line">for _ in range(10000):</div><div class="line">    sess.run(train_op, feed_dict=&#123;x: x_, y: y_&#125;)</div><div class="line">sess.run([w, b])</div><div class="line">[array([4.1606426], dtype=float32), array([2.5844746], dtype=float32)]</div></pre></td></tr></table></figure>
<p>可以看出随着训练步骤的增多，w，b 逐渐逼近目标值 4， 3</p>
<h4 id="关闭会话，释放资源"><a href="#关闭会话，释放资源" class="headerlink" title="关闭会话，释放资源"></a>关闭会话，释放资源</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sess.close()</div></pre></td></tr></table></figure>
<h3 id="Caveats"><a href="#Caveats" class="headerlink" title="Caveats"></a>Caveats</h3><p>在上面我们定义的loss是如下的均值（0-d Tensor/scalar/shape ()）形式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loss = tf.reduce_mean(tf.square(y_pred - y))</div></pre></td></tr></table></figure></p>
<p>假设我们没有进行 <code>reduce_mean</code>，即 loss 是如下的形式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loss = tf.square(y_pred - y)</div></pre></td></tr></table></figure></p>
<p>当 <code>x_ = [2, 3]</code>、<code>y_ = [11, 15]</code>时，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sess.run([loss, grads_and_vars], feed_dict=&#123;x: x_, y: y_&#125;)</div></pre></td></tr></table></figure></p>
<p>输出为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[array([49., 81.], dtype=float32),</div><div class="line"> [(array([-82.], dtype=float32), array([2.], dtype=float32)),</div><div class="line">  (array([-32.], dtype=float32), array([0.], dtype=float32))]]</div></pre></td></tr></table></figure></p>
<p>有两个 loss 值分别为49 和 81，和我们手工计算出的值是一致的，而两个变量的梯度为各自的两次梯度值的加和。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">-82 == (-28) + (-54)</div><div class="line">-32 == (-14) + (-18)</div></pre></td></tr></table></figure></p>
<p>即有两个 loss，相当于更新了两次。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>Loss 仅仅是一个度量指标，其均值是对模型性能的一个更有意义的评价，是为此而求的均值，这个均值也仅仅用于模型性能的评价，其具体数值并不实际参与到反向传播更新各个参数的过程。loss 的具体数值意义不大，因为反向传播最起始的upstream gradient总是1。<br>这也就解释了我之前遇到过的一个问题，为什么单纯给优化器传入一个loss 的数值，TF 会报错，无法计算梯度，因为真正重要的不是这个末端的loss 数值，而是在计算这个末端loss 数值的过程中的每一个中间值及涉及到的操作类型（add mul max sub square ）。利用这些中间值和操作类型结合，反向进行梯度的传播。</li>
<li>对于batch 样本输入，正确的 loss 定义计算的是各个样本上loss 的均值，各个参数上的梯度也是各自在各个样本上梯度值的均值，可以这样看，各个样本单独输入求loss ，求梯度，然后再在各个样本的结果上进行平均。</li>
</ul>
<h3 id="Reminder-from-cs231n"><a href="#Reminder-from-cs231n" class="headerlink" title="Reminder from cs231n"></a>Reminder from cs231n</h3><table>
<thead>
<tr>
<th>Operation</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>add</td>
<td>distributor</td>
</tr>
<tr>
<td>max</td>
<td>router</td>
</tr>
<tr>
<td>mul</td>
<td>switcher</td>
</tr>
</tbody>
</table>

    </div>

    

    
        <div class="post-tags">
            <i class="fa fa-tags" aria-hidden="true"></i>
            <a href="/tags/DL/">#DL</a>
        </div>
    

    <!-- Comments -->
    

</div>
        </section>

    </div>
</div>


</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This is Robert Lexis (FengCun Li). To see the world, things dangerous to come to, to see behind walls, to draw closer, to find each other and to feel. That is the purpose of LIFE.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2019/03/18/Emplace-back/">Emplace back</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2019/03/14/Perfect-forward/">Perfect forward</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2019/03/14/Move-semantic-perfect-forward/">Move semantic &amp; perfect f</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2019/03/14/RAII-smart-pointer/">RAII &amp; smart pointer</a>
            </li>
            
        </ul>
    </div>



            
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/RobertLexis">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:robert_lexis@163.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Robert Lexis Loves Wenny
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



</body>

</html>