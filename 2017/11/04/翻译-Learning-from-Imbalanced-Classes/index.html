<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="在参加数据挖掘的比赛时，遇到了类别不平衡的问题，在着手解决这个问题的过程中发现了来自 Silicon Valley Data Science(svds) TOM FAWCETT 的一篇博客 Learning from Imbalanced Classes 写的着实精彩，因此决定将其翻译过来。
Learning from Imbalanced Classes如果你刚刚上完一门机器学习的课程，你使用的大部分数据集可能是很简单的。当你拟合分类器时，样本的类别是平衡的，也就是说这些类别的样本量大体相等。导师通常是使用清洗过的数据集进行教学，从而能够集中精力在讲解特定的算法和技术上。通常你看到的样本是下面这样的，点代表样本，不同的颜色或者形状代表不同的类型。">
    

    <!--Author-->
    
        <meta name="author" content="Robert Lexis">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="翻译 Learning from Imbalanced Classes"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="This is Robert Lexis."/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>翻译 Learning from Imbalanced Classes - This is Robert Lexis.</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
    </div>
</header>

        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/2017/11/04/翻译-Learning-from-Imbalanced-Classes/">
                翻译 Learning from Imbalanced Classes
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2017-11-04</span>
            
            
            
        </div>
    </div>

    <div class="content">

        <!-- Gallery -->
        

        <!-- Post Content -->
        <p>在参加数据挖掘的比赛时，遇到了类别不平衡的问题，在着手解决这个问题的过程中发现了来自 <a href="https://www.svds.com/" target="_blank" rel="external">Silicon Valley Data Science(svds)</a> <a href="https://www.svds.com/team/tom-fawcett/" target="_blank" rel="external">TOM FAWCETT</a> 的一篇博客 <a href="https://www.svds.com/learning-imbalanced-classes/" target="_blank" rel="external">Learning from Imbalanced Classes</a> 写的着实精彩，因此决定将其翻译过来。</p>
<h2 id="Learning-from-Imbalanced-Classes"><a href="#Learning-from-Imbalanced-Classes" class="headerlink" title="Learning from Imbalanced Classes"></a>Learning from Imbalanced Classes</h2><p>如果你刚刚上完一门机器学习的课程，你使用的大部分数据集可能是很简单的。当你拟合分类器时，样本的类别是平衡的，也就是说这些类别的样本量大体相等。导师通常是使用清洗过的数据集进行教学，从而能够集中精力在讲解特定的算法和技术上。通常你看到的样本是下面这样的，点代表样本，不同的颜色或者形状代表不同的类型。<br><img src="http://oytnj8g2y.bkt.clouddn.com/clean.png" alt="clean.png"></p>
<a id="more"></a>
<p>分类算法的目的是尝试学习出一个分类器能够区分这两者。基于不同的数学、统计或者几何假设，会有很多不同的算法。<br><img src="http://oytnj8g2y.bkt.clouddn.com/ImbalancedClasses_fig2.png" alt="ImbalancedClasses_fig2.png"><br>但是当你接触到真实的未清洗的数据时，你会注意到的第一件事情就是数据会含有很多的噪声，并且类别不平衡。真实数据的散点图可能是下面这样的：<br><img src="http://oytnj8g2y.bkt.clouddn.com/messy.png" alt="messy.png"><br>这个数据的主要问题是类别不平衡，红色点几乎被蓝色点淹没了。<br>学术界在研究不平衡类别问题时少数类样本量会占到10%到20%。但是真实数据集会比这更加失衡。下面是几个这样的例子：</p>
<ol>
<li>每年有大约2%的信用卡遭到欺诈（大部分的欺诈检测领域是严重失衡的） [1][1] 。</li>
<li>对疾病进行医学筛查通常是在没有疾病的大规模人群中进行的，检测出的患者是很少的一部分。</li>
<li>硬盘的坏损率每年大概是~1%。</li>
<li>在线广告的转化率大约在 $10_-3$ 到 $10_-6$ 之间。</li>
<li>工厂产品的瑕疵率大约0.1%。</li>
</ol>
<p>这些领域都是类别不平衡的，我称之为“稻草堆中的针”问题，在这样的问题中，分类器需要从大量的负样本中找到少量的正样本。当你遇到这样的问题时，使用标准的算法会遇到问题。传统的算法通常会偏向多数类，因为它们的损失函数是为优化像错误率这样的指标而设计的，并没有考虑进数据分布[2][2]。在极坏的情况下，少数类会被当作多数类的异常值而被忽略，学习算法会产生一个将所有样本分类为多数类的没有价值的分类器。</p>
<p>这看起来是一个病态行为，但是实际上并不是，特别是当你的目标是最大化最简单的指标准确率时，此时忽略少数类就是一个可以被接受的方式。但是如果我们假设分类出少数类样本才是更重要的，我们就必须更加小心地处理这个类别不平衡问题了。</p>
<h3 id="处理类别不平衡数据"><a href="#处理类别不平衡数据" class="headerlink" title="处理类别不平衡数据"></a>处理类别不平衡数据</h3><p>从类别不平衡数据中学习是一个研究了很久的问题。人们尝试了大量的技术，有或好或坏的结果，并没有一个清晰的答案。第一次遇到这个问题的数据科学家们通常的第一个问题是“数据类别不平衡时我该怎么办？”，但是这就像“哪一个学习算法才是最好的一样”，没有准确的答案，需要根据数据而定。下面大概依据实施的工作量给出了一个提纲：</p>
<ul>
<li>什么也不做。有的时候，你很幸运，什么也不用做，直接在原始自然（或者称之为分层）分布的数据上进行训练。</li>
<li>对训练集进行平衡：<ul>
<li>过采样少数类样本</li>
<li>下采样多数类样本</li>
<li>生成新的少数类样本</li>
</ul>
</li>
<li>丢弃少数类样本，采用异常检测框架</li>
<li>在算法层面，或者在算法之后：<ul>
<li>调整类别的权重（错误分类的代价）</li>
<li>调整决策的门限</li>
<li>调整算法使其对少数类更加的敏感</li>
</ul>
</li>
<li>构建全新的算法</li>
</ul>
<h3 id="离题：评估的-dos-与-don’ts"><a href="#离题：评估的-dos-与-don’ts" class="headerlink" title="离题：评估的 dos 与 don’ts"></a>离题：评估的 dos 与 don’ts</h3><p>在谈论如何使用类别不平衡的数据训练分类器之前，先讨论一下如何正确的评估一个分类器。这一点如何强调也不足为过。只有考核正确的指标你才能取得进步。</p>
<ol>
<li>不要使用准确率（或者错误率）去评估你的分类器。因为这有两大缺陷。准确率朴素地应用了一个 0.50 的门限去决定样本到底属于哪一类，这在类别不平衡时通常是错误的。第二点是，分类准确率仅仅是基于错误分类的计数，你应该知道的更多，比如哪些类经常被混淆，在评分曲线的高的一端，低的一端，还是整个曲线上都被混淆着。如果你不懂这些点，你可以去看一下这篇博客 <a href="http://www.svds.com/classifiers2/" target="_blank" rel="external">The Basics of Classifier Evaluation, Part 2</a>。你应该使用 ROC 曲线，precision-recall 曲线，lift 曲线，或者 profit（gain）曲线去可视化分类器的性能。<br><img src="http://oytnj8g2y.bkt.clouddn.com/Sample-ROC.png" alt="Sample-ROC"><br>ROC curve<br><img src="http://oytnj8g2y.bkt.clouddn.com/Sample-PR-1.png" alt="Sample-PR"><br>PR curve</li>
<li>不要使用<code>score</code>[3][3]或者<code>predict</code>从分类器得到硬的分类标签，而应该使用<code>proba</code>或者<code>predict_proba</code>得到概率估计。</li>
<li>当你得到概率估计，不要盲目地使用一个 0.50 的决策门限来区分类别。看一下性能曲线来决定一个合适的门限。</li>
<li>不管你在训练时做了什么，一定要在自然（分层）的分布上测试。看一下<code>sklearn.cross_validation.StratifiedKFold</code>。</li>
<li>你可以不使用概率估计，但是如果你需要它们，使用校准。看一下<code>sklearn.calibration.CalibratedClassifierCV</code>。</li>
</ol>
<p>上面展示的二维图表要比一个准确率数值含有更多的信息。但是如果必须要一个数字度量，下面的指标比准确率更可取。</p>
<ol>
<li><a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve" target="_blank" rel="external">The Area Under the ROC curve (AUC)</a> is a good general statistic. It is equal to the probability that a random positive example will be ranked above a random negative example.</li>
<li>The <a href="https://en.wikipedia.org/wiki/F1_score" target="_blank" rel="external">F1 Score</a> is the harmonic mean of precision and recall. It is commonly used in text processing when an aggregate measure is sought.</li>
<li><a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa" target="_blank" rel="external">Cohen’s Kappa</a> is an evaluation statistic that takes into account how much agreement would be expected by chance.</li>
</ol>
<h3 id="过采样或者下采样"><a href="#过采样或者下采样" class="headerlink" title="过采样或者下采样"></a>过采样或者下采样</h3><p>它们是最简单的方式，仅仅需要对处理步骤的很少改变，对样本集合进行调整直到达到平衡。过采样随机地重复少数类样本来增加少数类样本的数目，下采样随机地下采样多数类样本。一些数据科学家幼稚地认为过采样比下采样要好，因为过采样会产生更多的数据样本，而下采样却丢弃了数据样本。但是需要记住的是，重复数据并不是没有后果，重复数据会导致变量的方差变得比其真实方差要小，好的一面是，重复数据也增加了分类器所犯错误的计数，如果一个分类器在原始的数据集上犯了一个 false negative 的错误，并且数据集重复了 5 倍，那么分类器就会在新的数据集上犯 6 个错误。相反地，下采样会导致变量的方差变得比其真实方差要大。就因为如此，机器学习中过采样，下采样和使用自然分布都有应用。<br><img src="http://oytnj8g2y.bkt.clouddn.com/ImbalancedClasses_fig5.jpg" alt="ImbalancedClasses_fig5.png"><br>大部分的机器学习包能够进行简单的采样调整。R包<code>unbalanced</code>实现了一些专门用于类别不平衡数据集的采样技术，并且<code>scikit-learn.cross_validation</code>也有基本的采样算法。</p>
<h3 id="Bayesian-argument-of-Wallace-et-al"><a href="#Bayesian-argument-of-Wallace-et-al" class="headerlink" title="Bayesian argument of Wallace et al."></a>Bayesian argument of Wallace et al.</h3><p>在论文<em>Class Imbalance, Redux</em>, by Wallace, Small, Brodley and Trikalinos[4][4] 中提出了可能是最好的关于类别不平衡的理论证明和实践建议。他们支持下采样多数类。他们提出的证明是数学理论上的，但是在这里我仅会展示一个他们用来表达观点的例子。他们认为两个类别在一些具有解释性的变量分布的尾部必须可分辨的。假设有两个类别只有一个依赖变量<code>x</code>，每一个类别产生自一个标准差为 1 的高斯分布，类别 1 的高斯分布的均值为 1，类别 2 的高斯分布的均值为 2。它们看起来是这样的：<br><img src="http://oytnj8g2y.bkt.clouddn.com/ImbalancedClasses_fig6.png" alt="ImbalancedClasses_fig6.png"><br>给定一个 x 值，你会使用哪一个门限来确定这个样本来自于哪一个类别？很明显是中间的点 x=1.5，即图中的垂直线，如果一个新的样本 x 落在了小于 1.5 的一边，那么它可能就是类别 1，否则的话就是类别 2。当从这些样本中进行学习的时候，我们期望学习到的判别线就是 1.5。事实上，如果类别是平衡的话，这确实是我们会得到的结果。在 x 轴上的点是从每个分布得到的样本。<br>但是如果类别 1 是少数类，并假设我们有 10 个类别 1 的样本，50个类别 2 的样本，这种情况下，我们很可能学习到的就是一个发生了偏移的分隔线。<br><img src="http://oytnj8g2y.bkt.clouddn.com/ImbalancedClasses_fig7.png" alt="ImbalancedClasses_fig7.png"><br>我们可以通过下采样多数类的方式让其在数量上匹配少数类。问题是在于由于样本量的减少，学习出来的分割线会具有较高的易变性。做了10次采样，会导致10根不同的垂直线。<br><img src="http://oytnj8g2y.bkt.clouddn.com/ImbalancedClasses_fig8.png" alt="ImbalancedClasses_fig8.png"><br>因此最后一个步骤就是使用 <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" target="_blank" rel="external">bagging</a> 的方式将这些分类器进行组合。整个过程看起来就是下面这样的：<br><img src="http://oytnj8g2y.bkt.clouddn.com/ImbalancedClasses_fig9.png" alt="ImbalancedClasses_fig9.png"><br>这个技术在 Scikit-learn 中并没有实现，但是一个叫做 <a href="https://github.com/silicon-valley-data-science/learning-from-imbalanced-classes/blob/master/blagging.py" target="_blank" rel="external"><code>blagging.py</code></a>（balanced bagging）实现了一个 <code>BlaggingClassifier</code>，它在聚合之前平衡了 bootstrap 之后的样本。</p>
<h3 id="Neighbor-based-approaches"><a href="#Neighbor-based-approaches" class="headerlink" title="Neighbor-based approaches"></a>Neighbor-based approaches</h3><p>上采样或者下采样使用随机选择样本的方式来调整样本的比例。一些其他的方式会仔细地检查样本空间，然后基于近邻关系采取一些行为。<br>例如，Tomek link 是指互为最近邻的分属两类的一对样本。换句话说，就是它们分属不同的类别但是却彼此很邻近。<br><img src="http://oytnj8g2y.bkt.clouddn.com/ImbalancedClasses_fig10.png" alt="ImbalancedClasses_fig10.png"><br>Tomek 算法找到这样的样本对，并且从中移除多数类的样本。思想就是让多数类和少数类之间的边界变得更加清晰，让少数类区域更加清楚。上面的图展示了一个简单的 Tomek link 移除的例子。R 包的<code>unbalanced</code>除了一些专门用于类别不平衡数据集的采样技术，也实现了 Tomek link 移除。 Scikit-learn 虽然没有内建的这种功能，但是有一些可用的独立包（比如<a href="https://github.com/ojtwist/TomekLink" target="_blank" rel="external">TomekLink</a>）。</p>
<h3 id="Synthesizing-new-examples-SMOTE-and-descendants"><a href="#Synthesizing-new-examples-SMOTE-and-descendants" class="headerlink" title="Synthesizing new examples: SMOTE and descendants"></a>Synthesizing new examples: SMOTE and descendants</h3><p>另外一个不涉及重采样样本的方式是合成新的样本。最著名的一个例子就是 Chawla 的<a href="https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume16/chawla02a-html/chawla2002.html" target="_blank" rel="external">SMOTE</a> (Synthetic Minority Oversampling TEchnique)系统。思想就是通过在现存的少数类样本之间内插产生新的少数类样本。基本过程如下：<br><img src="http://oytnj8g2y.bkt.clouddn.com/ImbalancedClasses_fig11.png" alt="ImbalancedClasses_fig11.png"><br>SMOTE 很是成功，启发了很多的变种，扩展。在 R 的 <code>unbalanced</code> 和 Python 的 <a href="https://github.com/scikit-learn-contrib/imbalanced-learn" target="_blank" rel="external">UnbalancedDatasete</a> 中可以找到 SMOTE 及其扩展。<br>需要注意的是 SMOTE 的一个本质局限。由于是内插少数类样本，它仅仅能够在少数类样本的内部产生样本，外部是不行的。从形式上说， SMOTE 仅仅能够填充现存少数类样本的凸包，并不能创建新的少数类样本外部区域。</p>
<h3 id="Adjusting-class-weights"><a href="#Adjusting-class-weights" class="headerlink" title="Adjusting class weights"></a>Adjusting class weights</h3><p>许多机器学习工具包能够调整类别的重要性。例如， Scikit-learn 中的很多分类器有一个额外的参数<code>class_weight</code>，能够让一个类别的重要性高于其他的类别。下面就是一个直接来自 scikit-learn <a href="http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html" target="_blank" rel="external">文档</a>的例子，展示了10倍增加少数类类别权重的作用。实线是使用默认设置（类别权重相同）时的边界，虚线是使用<code>class_weight</code>之后的效果。<br><img src="http://oytnj8g2y.bkt.clouddn.com/ImbalancedClasses_fig12.png" alt="ImbalancedClasses_fig12.png"><br>你可以看得到少数类在重要性上获得了增强，在少数类上犯下错误要比在其他类上犯下错误付出的代价更大，分隔超平面会进行调整去减少损失。需要注意的一点是，调整类别的重要性通常只在一种类别错误上具有作用（如果少数类是正例的话，那就是 False Negative 这种错误）。当然，如果分类器在训练集上没有犯下这样的错误，调整类别权重就没有作用。</p>
<h3 id="And-beyond"><a href="#And-beyond" class="headerlink" title="And beyond"></a>And beyond</h3><p>This post has concentrated on relatively simple, accessible ways to learn classifiers from imbalanced data. Most of them involve adjusting data before or after applying standard learning algorithms. It’s worth briefly mentioning some other approaches.</p>
<h4 id="New-algorithms"><a href="#New-algorithms" class="headerlink" title="New algorithms"></a>New algorithms</h4><p>Learning from imbalanced classes continues to be an ongoing area of research in machine learning with new algorithms introduced every year. Before concluding I’ll mention a few recent algorithmic advances that are promising.</p>
<p>In 2014 Goh and Rudin published a paper <em>Box Drawings for Learning with Imbalanced Data</em>[5][5] which introduced two algorithms for learning from data with skewed examples. These algorithms attempt to construct “boxes” (actually axis-parallel hyper-rectangles) around clusters of minority class examples:<br><img src="http://oytnj8g2y.bkt.clouddn.com/ImbalancedClasses_fig13.png" alt="ImbalancedClasses_fig13.png"><br>Their goal is to develop a concise, intelligible representation of the minority class. Their equations penalize the number of boxes and the penalties serve as a form of regularization.</p>
<p>They introduce two algorithms, one of which (Exact Boxes) uses mixed-integer programming to provide an exact but fairly expensive solution; the other (Fast Boxes) uses a faster clustering method to generate the initial boxes, which are subsequently refined. Experimental results show that both algorithms perform very well among a large set of test datasets.</p>
<p>Earlier I mentioned that one approach to solving the imbalance problem is to discard the minority examples and treat it as a single-class (or anomaly detection) problem. One recent anomaly detection technique has worked surprisingly well for just that purpose. Liu, Ting and Zhou introduced a technique called Isolation Forests[6][6] that attempted to identify anomalies in data by learning random forests and then measuring the average number of decision splits required to isolate each particular data point. The resulting number can be used to calculate each data point’s anomaly score, which can also be interpreted as the likelihood that the example belongs to the minority class. Indeed, the authors tested their system using highly imbalanced data and reported very good results. A follow-up paper by Bandaragoda, Ting, Albrecht, Liu and Wells[7][7] introduced Nearest Neighbor Ensembles as a similar idea that was able to overcome several shortcomings of Isolation Forests.</p>
<h4 id="Buying-or-creating-more-data"><a href="#Buying-or-creating-more-data" class="headerlink" title="Buying or creating more data"></a>Buying or creating more data</h4><p>As a final note, this blog post has focused on situations of imbalanced classes under the tacit assumption that you’ve been given imbalanced data and you just have to tackle the imbalance. In some cases, as in a Kaggle competition, you’re given a fixed set of data and you can’t ask for more.</p>
<p>But you may face a related, harder problem: you simply don’t have enough examples of the rare class. None of the techniques above are likely to work. What do you do?</p>
<p>In some real world domains you may be able to buy or construct examples of the rare class. This is an area of ongoing research in machine learning. If rare data simply needs to be labeled reliably by people, a common approach is to crowdsource it via a service like Mechanical Turk. Reliability of human labels may be an issue, but work has been done in machine learning to combine human labels to optimize reliability. Finally, Claudia Perlich in her Strata talk All The Data and Still Not Enough gives examples of how problems with rare or non-existent data can be finessed by using surrogate variables or problems, essentially using proxies and latent variables to make seemingly impossible problems possible. Related to this is the strategy of using transfer learning to learn one problem and transfer the results to another problem with rare examples, as described here.</p>
<h4 id="Comments-or-questions"><a href="#Comments-or-questions" class="headerlink" title="Comments or questions?"></a>Comments or questions?</h4><p>Here, I have attempted to distill most of my practical knowledge into a single post. I know it was a lot, and I would value your feedback. Did I miss anything important? Any comments or questions on this blog post are welcome.</p>
<h4 id="Resources-and-further-reading"><a href="#Resources-and-further-reading" class="headerlink" title="Resources and further reading"></a>Resources and further reading</h4><ol>
<li><p>Several Jupyter notebooks are available illustrating aspects of imbalanced learning.</p>
<ul>
<li>A notebook illustrating sampled Gaussians, above, is at <a href="https://github.com/silicon-valley-data-science/learning-from-imbalanced-classes/blob/master/Gaussians.ipynb" target="_blank" rel="external">Gaussians.ipynb</a>.</li>
<li>A simple implementation of Wallace’s method is available at <a href="https://github.com/silicon-valley-data-science/learning-from-imbalanced-classes/blob/master/blagging.py" target="_blank" rel="external">blagging.py</a>. It is a simple fork of the existing bagging implementation of sklearn, specifically ./sklearn/ensemble/bagging.py.</li>
<li>A notebook using this method is available at <a href="https://github.com/silicon-valley-data-science/learning-from-imbalanced-classes/blob/master/ImbalancedClasses.ipynb" target="_blank" rel="external">ImbalancedClasses.ipynb</a>. It loads up several domains and compares blagging with other methods under different distributions.</li>
</ul>
</li>
<li><p>Source code for Box Drawings in MATLAB is available from: <a href="http://web.mit.edu/rudin/www/code/BoxDrawingsCode.zip" target="_blank" rel="external">http://web.mit.edu/rudin/www/code/BoxDrawingsCode.zip</a></p>
</li>
<li>Source code for Isolation Forests in R is available at: <a href="https://sourceforge.net/projects/iforest/" target="_blank" rel="external">https://sourceforge.net/projects/iforest/</a></li>
</ol>
<p>[1] Natalie Hockham makes this point in her talk Machine learning with imbalanced data sets, which focuses on imbalance in the context of credit card fraud detection.<br>[2] By definition there are fewer instances of the rare class, but the problem comes about because the cost of missing them (a false negative) is much higher.<br>[3] The details in courier are specific to Python’s Scikit-learn.<br>[4] “Class Imbalance, Redux”. Wallace, Small, Brodley and Trikalinos. IEEE Conf on Data Mining. 2011.<br>[5] Box Drawings for Learning with Imbalanced Data.” Siong Thye Goh and Cynthia Rudin. KDD-2014, August 24–27, 2014, New York, NY, USA.<br>[6] “Isolation-Based Anomaly Detection”. Liu, Ting and Zhou. ACM Transactions on Knowledge Discovery from Data, Vol. 6, No. 1. 2012.<br>[7] “Efficient Anomaly Detection by Isolation Using Nearest Neighbour Ensemble.” Bandaragoda, Ting, Albrecht, Liu and Wells. ICDM-2014</p>

    </div>

    

    

    <!-- Comments -->
    

</div>
        </section>

    </div>
</div>


</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This is Robert Lexis (FengCun Li). To see the world, things dangerous to come to, to see behind walls, to draw closer, to find each other and to feel. That is the purpose of LIFE.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2019/07/12/Unnamed-namespace/">Unnamed namespace</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2019/05/08/Static-variable-in-inlined-function/">Static variable in inline</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2019/04/23/Iterator-invalidation-rules/">Iterator invalidation rul</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2019/03/18/Emplace-back/">Emplace back</a>
            </li>
            
        </ul>
    </div>



            
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/RobertLexis">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:robert_lexis@163.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Robert Lexis Loves Wenny
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



</body>

</html>