<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Chapter 5">
<meta property="og:type" content="article">
<meta property="og:title" content="RLAI Monte Carlo Methods 学习笔记">
<meta property="og:url" content="http://yoursite.com/2017/12/12/RLAI-Monte-Carlo-Methods-学习笔记/index.html">
<meta property="og:site_name" content="The next stop - Antarctica">
<meta property="og:description" content="Chapter 5">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/first-visit-mc-prediction.png">
<meta property="og:image" content="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/GPI.png">
<meta property="og:image" content="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/evaluation-improvement.png">
<meta property="og:image" content="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/MC-ES.png">
<meta property="og:image" content="http://yoursite.com/2017/12/12/">
<meta property="og:image" content="http://yoursite.com/2017/12/12/">
<meta property="og:image" content="http://yoursite.com/2017/12/12/">
<meta property="og:image" content="http://yoursite.com/2017/12/12/">
<meta property="og:image" content="http://yoursite.com/2017/12/12/">
<meta property="og:updated_time" content="2020-11-09T04:23:41.900Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RLAI Monte Carlo Methods 学习笔记">
<meta name="twitter:description" content="Chapter 5">
<meta name="twitter:image" content="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/first-visit-mc-prediction.png">

<link rel="canonical" href="http://yoursite.com/2017/12/12/RLAI-Monte-Carlo-Methods-学习笔记/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>RLAI Monte Carlo Methods 学习笔记 | The next stop - Antarctica</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">The next stop - Antarctica</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">To see behind walls. To draw closer.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/12/RLAI-Monte-Carlo-Methods-学习笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Fengcun Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="The next stop - Antarctica">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          RLAI Monte Carlo Methods 学习笔记
        </h1>

        <div class="post-meta">
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-11-09 12:23:41" itemprop="dateModified" datetime="2020-11-09T12:23:41+08:00">2020-11-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Chapter-5"><a href="#Chapter-5" class="headerlink" title="Chapter 5"></a>Chapter 5</h2><a id="more"></a>
<h2 id="Monte-Carlo-Methods-一种估计值函数和发现最优策略的方法"><a href="#Monte-Carlo-Methods-一种估计值函数和发现最优策略的方法" class="headerlink" title="Monte Carlo Methods 一种估计值函数和发现最优策略的方法"></a>Monte Carlo Methods 一种估计值函数和发现最优策略的方法</h2><p>不需知道环境的完整信息，MC 方法只需要 experience，也就是来自真实或者仿真环境下的状态、动作、奖励的序列。尽管需要环境模型，但是仅仅需要这个模型产生样本，而不是像动态规划那样需要完整的转移概率分布。在很多情况下，获得样本是容易的，而给出具体形式的分布确实困难的，这就使得 MC 方法十分的有用。</p>
<p>MC 方法基于采样与平均。experience 分为不同的 episode，只有在一个 episode 完成之后才会进行值估计和策略提升。MC 方法是 episode-by-episode （offline）的形式，而不是 step-by-step （online）的方式。</p>
<h3 id="Monte-Carlo-Prediction"><a href="#Monte-Carlo-Prediction" class="headerlink" title="Monte Carlo Prediction"></a>Monte Carlo Prediction</h3><p>一个状态的 value 是一个期望回报，即从该状态出发遵循一定的策略可以期望获得的累积未来折扣奖励。</p>
<p>MC 方法内在的想法就是，从 experience 中估计状态值函数，对访问一个状态之后获得的回报进行平均，随着 episode 次数的增多，这个平均值会收敛到期望值。\(v<em>{\pi}(s)\) 是在策略 \(\pi\) 下，状态 \(s\) 的 value，我们要做的就是在给定一些遵循策略 \(\pi\) 经历了状态 \(s\) 的 episode 时，对 \(v</em>{\pi}(s)\) 进行估计。在 episode 中的 \(s\) 每一次出现就称之为对状态 \(s\) 的一次访问。根据进行估计时使用 \(s\) 的第一访问还是每一次访问，MC 方法可以分为 First-visit MC 和 Every-visit MC。<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/first-visit-mc-prediction.png" alt="first-visit-mc-prediction"></p>
<h3 id="Monte-Carlo-Estimation-of-Action-Values"><a href="#Monte-Carlo-Estimation-of-Action-Values" class="headerlink" title="Monte Carlo Estimation of Action Values"></a>Monte Carlo Estimation of Action Values</h3><p>上面讲了在给定策略的条件下使用 MC 对状态值函数进行估计。</p>
<p>当环境模型未知时，对状态-动作值函数进行估计就比对状态值函数进行估计要有用了，这是因为在环境模型已知时，状态值函数对于确定（给出）一个策略就足够了，即在根据当前状态选择动作时，仅仅需要选择立即奖励和下一状态的 value 之和最大的那个动作。这样可行是因为，当环境模型已知时，一个动作带来的奖励是已知的，状态转移概率是已知的，即在当前状态下采取一个动作转移到下一状态及由下一状态带来的期望回报都是已知的。但是在没有环境模型时，状态值函数不足以给出一个策略，为了可以得到一个策略，就需要更加显式直接的状态-动作对值函数。</p>
<p>\(q_{\pi}(s,a)\) the expected return when starting in state \(s\), taking action \(a\), and thereafter following policy \(\pi\)（从状态 \(s\) 出发，采取动作 \(a\) 并在后续遵循策略 \(\pi\) 可以得到的期望回报）。在使用 MC 进行估计时，和状态值函数的估计很像，不做在这里我们考虑的不再是对状态 \(s\) 的访问而是对状态-动作对 \(s\)-\(a\) 的访问。</p>
<p>这里唯一的问题在于，很多的状态-动作对可能永远不会被访问到。如果策略是确定性的，那么在一个状态下做出的动作就是确定性的，有的状态-动作对就不会被访问到，就无法更新状态-动作对的 value, 这是一个严重的问题，因为学习状态-动作对的 value 的目的就是用来在一个状态下可用的动作中做出选择，这就需要我们能够对一个状态的所有可能动作的 value 做出估计，而不是仅仅估计我们目前策略所指定的状态-动作对的 value。这就是一个保持探索（maintaining exploration）的问题，一个解决方法就是 exploring starts 假设，即每一个状态-动作对都有不为零的概率被选择成为一个 episode 的开始。但是在“实际”环境中这通常是不可行的。另外一个确保所有的状态-动作对都被访问到的方法是使用在每一个状态下以非零概率选择各个动作的随机策略。</p>
<h3 id="Monte-Carlo-Control"><a href="#Monte-Carlo-Control" class="headerlink" title="Monte Carlo Control"></a>Monte Carlo Control</h3><p>遵循 GPI（generalized policy iteration 泛化策略迭代）的指导，即策略的评估（利用上面讲到的状态-动作对值函数对一个策略的好坏进行评估）和策略的提高（对得到的状态-动作对值函数进行利用从而提高策略）。<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/GPI.png" alt="GPI"><br>policy evaluation and policy improvement, beginning with an arbitrary policy \(\pi<em>{0}\) and ending with the optimal policy and optimal action-value function:<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/evaluation-improvement.png" alt="evaluation-improvement"><br>策略评估是通过多个 episode，用状态-动作对值函数的估计渐进地逼近真实的函数。<br>策略提升是通过对状态-动作对值函数进行利用得到的贪婪策略，即\$\$\pi(s)=argmax</em>{a}q(s,a)$$<br>策略提高之所以有效是因为：<br>$$q<em>{\pi</em>{k} }(s, \pi<em>{k+1}(s))=q</em>{\pi<em>{k} }(s, argmax</em>{a}q<em>{\pi</em>{k} }(s,a))$$<br>$$=max<em>{a}q</em>{\pi<em>{k} }(s,a)$$<br>$$\ge q</em>{\pi<em>{k} }(s, \pi</em>{k}(s))\$\$<br>上面的公式可以做如下解释：<br>\(q<em>{\pi</em>{k} }(s,a)\) 是在策略 \(\pi<em>{k}\) 指导下进行多次 episode 而得到的对状态-动作对值函数的估计，使用贪心的方式得到的新策略 \(\pi</em>{k+1}(s)\)，\(q<em>{\pi</em>{k} }(s, \pi<em>{k+1}(s))\) 是在新的策略 \(\pi</em>{k+1}(s)\)指导下可以得到的状态-动作对值函数的预期，实际上尚未在其指导下进行过一次 episode，可以看到这个预期是大于等于已经得到的状态-动作对值函数的估计的，因此新的策略 \(\pi<em>{k+1}(s)\) 相较于 \(\pi</em>{k}(s)\) 是提升了的。可以看到状态-动作对值函数的提高提升了策略，而策略的提高提升了状态-动作对值函数，两者交替上升。由此可见，MC 方法可以在对环境模型未知的情况下仅仅通过 episode 找到最优的策略。</p>
<p>We made two unlikely assumptions above in order to easily obtain this guarantee of convergence for the Monte Carlo method. One was that the episodes have exploring starts, and the other was that policy evaluation could be done with an infinite number of episodes.第二个假设相对而言比较容易移除，一种有效的避免策略评估需要无限次数的 episode 的途径是，我们放弃完整地完成策略评估，而是中途进入策略提升阶段，我们并不需要直达或者逼近真实的\(q<em>{\pi</em>{k} }\)，而只需要 move toward \(q<em>{\pi</em>{k} }\) step-by-step。</p>
<p>一个极端的例子就是值迭代，在两次策略提升之间是只有迭代策略评估的一次迭代；in-place 版本的值迭代更加极端，在提升和评估之间只有一个状态。</p>
<p>For Monte Carlo policy evaluation it is natural to alternate between evaluation and improvement on<br>an episode-by-episode basis. 在每次 episode 之后，观测到的回报被用于策略评估，然后策略会在此次 episode 中被访问的所有状态上进行提升。<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/MC-ES.png" alt="MC-ES"></p>
<h3 id="Monte-Carlo-Control-without-Exploring-Starts"><a href="#Monte-Carlo-Control-without-Exploring-Starts" class="headerlink" title="Monte Carlo Control without Exploring Starts"></a>Monte Carlo Control without Exploring Starts</h3><p>如何去掉 exploring starts 假设呢？有两种方式：</p>
<ul>
<li>on-policy：evaluate or improve the policy that is used to make decisions</li>
<li>off-policy：evaluate or improve a policy different from that used to generate the data<br>MC ES 是一种 on-policy 的例子。<br>在 on-policy 方法中的策略通常是软的，也就是$$\pi(a|s)&gt;0\quad for\quad all\quad s\quad \in\quad S\quad and\quad a\quad \in\quad A(s)$$<br>但是向确定性的最优策略偏移地越来越近。<br>\(\varepsilon-soft\) 中的一个例子就是 \(\varepsilon-greedy\)，在 \(\varepsilon-greedy\) 中所有的非贪婪动作被选择的概率为 \(\frac{\varepsilon}{|A(s)|}\)，贪婪动作被选择的概率为 \(1-\varepsilon+\frac{\varepsilon}{|A(s)|}\)，它是对于所有的状态和动作都有 \(\pi(a|s)\ge\frac{\varepsilon}{|A(s)|}\) 的 \(\varepsilon-soft\)。<br>在去掉 exploring starts 假设之后，不能简单地使用贪婪地方式进行策略提升，因为这会阻碍非贪婪动作的探索，相应的使用 \(\varepsilon-greedy\) 策略。<br><img src="/2017/12/12/RLAI-Monte-Carlo-Methods-学习笔记/.." alt="epsilon-soft"><br>关于算法有效性的证明请参考 Reinforcement Learning: An Introduction 5.4。</li>
</ul>
<h3 id="Off-policy-Prediction-via-Importance-Sampling"><a href="#Off-policy-Prediction-via-Importance-Sampling" class="headerlink" title="Off-policy Prediction via Importance Sampling"></a>Off-policy Prediction via Importance Sampling</h3><p>所有的 learning control 方式都有一个困境，那就是假设在后续最优动作的条件下得到状态-动作对的 value，但是又需要表现的非最优来探索所有的动作，即最优动作是确定的，但是又需要执行非最优动作来探索动作空间进行最优动作的寻找，使得最优策略的状态-动作对 value 的求解变得不可得。<br>How to learn about the optimal policy while behaving according to<br>an exploratory policy?<br>上面讲到的得 on-policy 是一种折中的方式，即学习到的并不是最优策略的状态-动作对的值，而是接近最优的仍在探索的策略的状态-动作对的值，即最优策略是固定的，但此处得到的策略是随机的非最优的。更直接的方式是使用两个策略，一个进行学习从而成为最优策略（target policy），另一个更具有探索性用来产出动作（behaviour policy），也就是 off-policy learning，这种方式方差较大，收敛地更慢。<br>下面首先进行策略的评估，也就是 prediction，从策略 \(b(b\ne\pi)\) 指导下的一些 episode 出发估计 \(v<em>{\pi}\) 或者 \(q</em>{\pi}\)。<br>收敛假设：$$\pi(a|s)&gt;0\ implies\ b(a|s)&gt;0$$<br>另外，\(b\) 必须是随机的，而 \(\pi\) 可以是确定性的，在控制学里，典型的情形是 \(\pi\) 是在当前状态-动作值函数估计下的确定性策略，behaviour policy 保持随机，而 target policy 逐渐成为一个确定性的最优策略。<br>几乎所有的 off-policy 方法使用了重要性采样，a general technique for estimating expected<br>values under one distribution given samples from another。<br>在这里我们通过根据目标策略下的轨迹和行为策略下的轨迹之间的相对概率关系对回报进行加权的方式来将重要性采样应用到 off-policy 的学习中。<br>给定一个初始状态 \(S<em>{t}\)，\([A</em>{t}, S<em>{t+1}, A</em>{t+1},…,S<em>{T}]\)这样的一条轨迹出现的概率<br><img src="/2017/12/12/RLAI-Monte-Carlo-Methods-学习笔记/.." alt="trajectory_probability"><br>这样的一条轨迹在 target policy 和 behaviour policy 之间的相对概率也就是 importance-sampling ratio：<br><img src="/2017/12/12/RLAI-Monte-Carlo-Methods-学习笔记/.." alt="trajectory_importance_ratio"><br>Although the trajectory probabilities depend on the MDP’s transition probabilities, which are generally<br>unknown, they appear identically in both the numerator and denominator, and thus cancel. The<br>importance sampling ratio ends up depending only on the two policies and the sequence, not on the<br>MDP.<br>下面给出一个 MC 算法，该算法使用观察到的遵循策略 \(b\) 的一批次 episode 来估计 \(v</em>{\pi}(s)\).<br>给所有的 episode 中的时间步打破 episode 边界赋予一个唯一的递增的时间步序号，在所有的 episode 中，对于 every-visit 方式，定义对状态 \(s\) 进行了访问的时间步集合是 \(J(s)\)；对于 first-visit 方式，定义第一次对状态 \(s\)进行了访问的时间步集合是 \(J(s)\)。<br>let \(T(t)\) denote the first time<br>of termination following time \(t\), and \(G<em>{t}\) denote the return after \(t\) up through \(T(t)\)。<br>\(\lbrace G</em>{t}\rbrace<em>{t \in J(s)}\) 就是状态 \(s\) 的回报，\(\lbrace \rho</em>{t:T(t)-1} \rbrace<em>{t \in J(s)}\) 就是状态 \(s\) 的重要性采样比率。<br>To estimate \(v</em>{\pi}(s)\), we simply scale the returns by the ratios and average the results:<br><img src="/2017/12/12/RLAI-Monte-Carlo-Methods-学习笔记/.." alt="ordinary_scale_return_and_average"><br>通过简单平均完成的重要性采样叫做 ordinary importance sampling。<br><img src="/2017/12/12/RLAI-Monte-Carlo-Methods-学习笔记/.." alt="weighted_scale_return_and_average"><br>通过加权平均完成的重要性采样叫做 weighted importance sampling。<br>关于这两者的不同，请参考 Reinforcement Learning: An Introduction 5.5。</p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2017/12/09/强化学习-3-估值网络及其-TensorFlow-实现/" rel="prev" title="强化学习 3 估值网络及其 TensorFlow 实现">
      <i class="fa fa-chevron-left"></i> 强化学习 3 估值网络及其 TensorFlow 实现
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/01/23/数据结构与算法分析C/" rel="next" title="数据结构与算法分析C++">
      数据结构与算法分析C++ <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter-5"><span class="nav-number">1.</span> <span class="nav-text">Chapter 5</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Monte-Carlo-Methods-一种估计值函数和发现最优策略的方法"><span class="nav-number">2.</span> <span class="nav-text">Monte Carlo Methods 一种估计值函数和发现最优策略的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Monte-Carlo-Prediction"><span class="nav-number">2.1.</span> <span class="nav-text">Monte Carlo Prediction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Monte-Carlo-Estimation-of-Action-Values"><span class="nav-number">2.2.</span> <span class="nav-text">Monte Carlo Estimation of Action Values</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Monte-Carlo-Control"><span class="nav-number">2.3.</span> <span class="nav-text">Monte Carlo Control</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Monte-Carlo-Control-without-Exploring-Starts"><span class="nav-number">2.4.</span> <span class="nav-text">Monte Carlo Control without Exploring Starts</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Off-policy-Prediction-via-Importance-Sampling"><span class="nav-number">2.5.</span> <span class="nav-text">Off-policy Prediction via Importance Sampling</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Fengcun Li"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Fengcun Li</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">123</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Fengcun Li</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  











<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
