<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="Basic Concepts张量Tensor 存储的不是数字，而是运算（Operation）结果的一个引用，例如 Tensor(&amp;quot;add:0&amp;quot;, shape=(2,), dtype=float32)。">
    

    <!--Author-->
    
        <meta name="author" content="Robert Lexis">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Learning TensorFlow 1 Basic"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="This is Robert Lexis."/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>Learning TensorFlow 1 Basic - This is Robert Lexis.</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
    </div>
</header>

        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/2017/12/07/Learning-TensorFlow-1-Basic/">
                Learning TensorFlow 1 Basic
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2017-12-07</span>
            
            
            
        </div>
    </div>

    <div class="content">

        <!-- Gallery -->
        

        <!-- Post Content -->
        <h2 id="Basic-Concepts"><a href="#Basic-Concepts" class="headerlink" title="Basic Concepts"></a>Basic Concepts</h2><h4 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h4><p><strong>Tensor</strong> 存储的不是数字，而是<strong>运算（Operation）</strong>结果的一个引用，例如 <code>Tensor(&quot;add:0&quot;, shape=(2,), dtype=float32)</code>。<br><a id="more"></a></p>
<blockquote>
<p>Tensor<br>Represents one of the outputs of an <strong>Operation</strong>.</p>
<p>A Tensor is a symbolic handle to one of the outputs of an Operation. It does not hold the values of that operation’s output, but instead provides a means of computing those values in a TensorFlow tf.Session.<br>This class has two primary purposes:</p>
<ul>
<li>A Tensor can be passed as an input to another Operation. This builds a dataflow connection between operations, which enables TensorFlow to execute an entire Graph that represents a large, multi-step computation.</li>
<li>After the graph has been launched in a session, the value of the Tensor can be computed by passing it to tf.Session.run. <strong>t.eval() is a shortcut for calling tf.get_default_session().run(t)</strong>.<br>In the following example, c, d, and e are symbolic Tensor objects, whereas result is a numpy array that stores a concrete value:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Build a dataflow graph.</span></div><div class="line">c = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]])</div><div class="line">d = tf.constant([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]])</div><div class="line">e = tf.matmul(c, d)</div><div class="line"></div><div class="line"><span class="comment"># Construct a `Session` to execute the graph.</span></div><div class="line">sess = tf.Session()</div><div class="line"></div><div class="line"><span class="comment"># Execute the graph and store the value that `e` represents in `result`.</span></div><div class="line">result = sess.run(e)</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="运算-Operation"><a href="#运算-Operation" class="headerlink" title="运算 Operation"></a>运算 Operation</h4><p>运算是对张量的操作, 张量与张量之间的联系是通过运算发生的。</p>
<blockquote>
<p>Represents a graph node that performs computation on tensors.</p>
<p>An Operation is a node in a TensorFlow Graph that takes zero or more Tensor objects as input, and produces zero or more Tensor objects as output. Objects of type Operation are created by calling a Python op constructor (such as tf.matmul) or tf.Graph.create_op.<br>For example <code>c = tf.matmul(a, b)</code> creates <strong>an Operation of type “MatMul”</strong> that takes tensors <code>a</code> and <code>b</code> as input, and produces <code>c</code> as output.<br>After the graph has been launched in a session, an Operation can be executed by passing it to <code>tf.Session.run</code>. <strong>op.run() is a shortcut for calling tf.get_default_session().run(op)</strong>.</p>
</blockquote>
<h4 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h4><p>TensorFlow 会自动生成一个默认的计算图，如果没有特殊指定，运算（即节点）会自动被加入到这个计算图中。</p>
<h4 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h4><p>TensorFlow 不会自动生成一个默认的会话，但是可以在指定默认会话之后通过 <code>tf.Tensor.eval()</code>来获得张量所引用的运算结果的值。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sess = tf.Session()</div><div class="line"># 通过上下文管理器就可以完成默认会话的指定</div><div class="line">with sess.as_default():</div><div class="line">     print(result.eval())</div></pre></td></tr></table></figure></p>
<p>上面的代码和<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sess = tf.Session()</div><div class="line">sess.run(result)</div></pre></td></tr></table></figure></p>
<p>及<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sess = tf.Session()</div><div class="line">result.eval(session=sess)</div></pre></td></tr></table></figure></p>
<p>是一样的。<br>在交互式环境下，通过指定默认会话的方式来执行计算是十分方便的，但是指定默认会话的语句 <code>with sess.as_default()</code> 又带来缩进问题等比较麻烦，因此 <code>tf.InteractiveSession()</code> 应运而生，使用这个函数可以在交互式环境下将生成的会话直接指定为默认会话。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sess = tf.InteractiveSession()</div><div class="line">result.eval()</div><div class="line"># 关闭会话释放资源</div><div class="line">sess.close()</div></pre></td></tr></table></figure></p>
<h4 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h4><p><code>tf.Variable</code> 的作用就是保存和更新神经网络的参数。</p>
<blockquote>
<p>The Variable() constructor requires an initial value for the variable, which can be a Tensor of any type and shape. The initial value defines the <strong>type</strong> and <strong>shape</strong> of the variable. <strong>After construction, the type and shape of the variable are fixed. The value can be changed using one of the assign methods.</strong><br>If you want to change the shape of a variable later you have to use an assign Op with validate_shape=False.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line"></div><div class="line"># Create a variable.</div><div class="line">w = tf.Variable(&lt;initial-value&gt;, name=&lt;optional-name&gt;)</div><div class="line"></div><div class="line"># Use the variable in the graph like any Tensor.</div><div class="line">y = tf.matmul(w, ...another variable or tensor...)</div><div class="line"></div><div class="line"># The overloaded operators are available too.</div><div class="line">z = tf.sigmoid(w + y)</div><div class="line"></div><div class="line"># Assign a new value to the variable with `assign()` or a related method.</div><div class="line">w.assign(w + 1.0)</div><div class="line">w.assign_add(1.0)</div></pre></td></tr></table></figure>
<p>When you launch the graph, variables have to be explicitly initialized before you can run Ops that use their value. You can initialize a variable by <strong>running its initializer op</strong>, <strong>restoring the variable from a save file</strong>, or <strong>simply running an assign Op that assigns a value to the variable</strong>. In fact, the variable initializer op is just an assign Op that assigns the variable’s initial value to the variable itself.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># Launch the graph in a session.</div><div class="line">with tf.Session() as sess:</div><div class="line">  # Run the variable initializer.</div><div class="line">  sess.run(w.initializer)</div><div class="line">  # ...you now can run ops that use the value of &apos;w&apos;...</div></pre></td></tr></table></figure>
<p>The most common initialization pattern is to use the convenience function global_variables_initializer() to add an Op to the graph that initializes all the variables. You then run that Op after launching the graph.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># Add an Op to initialize global variables.</div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line"></div><div class="line"># Launch the graph in a session.</div><div class="line">with tf.Session() as sess:</div><div class="line">  # Run the Op that initializes global variables.</div><div class="line">  sess.run(init_op)</div><div class="line">  # ...you can now run any Op that uses variable values...</div></pre></td></tr></table></figure>
<p>If you need to create a variable with an initial value dependent on another variable, use the other variable’s <code>initialized_value()</code>. This ensures that variables are initialized in the right order.<br>All variables are automatically collected in the graph where they are created. By default, the constructor adds the new variable to <strong>the graph collection GraphKeys.GLOBAL_VARIABLES</strong>. The convenience function <code>global_variables()</code> returns the contents of that collection.<br>When building a machine learning model it is often convenient to distinguish between variables holding the trainable model parameters and other variables such as a global step variable used to count training steps. To make this easier, the variable constructor supports a <code>trainable=&lt;bool&gt;</code> parameter. If True, the new variable is <code>also</code> added to <code>the graph collection GraphKeys.TRAINABLE_VARIABLES</code>. The convenience function <code>trainable_variables()</code> returns the contents of this collection. <strong>The various Optimizer classes use this collection as the default list of variables to optimize</strong>.</p>
</blockquote>
<p>变量初始化：</p>
<ul>
<li>tf.random_normal</li>
<li>tf.truncated_normal</li>
<li>tf.random_uniform</li>
<li>tf.random_gamma</li>
<li>tf.zeros([2, 3], int32)</li>
<li>tf.ones([2, 3], int32)</li>
<li>tf.fill([2, 3], 9)</li>
<li>tf.constant([1, 2, 3])</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">biases = tf.Variable(tf.ones([3]))</div><div class="line"># 除了使用随机数和常数，也支持通过其他变量的初始值来初始化新的变量。</div><div class="line"># tf.Variable(biases.initialized_value() * 0.2)</div></pre></td></tr></table></figure>
<p>在TensorFlow中，一个变量在被运算使用之前， 这个变量的初始化需要被显式调用。<br>例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))</div><div class="line">w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))</div><div class="line">x = tf.constant([[0.7, 0.9]])</div><div class="line">a = tf.matmul(x, w1)</div><div class="line">y = tf.matmul(a, w2)</div><div class="line"></div><div class="line">sess = tf.Session()</div><div class="line"># 这里不能直接通过sess.run(y)来获取y的取值</div><div class="line"># 在变量定义时，虽然给出了初始化方法，但是这个初始化方法并没有被真正运行</div><div class="line"># 一个一个地调用初始化方法</div><div class="line">sess.run(w1.initializer)</div><div class="line">sess.run(w2.initializer)</div><div class="line">print(sess.run(y))</div><div class="line">sess.close()</div></pre></td></tr></table></figure></p>
<p>所有的变量都会被加入到 <code>GraphKeys.GLOBAL_VARIABLES</code> 这个集合，<code>global_variables</code> 函数可以拿到当前计算图的所有变量，拿到计算图的所有变量方便持久化整个计算图的运行状态。</p>
<p>变量的类型是不可改变的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1), name=&quot;w1&quot;) // 默认是tf.float32</div><div class="line">w2 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1, dtype=tf.float64), name=&quot;w2&quot;)</div><div class="line">w1.assign(w2)</div></pre></td></tr></table></figure></p>
<p>程序将报错<br>变量的维度是可以改变的，但是需要设置参数 <code>validate_shape=False</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1), name=&quot;w1&quot;)</div><div class="line">w2 = tf.Variable(tf.random_normal([2, 2], stddev=1, seed=1), name=&quot;w2&quot;)</div><div class="line">tf.assign(w1, w2)</div></pre></td></tr></table></figure></p>
<p>会报错：维度不匹配，使用 <code>tf.assign(w1, w2, validate_shape=False)</code> 就好了</p>
<h4 id="占位符"><a href="#占位符" class="headerlink" title="占位符"></a>占位符</h4><p>可以通过 <code>x = tf.constant([[0.7, 0.9]])</code> 表达一个batch的数据，但是每生成一个常量，计算图中就会增加一个节点，一般来说，神经网络的训练过程需要经过几百万甚至几亿次迭代，如果都用常量的方式输入值，这个计算图就爆了。为了避免这个问题，TensorFLow 使用 <strong>placeholder</strong> 的机制来输入数据。这样就不需要生成大量的常量来提供数据，只需要将数据通过 placeholder 输入即可。<br>placeholder 的类型不可更改，但是维度可以通过提供的数据推导得到。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))</div><div class="line">w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))</div><div class="line"></div><div class="line">// 这里的shape不一定给出，如果是确定的，给出维度可以降低出错的概率</div><div class="line">x = tf.placeholder(tf.float32, shape=(1, 2), name=&quot;input&quot;)</div><div class="line"></div><div class="line">a = tf.matmul(x, w1)</div><div class="line">y = tf.matmul(a, w2)</div><div class="line"></div><div class="line">sess = tf.Session()</div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line">sess.run(init_op)</div><div class="line"></div><div class="line">// print(sess.run(y))</div><div class="line">// 将报错，必须 feed</div><div class="line"></div><div class="line">print(sess.run(y, feed_dict=&#123;x: [[0.7, 0.9]]&#125;))</div><div class="line">sess.close()</div></pre></td></tr></table></figure></p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>在得到一个batch的前向传播结果之后，需要定义一个损失函数来刻画当前的预测值和真实值之间的差距，然后通过反向传播算法调整神经网络的参数使得差距被缩小。</p>
<h5 id="交叉熵是分类问题中常用的损失函数"><a href="#交叉熵是分类问题中常用的损失函数" class="headerlink" title="交叉熵是分类问题中常用的损失函数"></a>交叉熵是分类问题中常用的损失函数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cross_entropy = -tf.reduce_mean(_y * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))</div><div class="line">learning_rate = 0.001</div><div class="line">train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)</div></pre></td></tr></table></figure>
<p>通过运行 <code>sess.run(train_step)</code> 就可以对所有在 GraphKeys.TRAINABLE_VARIABLES 集合中的变量进行优化，使得当前 batch 下损失函数更小。<br><code>cross_entropy = -tf.reduce_mean(_y * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))</code>包含四个不同的运算，因为交叉熵与 softmax 回归经常一起使用，因此对这两个功能进行了封装，<code>tf.nn.softmax_cross_entropy_with_logits(y, y_)</code> y 是神经网络的原始输出，y_是真实label。</p>
<h5 id="均方误差是回归问题中常用的损失函数"><a href="#均方误差是回归问题中常用的损失函数" class="headerlink" title="均方误差是回归问题中常用的损失函数"></a>均方误差是回归问题中常用的损失函数</h5><p><code>mse = tf.reduce_mean(tf.square(y_ - y))</code> </p>
<h4 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h4><p><code>tf.train.exponential_decay</code> 函数实现了指数衰减学习率，通过它可以先使用较大的学习率来快速得到一个比较优的解，然后随着迭代逐步减少学习率，使得模型在后期更加稳定，而不是振荡。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">global_step = tf.Variable(0)</div><div class="line">learning_rate = tf.train.exponential_decay(0.1, global_step, 100, 0.96, staircase=True)</div><div class="line"></div><div class="line"># 使用指数衰减学习率，在minimize中传入global_step将自动更新global_step参数，从而使得学习率也得到相应的更新。</div><div class="line">learning_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</div></pre></td></tr></table></figure></p>
<h4 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h4><p>TensorFlow 可以优化带正则化的损失函数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">w = tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))</div><div class="line">y = tf.matmul(x, w)</div><div class="line"></div><div class="line">loss = tf.reduce_mean(tf.square(y_ - y)) + tf.contrib.layers.l2_regularizer(lambda)(w)</div></pre></td></tr></table></figure></p>
<p>比如 L1 和 L2 正则化<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">weights = tf.constant([[1.0, -2.0], [-3.0, 4.0]])</div><div class="line">with tf.Session() as sess:</div><div class="line">     (|1| + |-2| + |-3| + |4|) * 0.5 = 5</div><div class="line">     print sess.run(tf.contrib.layers.l1_regularizer(0.5)(weights))</div><div class="line">     (1 + 4 + 9 + 16)/2 + 0.5 = 7.5</div><div class="line">     print sess.run(tf.contrib.layers.l2_regularizer(0.5)(weights))</div></pre></td></tr></table></figure></p>
<h4 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h4><p>在简单的神经网络里，像 <code>loss = tf.reduce_mean(tf.square(y_ - y)) + tf.contrib.layers.l2_regularizer(lambda)(w)</code> 这样将权重的正则化加入损失函数就可以了，但是当网络复杂之后参数增多，首先会导致损失函数的定义很长，但是更重要的是网络结构的定义部分和损失函数的定义部分可能不在同一个函数里，这样通过变量来传递的方式就不方便了。为了解决这个问题，可以使用 TensorFlow 提供的集合， 它保存了一个计算图中的很多实体（比如张量）。<br>获取一层神经网络上的权重，并将这个权重的L2正则化损失加入到名为losses的集合中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">def get_weight(shape, lambda):</div><div class="line">    var = tf.Variable(tf.random_normal(shape), dtype=tf.float32)</div><div class="line">    tf.add_to_collection(&quot;losses&quot;, tf.contrib.layers.l2_regularizer(lambda)(var))</div><div class="line">    return var</div><div class="line"></div><div class="line">x = tf.placeholder(tf.float32, shape=(None, 2))</div><div class="line">y_ = tf.placeholder(tf.float32, shape=(None, 1))</div><div class="line">batch_size = 8</div><div class="line"></div><div class="line">layer_dimension = [2, 10, 10, 10, 1]</div><div class="line">n_layers = len(layer_dimension)</div><div class="line"></div><div class="line"># 当前层</div><div class="line">cur_layer = x</div><div class="line"># 当前层的节点个数</div><div class="line">in_dimension = layer_dimension[0]</div><div class="line"></div><div class="line"># 全连接神经网络</div><div class="line">for i in range(1, n_layers):</div><div class="line">     out_dimension = layer_dimension[i]</div><div class="line">     weight = get_weight([in_dimension, out_dimension], 0.001)</div><div class="line">     bias = tf.Variable(tf.constant(0.1, shape=[out_dimension]))</div><div class="line">     cur_layer = tf.nn.relu(tf.matmul(cur_layer, weight) + bias)</div><div class="line">     in_dimension = layer_dimension[i]</div><div class="line"></div><div class="line">mse_loss = tf.reduce_mean(tf.square(y_ - cur_layer))</div><div class="line">tf.add_to_collection(&quot;losses&quot;, mse_loss)</div><div class="line">loss = tf.add_n(tf.get_collection(&quot;losses&quot;))</div></pre></td></tr></table></figure></p>
<h4 id="name-scope-和-variable-scope"><a href="#name-scope-和-variable-scope" class="headerlink" title="name_scope 和 variable_scope"></a>name_scope 和 variable_scope</h4><p>经常使用 name_scope 做计算图可视化时的缩略控制，variable_scope 控制变量的命名，控制变量共享。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line">with tf.variable_scope(&apos;variable_scope_y&apos;) as scope:</div><div class="line">    var1 = tf.get_variable(name=&apos;var1&apos;, shape=[1], dtype=tf.float32)</div><div class="line">    scope.reuse_variables() # 设置共享变量</div><div class="line">    var1_reuse = tf.get_variable(name=&apos;var1&apos;)</div><div class="line">    var2 = tf.Variable(initial_value=[2.], name=&apos;var2&apos;, dtype=tf.float32)</div><div class="line">    var2_reuse = tf.Variable(initial_value=[2.], name=&apos;var2&apos;, dtype=tf.float32)</div><div class="line"></div><div class="line">with tf.Session() as sess:</div><div class="line">     sess.run(tf.global_variables_initializer())</div><div class="line">     print(var1.name, sess.run(var1))</div><div class="line">     print(var1_reuse.name, sess.run(var1_reuse))</div><div class="line">     print(var2.name, sess.run(var2))</div><div class="line">     print(var2_reuse.name, sess.run(var2_reuse))</div></pre></td></tr></table></figure></p>
<p>输出结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">variable_scope_y/var1:0 [-1.59682846]</div><div class="line">variable_scope_y/var1:0 [-1.59682846]</div><div class="line">variable_scope_y/var2:0 [ 2.]</div><div class="line">variable_scope_y/var2_1:0 [ 2.]</div></pre></td></tr></table></figure></p>
<p>可以看到变量 <code>var1_reuse</code> 重复使用了 <code>var1</code>，如果使用 Variable 的话，每次都会新建变量，但是大多数时候我们希望一些变量复用，所以就有了 get_variable()。</p>
<ul>
<li>tf.get_variable() 会根据 variable_scope 的模式不同而具有不同的行为，当 reuse=True 时，只想着复用，如果之前没有定义过这个变量则会报错；当 reuse=False 时只想着新建，如果之前定义过同名变量则会报错。 </li>
<li>tf.Variable() 只想着新建变量，不管 variable_scope 有没有设置共享变量， 都会新建变量，重名时会重命名。</li>
<li>tf.Variable 受 variable_scope 的命名空间的限制，但是由于自身的限制，不能对于 variable_scope 的 reuse=True 做出响应。</li>
<li>tf.get_variable() 不受 name_scope 的命名空间的限制。</li>
<li>name_scope 作用于运算，variable_scope 仅仅作用于变量。</li>
</ul>
<p>共享变量的例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">with tf.variable_scope(&apos;foo&apos;) as foo_scope:</div><div class="line">    v = tf.get_variable(&apos;v&apos;, [1])</div><div class="line">with tf.variable_scope(&apos;foo&apos;, reuse=True):</div><div class="line">    v1 = tf.get_variable(&apos;v&apos;)</div><div class="line">assert v1 == v</div></pre></td></tr></table></figure></p>
<p>或者这样：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">with tf.variable_scope(&apos;foo&apos;) as foo_scope:</div><div class="line">    v = tf.get_variable(&apos;v&apos;, [1])</div><div class="line">with tf.variable_scope(foo_scope, reuse=True):</div><div class="line">    v1 = tf.get_variable(&apos;v&apos;)</div><div class="line">assert v1 == v</div></pre></td></tr></table></figure></p>
<h4 id="miscellaneous"><a href="#miscellaneous" class="headerlink" title="miscellaneous"></a>miscellaneous</h4><ul>
<li>var2:0 的意义是这个name为 var2 的运算（节点）的第一个结果</li>
<li>一个回合： 就是全部的训练集数据都参与了运算就算是一次。</li>
<li>Why does c = tf.matmul(a, b) not execute the matrix multiplication immediately?<br>In the TensorFlow Python API, a, b, and c are tf.Tensor objects. A Tensor object is a symbolic handle to the result of an operation, but does not actually hold the values of the operation’s output. Instead, TensorFlow encourages users to build up complicated expressions (such as entire neural networks and its gradients) as a dataflow graph. You then offload the computation of the entire dataflow graph (or a subgraph of it) to a TensorFlow tf.Session, which is able to execute the whole computation much more efficiently than executing the operations one-by-one.</li>
</ul>

    </div>

    

    

    <!-- Comments -->
    

</div>
        </section>

    </div>
</div>


</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This is Robert Lexis (FengCun Li). To see the world, things dangerous to come to, to see behind walls, to draw closer, to find each other and to feel. That is the purpose of LIFE.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2018/09/03/Optimizers/">Optimizers</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/31/Train-Faster-R-CNN-s-RPN-as-a-standalone-app-using-TF-Object-Detecion-APIs/">Train Faster R-CNN&#39;s RPN </a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/28/Tensorflow-模型浮点数计算量和参数量统计/">TensorFlow 模型浮点数计算量和参数量统计</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/08/26/YOLO/">YOLO</a>
            </li>
            
        </ul>
    </div>



            
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/RobertLexis">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:robert_lexis@163.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Robert Lexis Loves Wenny
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



</body>

</html>