<!DOCTYPE html>
<html lang="zh-CN">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="强化学习主要用于解决连续决策问题三个主要概念：

环境（Environment）
动作（Action）
奖励（Reward）强化学习的目标就是让模型根据环境，动作和奖励学习出最佳的策略。强化学习不像监督学习那样具有明确的标签（目标），也不像无监督学习那样完全没有标签（目标），强化学习的目标一般是变化">
    

    <!--Author-->
    
        <meta name="author" content="Robert Lexis">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="强化学习 1 Basic"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="This is WHY."/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

        <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>强化学习 1 Basic - This is WHY.</title>

    <!-- Bootstrap Core CSS -->
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"/>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Google Analytics -->
    


    <!-- favicon -->
    
	
</head>


<body>

    <!-- Menu -->
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">This is WHY.</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                
                    <li>
                        <a href="/">
                            
                                Home
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/archives">
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/tags">
                            
                                Tags
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/categories">
                            
                                Categories
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="https://github.com/RobertLexis">
                            
                                <i class="fa fa-github fa-stack-2x"></i>
                            
                        </a>
                    </li>
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

    <!-- Main Content -->
    <!-- Page Header -->
<!-- Set your background image for this header in your post front-matter: cover -->

<header class="intro-header" style="background-image: url('http://www.codeblocq.com/assets/projects/hexo-theme-clean-blog/img/home-bg.jpg')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>强化学习 1 Basic</h1>
                    
                    <span class="meta">
                        <!-- Date and Author -->
                        
                        
                            2017-12-07
                        
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Tags and categories -->
           

            <!-- Gallery -->
            

            <!-- Post Main Content -->
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p>主要用于解决连续决策问题<br>三个主要概念：</p>
<ol>
<li>环境（Environment）</li>
<li>动作（Action）</li>
<li>奖励（Reward）<br>强化学习的目标就是让模型根据环境，动作和奖励学习出最佳的策略。<br>强化学习不像监督学习那样具有明确的标签（目标），也不像无监督学习那样完全没有标签（目标），强化学习的目标一般是变化的，不明确的，甚至不是绝对正确的。<br>Google 的 DQN（Deep Q-Network）结合了深度学习和强化学习。<br>DeepMind 的 AlphaGo 结合了策略网络（Policy Network）、估值网络（Value Network 即DQN）与蒙特卡洛搜索树。<br>无人驾驶传感，CNN，RNN 对环境信息进行处理，再结合强化学习做出决策。</li>
</ol>
<h2 id="两类方法"><a href="#两类方法" class="headerlink" title="两类方法"></a>两类方法</h2><ol>
<li>Policy-Based （Policy Gradients） <ul>
<li>直接给出某个环境状态下应该采取的动作</li>
<li>适用于动作种类非常多或者连续取值的动作</li>
</ul>
</li>
<li>Value-Based（Q-Learning）<ul>
<li>给出某个环境状态下所有动作的Q值，之后可以选择Q值最高的动作。</li>
<li>少量离散取值的动作</li>
</ul>
</li>
</ol>
<h2 id="Model-Based-or-Model-Free"><a href="#Model-Based-or-Model-Free" class="headerlink" title="Model-Based or Model-Free"></a>Model-Based or Model-Free</h2><p>是否可以对环境进行建模，在复杂环境下一般使用 Model-Free 强化学习，同时给予更多的样本，弥补对于环境建模的缺失。</p>
<h2 id="策略网络"><a href="#策略网络" class="headerlink" title="策略网络"></a>策略网络</h2><p>在强化学习中，我们并不知道标签，对于某一个特定的环境状态，我们并不知道它对应的最好的动作是什么，只知道当前动作获得的奖励和试验后获得的未来奖励。我们需要让模型通过试验自己学习什么动作才是某一个特定环境状态下的较优动作，而不是告诉模型较优动作是什么，因为我们也不知道正确的答案。<br>为了让策略网络更好的理解未来，不仅仅要考虑动作的当前奖励，还要动作的考虑未来的、潜在的奖励，即奖励为 Discounted Reward。在训练过程中，模型会接触到好的动作以及它们带来的高奖励，差的动作以及它们带来的低奖励，通过学习，模型会逐渐增加选择好的动作的概率，降低选择差动作的概率，这样就逐渐完成了策略的学习。策略网络是一个End-to-End（端到端）的方法，直接产生动作。</p>
<h2 id="Gym"><a href="#Gym" class="headerlink" title="Gym"></a>Gym</h2><p>核心概念</p>
<ol>
<li>Environment，</li>
<li>Agent（编写的算法/模型）<br>Agent 观察环境状态（Observation），执行动作（Action）改变环境（Environment），得到奖励（Reward），再观察环境状态执行动作得到奖励，周而复始。即 Observation-Action-Reward 循环。<h2 id="策略网络解决-CartPole-任务"><a href="#策略网络解决-CartPole-任务" class="headerlink" title="策略网络解决 CartPole 任务"></a>策略网络解决 CartPole 任务</h2></li>
<li>Observation 四元组（车的位置，车的速度，杆的角度，杆的速度）</li>
<li>Action Space 给小车施加正向或者反向的力（0， 1）</li>
<li>任务目标时尽可能保持杆竖直不倒，当小车偏离中心超过2.4个单位或者杆的倾角大于15度时，任务失败。</li>
<li>在每坚持一步就会获得+1的奖励，模型要坚持尽可能长的时间而不导致任务失败。</li>
<li>模型为了获得尽可能多的奖励就要有远见，不能仅考虑当前的奖励，也要考虑未来长远的奖励<br>采取随机动作的 Agent：<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> gym</div><div class="line">env = gym.make(<span class="string">"CartPole-v0"</span>)</div><div class="line"></div><div class="line">env.reset()</div><div class="line">random_episodes = <span class="number">0</span></div><div class="line">reward_sum = <span class="number">0</span></div><div class="line"><span class="keyword">while</span> random_episodes &lt; <span class="number">10</span>:</div><div class="line">    observation, reward, done, _ = env.step(np.random.choice([<span class="number">0</span>, <span class="number">1</span>]))</div><div class="line">    reward_sum += reward</div><div class="line">    <span class="keyword">if</span> done:</div><div class="line">        random_episodes += <span class="number">1</span></div><div class="line">        print(<span class="string">"Reward for this episode was: "</span>, reward_sum)</div><div class="line">        reward_sum = <span class="number">0</span></div><div class="line">        env.reset()</div></pre></td></tr></table></figure>
</li>
</ol>
<p>MLP Policy Network：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!coding: utf-8</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </div><div class="line"><span class="keyword">import</span> gym </div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </div><div class="line">env = gym.make(<span class="string">"CartPole-v0"</span>)</div><div class="line"></div><div class="line"><span class="comment">################################# 网络结构 #####################</span></div><div class="line">hidden_n = <span class="number">50</span></div><div class="line">batch_size = <span class="number">25</span></div><div class="line">learning_rate = <span class="number">1e-1</span></div><div class="line">input_dim = <span class="number">4</span></div><div class="line">output_dim = <span class="number">1</span></div><div class="line">gamma = <span class="number">0.99</span></div><div class="line"></div><div class="line">observation_placeholder = tf.placeholder(tf.float32, [<span class="keyword">None</span>, input_dim], name=<span class="string">"observation_placeholder"</span>)</div><div class="line">w_1 = tf.get_variable(<span class="string">"w_1"</span>, shape=[input_dim, hidden_n], initializer=tf.contrib.layers.xavier_initializer())</div><div class="line">layer_1 = tf.nn.relu(tf.matmul(observation_placeholder, w_1))</div><div class="line">w_2 = tf.get_variable(<span class="string">"w_2"</span>, shape=[hidden_n, output_dim], initializer=tf.contrib.layers.xavier_initializer())</div><div class="line">probability = tf.nn.sigmoid(tf.matmul(layer_1, w_2))</div><div class="line"></div><div class="line"><span class="comment">################################# 批优化 #####################</span></div><div class="line"><span class="string">'''</span></div><div class="line"><span class="string">深度强化学习的训练也是采用 batch training，不逐个“样本”的更新参数，而累计 batch_size 个回合再更新参数，防止</span></div><div class="line"><span class="string">单一回合随机扰动噪声对模型训练带来的不利影响。</span></div><div class="line"><span class="string">不像 CNN 那样一个 batch 样本 feed 进模型，直接产生一个梯度平均值就可以进行一次参数的更新，</span></div><div class="line"><span class="string">这里需要进行 batch_size 次经历从而得到 batch_size 个回合，需要存储每回合中的平均梯度，并进行平均，最终更新一次参数。</span></div><div class="line"><span class="string">'''</span></div><div class="line">adam = tf.train.AdamOptimizer(learning_rate=learning_rate)</div><div class="line">w_1_grad = tf.placeholder(tf.float32, name=<span class="string">"batch_grad_1"</span>)</div><div class="line">w_2_grad = tf.placeholder(tf.float32, name=<span class="string">"batch_grad_2"</span>)</div><div class="line">batch_grad = [w_1_grad, w_2_grad]</div><div class="line">update = adam.apply_gradients(zip(batch_grad, tf.trainable_variables()))</div><div class="line"></div><div class="line"><span class="comment">################################# 损失 #####################</span></div><div class="line"><span class="string">'''</span></div><div class="line"><span class="string">在 CartPole 问题中，每一次得到的奖励和这次奖励之前的所有动作都有关，也就说之前的所有动作导致了这次奖励的得到，</span></div><div class="line"><span class="string">为了计算一个动作带来的奖励我们要计算这个动作之后全部奖励的折扣和。</span></div><div class="line"><span class="string">我们倒推求解每一个动作带来的奖励。在 CartPole 任务中除了导致任务失败的那次动作之外，所有动作的即时奖励都是 1。</span></div><div class="line"><span class="string">一个动作带来的奖励是后一时间步动作带来的奖励的折扣加上这个动作的即时奖励。</span></div><div class="line"><span class="string">'''</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">discount_reward</span><span class="params">(r)</span>:</span></div><div class="line">    d_r = np.zeros_like(r)</div><div class="line">    d_r[<span class="number">-1</span>] = r[<span class="number">-1</span>]</div><div class="line">    <span class="keyword">for</span> time <span class="keyword">in</span> range(len(r)<span class="number">-1</span>)[::<span class="number">-1</span>]:</div><div class="line">        d_r[time] =  d_r[time+<span class="number">1</span>] * gamma + r[time]</div><div class="line">    <span class="keyword">return</span> d_r</div><div class="line"></div><div class="line"><span class="comment"># 模型做出的动作的反动作，学习目标，相当于 label，是来自于模型的概率输出与一个来自均匀分布的随机数的比较</span></div><div class="line">opposite_action = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>], name=<span class="string">"opposite_action"</span>)</div><div class="line"><span class="comment"># 动作带来的奖励</span></div><div class="line">advantage = tf.placeholder(tf.float32, name=<span class="string">"reward_signal"</span>)</div><div class="line"><span class="string">'''</span></div><div class="line"><span class="string">模型得到这份奖励的概率：</span></div><div class="line"><span class="string">opposite_action=1 时，即 action=0，1- probability</span></div><div class="line"><span class="string">opposite_action=0 时，即 action=1，probability</span></div><div class="line"><span class="string">综合上面两个式子，模型得到这份奖励的概率：</span></div><div class="line"><span class="string">opposite_action * (opposite_action - probability) + (1 - opposite_action) * (opposite_action + probability)</span></div><div class="line"><span class="string">'''</span></div><div class="line"><span class="comment"># 模型得到这份奖励的概率的对数：</span></div><div class="line">log_prob = tf.log(opposite_action * (opposite_action - probability) </div><div class="line">    + (<span class="number">1</span> - opposite_action) * (opposite_action + probability))</div><div class="line">loss = -tf.reduce_mean(log_prob * advantage)</div><div class="line">gradients = tf.gradients(loss, tf.trainable_variables())</div><div class="line"></div><div class="line"><span class="comment">################################# 训练 #####################</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    init = tf.global_variables_initializer()</div><div class="line">    sess.run(init)</div><div class="line"></div><div class="line">    observation = env.reset()</div><div class="line">    xs, ys, rewards = [], [], []</div><div class="line">    reward_sum = <span class="number">0</span></div><div class="line">    episode = <span class="number">1</span></div><div class="line">    total_episodes = <span class="number">10000</span></div><div class="line"></div><div class="line">    grad_buffer = sess.run(tf.trainable_variables())</div><div class="line">    <span class="keyword">for</span> i, grad <span class="keyword">in</span> enumerate(grad_buffer):</div><div class="line">        grad_buffer[i] = grad * <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="keyword">while</span> episode &lt;= total_episodes:</div><div class="line">        x = np.reshape(observation, [<span class="number">1</span>, input_dim])</div><div class="line">        prob = sess.run(probability, feed_dict=&#123;observation_placeholder: x&#125;)</div><div class="line">        action = <span class="number">1</span> <span class="keyword">if</span> np.random.uniform() &lt; prob <span class="keyword">else</span> <span class="number">0</span></div><div class="line">        xs.append(x)</div><div class="line">        ys.append(<span class="number">1</span> - action)</div><div class="line">        observation, reward, done, info = env.step(action)</div><div class="line">        reward_sum += reward</div><div class="line">        rewards.append(reward)</div><div class="line">        <span class="keyword">if</span> done:</div><div class="line">            <span class="comment">########## 构建一个回合的样本，计算该回合中所有样本上的梯度和，并加入到此批次的梯度buffer中 #########</span></div><div class="line">            episode += <span class="number">1</span></div><div class="line">            xs_per_episode = np.vstack(xs)</div><div class="line">            ys_per_episode = np.vstack(ys)</div><div class="line">            rewards_per_episode = np.vstack(rewards)</div><div class="line">            xs, ys, rewards = [], [], []</div><div class="line">            d_r = discount_reward(rewards_per_episode)</div><div class="line">            d_r -= np.mean(d_r)</div><div class="line">            d_r /= np.std(d_r)</div><div class="line">            grad_per_episode = sess.run(gradients, feed_dict=&#123;observation_placeholder: xs_per_episode,</div><div class="line">                opposite_action: ys_per_episode,</div><div class="line">                advantage: d_r&#125;)</div><div class="line">            <span class="keyword">for</span> i, grad <span class="keyword">in</span> enumerate(grad_per_episode):</div><div class="line">                grad_buffer[i] += grad</div><div class="line"></div><div class="line">            <span class="keyword">if</span> episode % batch_size == <span class="number">0</span>:</div><div class="line">                sess.run(update, feed_dict=&#123;w_1_grad: grad_buffer[<span class="number">0</span>], w_2_grad: grad_buffer[<span class="number">1</span>]&#125;)</div><div class="line"></div><div class="line">                <span class="keyword">for</span> i, grad <span class="keyword">in</span> enumerate(grad_buffer):</div><div class="line">                    grad_buffer[i] = grad * <span class="number">0</span></div><div class="line"></div><div class="line">                print(<span class="string">"Average reward for episode %d : %.2f"</span> %(episode, reward_sum/batch_size))</div><div class="line"></div><div class="line">                <span class="keyword">if</span> reward_sum/batch_size &gt;= <span class="number">200</span>:</div><div class="line">                    print(<span class="string">"Task solved in %d episodes"</span> % episode)</div><div class="line">                    <span class="keyword">break</span></div><div class="line">                reward_sum =<span class="number">0</span></div><div class="line">            observation = env.reset()</div></pre></td></tr></table></figure></p>
<h2 id="估值网络"><a href="#估值网络" class="headerlink" title="估值网络"></a>估值网络</h2><h4 id="概念："><a href="#概念：" class="headerlink" title="概念："></a>概念：</h4><ol>
<li>Q-learning 中的期望价值指从当前的这一步到后续的所有步骤，总共可以期望获得的最大价值。也就说是对未来进行了断言，这一步采取这样的动作，不管未来采取什么样的动作，可以期望获得的价值不会超过这个Q值。Q值的意义就是最大价值，当前最大 = 当前 + gamma * 以后续一步为当前的最大。</li>
<li>Q-learning 的目的就是求解 \(Q(s_t, a_t)\)，可以将其看作一个表格，每一行是不同的状态，每一列是不同的动作，学习的过程就是把这个表用合适的值填充起来。</li>
<li>Q-learning 的训练思路也很简单，以（状态，动作，奖励，下一状态）四元组（\((s_t, a_t, r_{t+1}, s_{t+1})\)）作为样本进行训练。</li>
<li>学习目标 \(r_{t+1} + \gamma \cdot \max\limits_{a} Q(s_{t+1}, a)\)，即相当于 label。</li>
<li>学习过程 \(Q_{new}(s_t, a_t) \gets (1-\alpha) \cdot Q_{old}(s_t, a_t) + \alpha \cdot (r_{t+1} + \gamma \cdot \max\limits_{a} Q_{old}(s_{t+1}, a))\)</li>
<li>Q-learning 的模型是神经网络时，即将\(Q(s_t, a_t)\)表达为神经网络，就是估值网络。</li>
<li>DQN 是 DeepMind 在 Human-level control through deep reinforcement learning 中提出的。</li>
</ol>
<h4 id="Tricks："><a href="#Tricks：" class="headerlink" title="Tricks："></a>Tricks：</h4><ol>
<li>Experience Replay，因为深度学习需要大量的样本，所以传统的Q-learning 的 online update 的方法不太适合于 DQN，可以像训练CNN 那样进行多个 epoch 的训练，主要思想就是存储 Agent 的 Experience，并且每次训练时随机抽取一部分样本训练网络。这样就能比较稳定的完成学习任务，避免只短视地学习新接触地样本，而是综合地反复利用过往的大量样本进行学习。可以创建一个用来存储Experience的缓存buffer，缓存满了之后就使用新的样本进行替换，如此就可以保证样本都有相近的概率被抽到，如果不替换，那么一开始就有的老样本在整个训练过程中被抽中的概率就会比新样本大得多。</li>
<li>使用 target DQN 进行辅助训练，它的意义是用来计算目标 Q 值（label），即提供\(\max\limits_{a} Q(s_{t+1}, a)\)。之所以拆分为两个网络，一个用来制造学习目标，一个用来进行实际训练，是为了让 Q-learning 训练的目标平稳。强化学习的学习目标每次更新后都在变化，即相同的输入，目标Q值（label）并不相同，如果模型参数更新地很频繁，幅度很大，训练过程就会因为目标剧烈变化而非常不稳定，容易失控。为了降低这个影响，需要让目标Q值（label）尽量平稳，因此需要一个比较稳定的 target DQN 辅助计算目标Q值，让target DQN 进行低频率的学习，让它输出的目标Q值波动也小，可以减少对训练过程的影响。</li>
<li>Double DQN 是在拆分出target DQN 的基础上更近一步，DeepMind 的研究者在 Deep Reinforcement Learning with Double Q-learning 中指出，传统的DQN通常会高估 Action 的Q值，如果这中高估是不均匀的，可能会导致本来次优的 Action 总是被高估而超过最后的 Action，这样就可能永远发现不了最优的 Action。之前，target DQN 完全负责生成目标Q值，先产生\(Q(s_{t+1}, a)\),再通过\(\max\limits_{a}\)选择那个最大的Q值。Double DQN 改变了第二步，不是直接选择 target DQN 输出中的最大Q值，而是在主DQN的输出中找到最大Q值对应的 Action，再找到 target DQN 的输出中这个 Action 对应的Q值，这个Q值作为目标Q值（即label）。使用的目标Q值不一定总是 target DQN 输出中的最大Q值，这样就避免了总是从 target DQN 的输出中选择被高估的同一个 Action 对应的Q值作为 label。此时学习目标就可以写成 $$Target=r_{t+1} + \gamma \cdot Q_{target}(s_{t+1}, argmax_{a}(Q_{main}(s_{t+1}, a)))$$</li>
<li>Dueling DQN 是 DQN 一大改进，在 Dueling Network Architectures for Deep Reinforcement Learning 中提出，将Q值函数\(Q(s_{t}, a)\)拆分成两部分，一部分是静态的环境本身具有的价值\(V(s_{t})\)，称为 Value；另外一部分是通过执行 Action 额外带来的价值\(A(a_{t})\)，称为 Advantage，即 \(Q(s_{t}, a_{t})=V(s_{t})+A(a_{t})\)。Dueling 的目的就是让网络分别计算环境本身的 Value 和 Action 带来的 Advantage，Advantage 或好或坏，设计成零均值。<br><img src="http://oytnj8g2y.bkt.clouddn.com/image/png/2017/12/dueling.png" alt="传统DQN和Dueling DQN 的对比"><br>Dueling DQN 的输出是一个标量和向量，分别代表 Value 和每个 Action 的 Advantage。<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script></li>
</ol>


                
            </div>

            <!-- Comments -->
            
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    


                </div>
            
        </div>
    </div>
</article>

    <!-- Footer -->
    <hr />

<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    

                    

                    
                        <li>
                            <a href="https://github.com/RobertLexis" target="_blank">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    

                    

                    

                    
                </ul>
                <p class="copyright text-muted">&copy; 2017 Robert Lexis<br></p>
                <p class="copyright text-muted">Original Theme <a target="_blank" href="http://startbootstrap.com/template-overviews/clean-blog/">Clean Blog</a> from <a href="http://startbootstrap.com/" target="_blank">Start Bootstrap</a></p>
                <p class="copyright text-muted">Adapted for <a target="_blank" href="https://hexo.io/">Hexo</a> by <a href="http://www.codeblocq.com/" target="_blank">Jonathan Klughertz</a></p>
            </div>
        </div>
    </div>
</footer>


    <!-- After footer scripts -->
    
<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Bootstrap -->
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->



</body>

</html>